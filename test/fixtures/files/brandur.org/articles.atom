<?xml version="1.0" encoding="UTF-8"?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
  <title>Articles â€” brandur.org</title>
  <id>tag:brandur.org,2013:/articles</id>
  <updated>2022-07-19T16:49:13Z</updated>
  <link rel="self" type="application/atom+xml" href="https://brandur.org/articles.atom"></link>
  <link rel="alternate" type="text/html" href="https://brandur.org"></link>
  <entry>
    <title>Soft Deletion Probably Isn&#39;t Worth It</title>
    <summary>The traditional soft deletion pattern using &lt;code&gt;deleted_at&lt;/code&gt; columns leaks into code, curbs the usefulness of foreign keys, and makes data removal difficult. Luckily, there&amp;rsquo;s an alternative.</summary>
    <content type="html"><![CDATA[<p>Anyone who&rsquo;s seen a couple different production database environments is likely familiar with the &ldquo;soft deletion&rdquo; pattern &ndash; instead of deleting data directly via <code>DELETE</code> statement, tables get an extra <code>deleted_at</code> timestamp and deletion is performed with an update statement instead:</p>

<pre><code class="language-sql">UPDATE foo SET deleted_at = now() WHERE id = $1;
</code></pre>

<p>The concept behind soft deletion is to make deletion safer, and reversible. Once a record&rsquo;s been hit by a hard <code>DELETE</code>, it may technically still be recoverable by digging down into the storage layer, but suffice it to say that it&rsquo;s really hard to get back. Theoretically with soft deletion, you just set <code>deleted_at</code> back to <code>NULL</code> and you&rsquo;re done:</p>

<pre><code class="language-sql">-- and like magic, it's back!!
UPDATE foo SET deleted_at = NULL WHERE id = $1;
</code></pre>

<h2 id="code-leakage" class="link"><a href="#code-leakage">Downsides: Code leakage</a></h2>

<p>But this technique has some major downsides. The first is that soft deletion logic bleeds out into all parts of your code. All our selects look something like this:</p>

<pre><code class="language-sql">SELECT *
FROM customer
WHERE id = @id
    AND deleted_at IS NULL;
</code></pre>

<p>And forgetting that extra predicate on <code>deleted_at</code> can have dangerous consequences as it accidentally returns data that&rsquo;s no longer meant to be seen.</p>

<p>Some ORMs or ORM plugins make this easier by automatically chaining the extra <code>deleted_at</code> clause onto every query (see <a href="https://github.com/ActsAsParanoid/acts_as_paranoid"><code>acts_as_paranoid</code></a> for example), but just because it&rsquo;s hidden doesn&rsquo;t necessarily make things better. If an operator ever queries the database directly they&rsquo;re even more likely to forget <code>deleted_at</code> because normally the ORM does the work for them.</p>

<h3 id="foreign-keys" class="link"><a href="#foreign-keys">Losing foreign keys</a></h3>

<p>Another consequence of soft deletion is that foreign keys are effectively lost.</p>

<p>The major benefit of foreign keys is that they guarantee referential integrity. For example, say you have customers in one table that may refer to a number of invoices in another. Without foreign keys, you could delete a customer, but forget to remove its invoices, thereby leaving a bunch of orphaned invoices that reference a customer that&rsquo;s gone.</p>

<p>With foreign keys, trying to remove that customer without removing the invoices first is an error:</p>

<pre><code class="language-sql">ERROR:  update or delete on table &quot;customer&quot; violates
    foreign key constraint &quot;invoice_customer_id_fkey&quot; on table &quot;invoice&quot;

DETAIL:  Key (id)=(64977e2b-40cc-4261-8879-1c1e6243699b) is still
    referenced from table &quot;invoice&quot;.
</code></pre>

<p>As with other relational database features like predefined schemas, types, and check constraints, the database is helping to keep data valid.</p>

<p>But with soft deletion, this goes out the window. A customer may be soft deleted with its <code>deleted_at</code> flag set, but we&rsquo;re now back to being able to forget to do the same for its invoices. Their foreign keys are still valid because the customer record is technically still there, but there&rsquo;s no equivalent check that the invoices are also soft deleted, so you can be left with your customer being &ldquo;deleted&rdquo;, but its invoices still live.</p>

<h3 id="pruning" class="link"><a href="#pruning">Pruning data is hard</a></h3>

<p>The last few years have seen major advances in terms of consumer data protection like the roll out of <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a> in Europe. As such, it&rsquo;s generally frowned upon for data to be retained infinitely, which by default would be the case for soft deleted rows.</p>

<p>So you may eventually find yourself writing a hard deletion process which looks at soft deleted records beyond a certain horizon and permanently deletes them from the database.</p>

<p>But the same foreign keys that soft deletion rendered mostly useless now make this job more difficult because a record can&rsquo;t be removed without also making sure that all its dependencies are removed as well (<code>ON DELETE CASCADE</code> could do this automatically, but use of cascade is fairly dangerous and not recommended for higher fidelity data).</p>

<p>Luckily, you can still do this in systems that support CTEs like Postgres, but you end up with some pretty elaborate queries. Here&rsquo;s a snippet from one that I wrote recently which keeps all foreign keys satisfied by removing everything as part of a single operation:</p>

<pre><code class="language-sql">WITH team_deleted AS (
    DELETE FROM team
    WHERE (
        team.archived_at IS NOT NULL
        AND team.archived_at &lt; @archived_at_horizon::timestamptz
    )
    RETURNING *
),

--
-- team resources
--
cluster_deleted AS (
    DELETE FROM cluster
    WHERE team_id IN (
        SELECT id FROM team_deleted
    )
    OR (
        archived_at IS NOT NULL
        AND archived_at &lt; @archived_at_horizon::timestamptz
    )
    RETURNING *
),
invoice_deleted AS (
    DELETE FROM invoice
    WHERE team_id IN (
        SELECT id FROM team_deleted
    )
    OR (
        archived_at IS NOT NULL
        AND archived_at &lt; @archived_at_horizon::timestamptz
    )
    RETURNING *
),

--
-- cluster + team resources
--
subscription_deleted AS (
    DELETE FROM subscription
    WHERE cluster_id IN (
        SELECT id FROM cluster_deleted
    ) OR team_id IN (
        SELECT id FROM team_deleted
    )
    RETURNING *
)

SELECT 'cluster', array_agg(id) FROM cluster_deleted
UNION ALL
SELECT 'invoice', array_agg(id) FROM invoice_deleted
UNION ALL
SELECT 'subscription', array_agg(id) FROM subscription_deleted
UNION ALL
SELECT 'team', array_agg(id) FROM team_deleted;
</code></pre>

<p>The unabridged version of this is five times as long and includes a full 30 separate tables. It&rsquo;s cool that this works, but is so overly elaborate as to be a liability.</p>

<p>And even with liberal testing, this kind of query can still end up being a reliability problem because in case a new dependency is added in the future but an update to the query is forgotten, it&rsquo;ll suddenly start failing after a year&rsquo;s (or whatever the hard delete horizon is) delay.</p>

<h3 id="undelete" class="link"><a href="#undelete">Does undelete really work?</a></h3>

<p>Once again, soft deletion is theoretically a hedge against accidental data loss. As a last argument against it, I&rsquo;d ask you to consider, realistically, whether undeletion is something that&rsquo;s ever actually done.</p>

<p>When I worked at Heroku, we used soft deletion.</p>

<p>When I worked at Stripe, we used soft deletion.</p>

<p>At my job right now, we use soft deletion.</p>

<p>As far as I&rsquo;m aware, never <em>once</em>, in ten plus years, did anyone at any of these places ever actually use soft deletion to undelete something. <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup></p>

<p>The biggest reason for this is that almost always, <strong>data deletion also has non-data side effects</strong>. Calls may have been made to foreign systems to archive records there, objects may have been removed in blob stores, or servers spun down. The process can&rsquo;t simply be reversed by setting <code>NULL</code> on <code>deleted_at</code> &ndash; equivalent undos need to exist for all those other operations too, and they rarely do.</p>

<p>We had a couple cases at Heroku where an important user deleted an app by accident and wanted to recover it. We had soft deletion, and theoretically other delete side effects could&rsquo;ve been reversed, but we still made the call not to try because no one had ever done it before, and trying to do it in an emergency was exactly the wrong time to figure it out &ndash; we&rsquo;d almost certainly get something wrong and leave the user in a bad state. Instead, we rolled forward by creating a new app, and helping them copy environment and data from the deleted app to it. So even where soft deletion was theoretically most useful, we still didn&rsquo;t use it.</p>

<h2 id="deleted-records" class="link"><a href="#deleted-records">Alternative: A deleted records table</a></h2>

<p>Although I&rsquo;ve never seen an undelete work in practice, soft deletion wasn&rsquo;t completely useless because we would occasionally use it to refer to deleted data &ndash; usually a manual process where someone wanted to see to a deleted object for purposes of assisting with a support ticket or trying to squash a bug.</p>

<p>And while I&rsquo;d argue against the traditional soft deletion pattern due to the downsides listed above, luckily there&rsquo;s a compromise.</p>

<p>Instead of keeping deleted data in the same tables from which it was deleted, there can be a new relation specifically for storing all deleted data, and with a flexible <code>jsonb</code> column so that it can capture the properties of any other table:</p>

<pre><code class="language-sql">CREATE TABLE deleted_record (
    id uuid PRIMARY KEY DEFAULT gen_ulid(),
    deleted_at timestamptz NOT NULL default now(),
    original_table varchar(200) NOT NULL,
    original_id uuid NOT NULL,
    data jsonb NOT NULL
);
</code></pre>

<p>A deletion then becomes this:</p>

<pre><code class="language-sql">WITH deleted AS (
    DELETE FROM customer
    WHERE id = @id
    RETURNING *
)
INSERT INTO deleted_record
		(original_table, original_id, data)
SELECT 'foo', id, to_jsonb(deleted.*)
FROM deleted
RETURNING *;
</code></pre>

<p>This does have a downside compared to <code>deleted_at</code> &ndash; the process of selecting columns into <code>jsonb</code> isn&rsquo;t easily reversible. While it&rsquo;s possible to do so, it would likely involve building one-off queries and manual intervention. But again, that might be okay &ndash; consider how often you&rsquo;re really going to be trying to undelete data.</p>

<p>This technique solves all the problems outlined above:</p>

<ul>
<li><p>Queries for normal, non-deleted data no longer need to include <code>deleted_at IS NULL</code> everywhere.</p></li>

<li><p>Foreign keys still work. Attempting to remove a record without also getting its dependencies is an error.</p></li>

<li><p>Hard deleting old records for regulatory requirements gets really, really easy: <code>DELETE FROM deleted_record WHERE deleted_at &lt; now() - '1 year'::interval</code>.</p></li>
</ul>

<p>Deleted data is a little harder to get at, but not by much, and is still kept around in case someone needs to look at it.</p>


]]></content>
    <published>2022-07-19T16:49:13Z</published>
    <updated>2022-07-19T16:49:13Z</updated>
    <link href="https://brandur.org/soft-deletion"></link>
    <id>tag:brandur.org,2022-07-19:soft-deletion</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Postgres: Boundless `text` and Back Again</title>
    <summary>The &lt;code&gt;text&lt;/code&gt; type in Postgres, why it&amp;rsquo;s awesome, and why you might want to use &lt;code&gt;varchar&lt;/code&gt; anyway. Also, a story about trying to get string parameters bounded at Stripe.</summary>
    <content type="html"><![CDATA[<p>One of the major revelations for almost every new user to Postgres is that there&rsquo;s no technical advantage of specifying columns as <code>varchar(n)</code> compared to just using bound-less <code>text</code>. Not only is the <code>text</code> type provided as a convenience (it&rsquo;s not in the SQL standard), but using it compared to constrained character types like <code>char</code> and <code>varchar</code> carries no performance penalty. From the Postgres <a href="https://www.postgresql.org/docs/current/datatype-character.html">docs on character type</a> (and note that <code>character varying</code> is the same thing as <code>varchar</code>):</p>

<blockquote>
<p>There is no performance difference among these three types, apart from increased storage space when using the blank-padded type, and a few extra CPU cycles to check the length when storing into a length-constrained column. While <code>character(n)</code> has performance advantages in some other database systems, there is no such advantage in PostgreSQL; in fact <code>character(n)</code> is usually the slowest of the three because of its additional storage costs. In most situations <code>text</code> or <code>character varying</code> should be used instead.</p>
</blockquote>

<p>For many of us this is a huge unburdening, as we&rsquo;re used to micromanaging length limits in other systems. Having worked in large MySQL and Oracle systems, I was in the habit of not just figuring out what column to add, but also how long it needed to be &ndash; should this be a <code>varchar(50)</code> or <code>varchar(100)</code>? <code>500</code>? (Or none of the above?) With Postgres, you just stop worrying and slap <code>text</code> on everything. It&rsquo;s freeing.</p>

<p>I&rsquo;ve since changed my position on that somewhat, and to explain why, I&rsquo;ll have to take you back to Stripe circa ~2018.</p>

<h2 id="s3ripe" class="link"><a href="#s3ripe">S3ripe</a></h2>

<p>One day we came to a rude awakening that we weren&rsquo;t checking length limits on text fields in Stripe&rsquo;s API. It wasn&rsquo;t just that a few of them weren&rsquo;t checked &ndash; it was that practically none of them were. While the API framework did allow for a maximum length, no one had ever thought to assign it a reasonable default, and as a matter of course the vast majority of parameters (of which there were thousands by this point) didn&rsquo;t set one. As long as senders didn&rsquo;t break any limits around size of request payload, they could send us whatever they wanted in any field they wanted. The API would happily pass it through and persist it to Mongo forever.</p>

<p>I don&rsquo;t remember how exactly we noticed, but sufficed to say we only did when it became a problem. Some user was sending us truly ginormous payloads and it was crashing HTTP workers, tying up database resources, or something equally bad.</p>

<p>As far as problems in computing go, checking string lengths isn&rsquo;t one that&rsquo;s considered to be particularly hard, so we set to work putting in a fix. But not so fast &ndash; these weren&rsquo;t the early days of the company anymore. We already had countless users, were processing millions of requests, and that meant by extension that we could expect many of those to include large-ish strings. We&rsquo;d never had rules around lengths before, and without a hard constraint, given enough users and enough time, someone (or many someones as it were) eventually starts sending long strings. Suddenly introducing maximums would break those integrations and create a lot of unhappy users. Stripe takes backwards compatibility very seriously, and would never do something like that on purpose.</p>

<p>Already fearing what I was about to find, I went ahead and put a probe in production that would generate statistics around text field lengths, including upper bounds and distribution, and waited a day to gather data.</p>

<p>It was even worse than we&rsquo;d thought &ndash; we had at least hundreds of users (and maybe thousands, my memory is bad) who were sending huge text payloads. Worse yet, these were all legitimate users &ndash; legitimate users who for one reason or another had decided over the years to build unconventional integration patterns. They&rsquo;d be doing something like sending us their whole product catalog, or a big JSON blob to store, and as part of their normal integration flows.</p>

<p>We&rsquo;d occasionally engage in active outreach campaigns to get users to change something, but it&rsquo;s a massive amount of work, and we have to offer generous deprecation timelines when we do. Given the nature of this problem and the number of users involved, it wasn&rsquo;t worth the effort. My dream of constraining most fields like customer or plan name to something reasonable like &ldquo;only&rdquo; 200 characters was a total non-starter.</p>

<p>Instead, we ran the numbers, and came up with a best fit compromise that would leave the maximum numbers of users unaffected while still bounding fields text fields to something not completely crazy (the chosen number was 5000, as viewable in the <a href="https://github.com/stripe/openapi">public OpenAPI spec</a>). And even the new very liberal limit was too long for a few users sending us giant payloads, so we gated them into an exemption.</p>

<p>Let me briefly restate Hyrum&rsquo;s law:</p>

<blockquote>
<p>With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody.</p>
</blockquote>

<p>Truer words have rarely been spoken.</p>

<h2 id="section-1" class="link"><a href="#section-1">varchars considered ~harm~helpful</a></h2>

<p>Starting my <a href="/nanoglyphs/024-new-horizons">new position back in April</a>, one thing I checked early on is whether we were checking the length of strings that we were passing on through to the database. Nope. It turns out that this is a <em>very</em> easy mistake to make.</p>

<p>This is a downside to the common Postgres wisdom of &ldquo;just use <code>text</code>&rdquo;. It&rsquo;s generally fine, but there are ramifications at the edges that are harder to see.</p>

<p>I&rsquo;ve gone back to the habit of making most text fields <code>varchar</code> again. But I still don&rsquo;t like micromanaging character lengths, or how after a while every <code>varchar</code> column has a different length seemingly picked at random, so I&rsquo;ve pushed that we adopt some common order of magnitude &ldquo;tiers&rdquo;. For example:</p>

<ul>
<li><code>varchar(200)</code> for shorter-length strings like names, addresses, email addresses, etc.</li>
<li><code>varchar(2000)</code> for longer text blocks like descriptions.</li>
<li><code>varchar(20000)</code> for really long text blocks.</li>
</ul>

<p>The idea is to pick <em>liberal</em> numbers that are easily long enough to hold any even semi-valid data. Hopefully you never actually reach any of these maximums &ndash; they&rsquo;re just there as a back stop to protect against data that&rsquo;s wildly wrong. I wouldn&rsquo;t even go so far as to encourage the use of the numbers I pitched above &ndash; if you try this, go with your own based on what works for you.</p>

<p>Having a constraint in the database doesn&rsquo;t mean that you shouldn&rsquo;t <em>also</em> check limits in code. Most programs aren&rsquo;t written to gracefully handle database constraint failures, so for the sake of your users, put in a standard error-handling framework and descriptive error messages in the event this ever happens. Once again, the database is the back stop &ndash; there as a last layer of protection when the others fail.</p>

<h3 id="coercible-types" class="link"><a href="#coercible-types">Coercible types and operations</a></h3>

<p>Back in the old days, there was a decent argument to avoid <code>varchar</code> for operational resilience if nothing else. Changing a column&rsquo;s data type is often an expensive process involving full table scans and rewrites that can put a hot database at major risk. Is the potential agony really worth it just to use a <code>varchar</code> that&rsquo;s later found to be too short?</p>

<p>Luckily, when it comes to <em>relaxing</em> constraints, this isn&rsquo;t too much of a problem anymore. From the <a href="https://www.postgresql.org/docs/current/sql-altertable.html">Postgres docs on <code>ALTER TABLE</code></a>:</p>

<blockquote>
<p>Adding a column with a volatile <code>DEFAULT</code> or changing the type of an existing column will require the entire table and its indexes to be rewritten. As an exception, when changing the type of an existing column, if the <code>USING</code> clause does not change the column contents and the old type is either binary coercible to the new type or an unconstrained domain over the new type, a table rewrite is not needed; but any indexes on the affected columns must still be rebuilt.</p>
</blockquote>

<p>Note the wording of &ldquo;unconstrained domain&rdquo;. A <code>varchar(200)</code> is an unconstrained domain over a <code>varchar(100)</code> because it&rsquo;s strictly longer. Postgres can relax the constraint without needing to lock the table for a scan. Going back the other way isn&rsquo;t as easy, but you shouldn&rsquo;t need to do that.</p>

<h3 id="sql-domains" class="link"><a href="#sql-domains">SQL domains</a></h3>

<p>Another idea I&rsquo;ve been experimenting with is encoding a standard set of text tiers as <a href="https://www.postgresql.org/docs/current/sql-createdomain.html">domains</a>, which defines a new data type with more constraints:</p>

<pre><code class="language-sql">CREATE DOMAIN text_standard AS varchar(200) COLLATE &quot;C&quot;;
CREATE DOMAIN text_long AS varchar(2000) COLLATE &quot;C&quot;;
CREATE DOMAIN text_huge AS varchar(20000) COLLATE &quot;C&quot;;
</code></pre>

<p>The domains can then be used by convention in table definitions:</p>

<pre><code class="language-sql"># CREATE TABLE mytext (standard text_standard, long text_long, huge text_huge);

# \d+ mytext
                                       Table &quot;public.mytext&quot;
  Column  |     Type      | Collation | Nullable | Default | Storage  | Stats target | Description
----------+---------------+-----------+----------+---------+----------+--------------+-------------
 standard | text_standard |           |          |         | extended |              |
 long     | text_long     |           |          |         | extended |              |
 huge     | text_huge     |           |          |         | extended |              |
</code></pre>

<p>The only thing I don&rsquo;t like about this set up is that it somewhat obfuscates what those columns are because they&rsquo;re no longer a common type. It is quite easy to get Postgres to hand you back domain definitions with <code>\dD</code>:</p>

<pre><code class="language-sql"># \dD
                                      List of domains
 Schema |     Name      |           Type           | Collation | Nullable | Default | Check
--------+---------------+--------------------------+-----------+----------+---------+-------
 public | text_huge     | character varying(20000) | C         |          |         |
 public | text_long     | character varying(2000)  | C         |          |         |
 public | text_standard | character varying(200)   | C         |          |         |
</code></pre>

<p>But &hellip; almost nobody will know how to do that off the top of their head.</p>

<h2 id="integrity" class="link"><a href="#integrity">Integrity in depth</a></h2>

<p>Constraints on text fields are a very small part of a broader story in how relational databases are built to help you. In the beginning, all their pedantry around data types, foreign keys, check constraints, ACID, and insert triggers may seem unnecessarily obscure and inflexible, but in the long run these features serve as strong enforcers of data integrity. You don&rsquo;t have to wonder whether your data is valid &ndash; you know it is.</p>
]]></content>
    <published>2021-09-10T15:55:19Z</published>
    <updated>2021-09-10T15:55:19Z</updated>
    <link href="https://brandur.org/text"></link>
    <id>tag:brandur.org,2021-09-10:text</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>How We Went All In on sqlc/pgx for Postgres + Go</title>
    <summary>Touring the ORM and Postgres landscape in Go, and why sqlc is today&amp;rsquo;s top pick.</summary>
    <content type="html"><![CDATA[<p>After a few months of research and experimentation with running a heavily DB-dependent Go app, we&rsquo;ve arrived at the conclusion that <a href="https://github.com/kyleconroy/sqlc">sqlc</a> is the figurative Correct Answer when it comes to using Postgres (and probably other databases too) in Go code beyond trivial uses. Let me walk you through how we got there.</p>

<p>First, let&rsquo;s take a broad tour of popular options in Go&rsquo;s ecosystem:</p>

<ul>
<li><p><code>database/sql</code>: Go&rsquo;s built-in database package. Most people agree &ndash; best to avoid it. It&rsquo;s database agnostic, which is kind of nice, but by extension that means it conforms to the lowest common denominator. No support for Postgres-specific features.</p></li>

<li><p><a href="https://github.com/lib/pq"><code>lib/pq</code></a>: An early Postgres frontrunner in the Go ecosystem. It was good for its time and place, but has fallen behind, and is no longer actively maintained.</p></li>

<li><p><a href="https://github.com/jackc/pgx"><code>pgx</code></a>: A very well-written and very thorough package for full-featured, performant connections to Postgres. However, it&rsquo;s opinionated about not offering any ORM-like features, and gets you very little beyond a basic query interface. Like with <code>database/sql</code>, hydrating database results into structs is painful &ndash; not only do you have to list target fields off ad nauseam in a <code>SELECT</code> statement, but you also have to <code>Scan</code> them into a struct manually.</p>

<ul>
<li><a href="https://github.com/georgysavva/scany"><code>scany</code></a>: Scany adds some quality-of-life improvement on top of pgx by eliminating the need to scan into every field of a struct. However, the desired field names must still be listed out in a <code>SELECT ...</code> statement, so it only reduces boilerplate by half.</li>
</ul></li>

<li><p><a href="https://github.com/go-pg/pg"><code>go-pg</code></a>: I&rsquo;ve used this on projects before, and it&rsquo;s a pretty good little Postgres-specific ORM. A little more below on why ORMs in Go aren&rsquo;t particularly satisfying, but another downside with go-pg is that it implements its own driver, and isn&rsquo;t compatible with pgx.</p>

<ul>
<li><a href="https://bun.uptrace.dev/guide/pg-migration.html#new-features">Bun</a>: go-pg has also been put in maintenance mode in favor of Bun, which is a go-pg rewrite that works with non-Postgres databases.</li>
</ul></li>

<li><p><a href="https://gorm.io/"><code>gorm</code></a>: Similar to go-pg except not Postgres specific. It can use pgx as a driver, but misses a lot of Postgres features.</p></li>
</ul>

<h2 id="strings" class="link"><a href="#strings">Queries as strings</a></h2>

<p>A big downside of vanilla <code>database/sql</code> or pgx is that SQL queries are strings:</p>

<pre><code class="language-go">var name string
var weight int64
err := conn.QueryRow(ctx, &quot;SELECT name, weight FROM widgets WHERE id = $1&quot;, 42).
	Scan(&amp;name, &amp;weight)
if err != nil {
	...
}
fmt.Println(name, weight)
</code></pre>

<p>This is fine for simple queries, but provides little in the way of confidence that queries actually work. The compiler just sees a string, so you need to write exhaustive test coverage to verify them.</p>

<p>And it gets worse. When you&rsquo;re writing a larger application that&rsquo;s trying to hydrate models, in an effort to reduce code duplication, you might start slicing and dicing those query strings &ndash; gluing little pieces together to share code. e.g.</p>

<pre><code class="language-go">err := conn.QueryRow(ctx, `SELECT ` + scanTeamFields + ` ...)
</code></pre>

<p>You can make it work, and still verify what you have is right by way of tests, but it gets messy fast.</p>

<h2 id="orms" class="link"><a href="#orms">ORMs</a></h2>

<p>ORMs like go-pg make this a little better by bringing some typing into the mix, which has some benefit for reducing mistakes:</p>

<pre><code class="language-go">story := new(Story)
err = db.Model(story).
    Relation(&quot;Author&quot;).
    Where(&quot;story.id = ?&quot;, story1.Id).
    Select()
if err != nil {
    panic(err)
}
</code></pre>

<p>However, without generics, Go&rsquo;s type system can only offer so much, and in practice, the compiler can&rsquo;t catch all that much more than when we were concatenating strings together. In the code above, <code>Model()</code> returns a <code>*Query</code> object. <code>Relation()</code> also returns a <code>*Query</code> object, and so does <code>Where()</code>. go-pq can  do some intelligent shuffling (e.g. putting a <code>LIMIT</code> before a <code>WHERE</code> wouldn&rsquo;t work in SQL, but go-pg will make it work because it&rsquo;s constructing the query lazily), but like with strings, there&rsquo;s a plethora of mistakes that will only be caught on runtime.</p>

<p>ORMs also have the problem of being an impedance mismatch compared to the raw SQL most people are used to, meaning you&rsquo;ve got the reference documentation open all day looking up how to do accomplish things when the equivalent SQL would&rsquo;ve been automatic. Easier queries are pretty straightforward, but imagine if you want to add an upsert or a <a href="https://www.postgresql.org/docs/current/queries-with.html">CTE</a>.</p>

<h2 id="sqlc" class="link"><a href="#sqlc">sqlc</a></h2>

<p>And that&rsquo;s where <a href="https://github.com/kyleconroy/sqlc">sqlc</a> comes in. With sqlc, you write <code>*.sql</code> files that contain table definitions along with queries annotated with a name and return type in a magic comment:</p>

<pre><code class="language-sql">CREATE TABLE authors (
  id   BIGSERIAL PRIMARY KEY,
  name text      NOT NULL,
  bio  text
);

-- name: CreateAuthor :one
INSERT INTO authors (
  name, bio
) VALUES (
  $1, $2
)
RETURNING *;
</code></pre>

<p>After running <code>sqlc generate</code> (which generates Go code from your SQL definitions) <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>, you&rsquo;re now able to run this:</p>

<pre><code class="language-go">author, err = dbsqlc.New(tx).CreateAuthor(ctx, dbsqlc.CreateAuthor{
    Name: &quot;Haruki Murakami&quot;,
    Bio:  &quot;Author of _Killing Commendatore_. Running and jazz enthusiast.&quot;,
    ...
})

if err != nil {
    return nil, xerrors.Errorf(&quot;error creating author: %w&quot;, err)
}

fmt.Printf(&quot;Author name: %s\n&quot;, author.Name)
</code></pre>

<p>sqlc isn&rsquo;t an ORM, but it implements one of the most useful features of one &ndash; mapping a query back into a struct without the need for boilerplate. If you have query with a <code>SELECT *</code> or <code>RETURNING *</code>, it knows which fields a table is supposed to have, and emits the result to a standard struct representing its records. All queries for a particular table that return its complete set of fields get to share the same output struct.</p>

<p>Rather than implement its own partially-complete SQL parser, sqlc uses PGAnalyze&rsquo;s <a href="https://github.com/pganalyze/pg_query_go">excellent <code>pg_query_go</code></a>, which bakes in the same query parser that Postgres really uses. It&rsquo;s never given me trouble so far &ndash; even complex queries with unusual Postgres embellishments work.</p>

<p>This query parsing also gives you some additional pre-runtime code verification. It won&rsquo;t protect you against logical bugs, but it won&rsquo;t compile invalid SQL queries, which is a far shot better than the guarantees you get with SQL-in-Go-strings. And thanks to SQL&rsquo;s declarative nature, it tends to produce fewer bugs than comparable procedural code. You&rsquo;ll still want to write tests, but you don&rsquo;t need to test every query and corner case as exhaustively.</p>

<h3 id="codegen" class="link"><a href="#codegen">Codegen</a></h3>

<p>I&rsquo;m slightly allergic to the idea of codegen on a philosophical level, and that made me reluctant to look too deeply into sqlc, but after finally getting into it, it&rsquo;s won me over.</p>

<p>Go makes programs like sqlc easily installable in one command (<code>go get github.com/kyleconroy/sqlc/cmd/sqlc</code>), and quickly with minimal fuss. Go&rsquo;s lightning fast startup and runtime speed means that your codegen loop runs in the blink of an eye. Our project is sitting around 100 queries broken up across a dozen input files and its codegen runs in (much) less than a second on commodity hardware:</p>

<pre><code class="language-go">$ time sqlc generate

real    0.07s
user    0.08s
sys     0.01s
</code></pre>

<p>Even if we expand our number of queries by 100x to 10,000, I think we&rsquo;ll still be comfortable with the timing on that development loop.</p>

<p>A GitHub Action verifies generated output, and between checkout, pulling down an sqlc binary, and running it, the whole job takes a grand total of 4 seconds to run.</p>

<h3 id="pgx" class="link"><a href="#pgx">pgx support appears</a></h3>

<p>Previously, a major reason not to use sqlc is that it didn&rsquo;t support pgx, which we were already bought into pretty deeply. A recent <a href="https://github.com/kyleconroy/sqlc/pull/1037">pull request</a> has addressed this problem by giving sqlc support for multiple drivers, and the feature&rsquo;s now available in the sqlc&rsquo;s latest release.</p>

<p>The authors also managed to write it in such a way that it&rsquo;s coupled very loosely &ndash; our mature codebase was making heavy use of pgx already and had a number of custom abstractions built on top of it, and yet I was able to get sqlc slotted in alongside them and fully operational in less than an hour. We could even weave sqlc invocations in amongst raw pgx invocations as part of the same transaction, giving us an easy way to migrate over to it incrementally.</p>

<h3 id="caveats" class="link"><a href="#caveats">Caveats and workarounds</a></h3>

<p>A few things in sqlc are less convenient compared to a more traditional ORM, but there are workarounds that land pretty well. For example, a noticeable one is that sqlc queries can&rsquo;t take an arbitrary number of parameters, so doing a multi-row insert doesn&rsquo;t work as easily as you&rsquo;d expect it to. However, you can get around this by sending batches as arrays which are unnested into distinct tuples in the SQL:</p>

<pre><code class="language-sql">-- Upsert many marketplaces, inserting or replacing data as necessary.
INSERT INTO marketplace (
    name,
    display_name
)
SELECT unnest(@names::text[]) AS name,
    unnest(@display_names::text[]) AS display_names
ON CONFLICT (name)
    DO UPDATE SET display_name = EXCLUDED.display_name
RETURNING *;
</code></pre>

<p>Another one is <code>UPDATE</code> where with a normal ORM you&rsquo;d just add as many target fields and values (i.e. <code>UPDATE foo SET a = 1, b = 2, c = 3, ...</code>) through the query builder as you wanted. Queries in sqlc must be fully structured in advance, so this doesn&rsquo;t work. What you can do is something like this where each field is conditionally updated based on the presence of an associated boolean:</p>

<pre><code class="language-sql">-- Update a team.
-- name: TeamUpdate :one
UPDATE team
SET
    customer_id = CASE WHEN @customer_id_do_update::boolean
        THEN @customer_id::VARCHAR(200) ELSE customer_id END,

    has_payment_method = CASE WHEN @has_payment_method_do_update::boolean
        THEN @has_payment_method::bool ELSE has_payment_method END,

    name = CASE WHEN @name_do_update::boolean
        THEN @name::text ELSE name END
WHERE
    id = @id
RETURNING *;
</code></pre>

<p>The Go code to update a field ends up looking like this:</p>

<pre><code class="language-go">team, err = queries.TeamUpdate(ctx, dbsqlc.TeamUpdateParams{
    NameDoUpdate: true,
    Name:         req.Name,
})
</code></pre>

<p>sqlc doesn&rsquo;t have any built-in conventions around how queries are named or organized, so you&rsquo;ll want to make sure to come up with your own so that you can find things.</p>

<h2 id="summary" class="link"><a href="#summary">Summary and future</a></h2>

<p>I&rsquo;ve largely covered sqlc&rsquo;s objective benefits and features, but more subjectively, it just <em>feels</em> good and fast to work with. Like Go itself, the tool&rsquo;s working for you instead of against you, and giving you an easy way to get work done without wrestling with the computer all day.</p>

<p>I won&rsquo;t go as far as to say that its the best answer across all ecosystems &ndash; the feats that Rust&rsquo;s SQL drivers can achieve with its type system are borderline wizardry &ndash; but sqlc&rsquo;s far and away my preferred solution when working in Go.</p>

<p>Lastly, generics are coming to Go, possibly <a href="https://go.dev/blog/generics-proposal">in beta form by the end of the year</a>, and that could change the landscape. I could imagine a world where they power a new generation of Go ORMs that can do better query checking and give you even better type completion. However, it&rsquo;s safe to say that&rsquo;s a good year or two out. Until then, we&rsquo;re happy with sqlc.</p>


]]></content>
    <published>2021-09-08T16:49:02Z</published>
    <updated>2021-09-08T16:49:02Z</updated>
    <link href="https://brandur.org/sqlc"></link>
    <id>tag:brandur.org,2021-09-08:sqlc</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Minimally Invasive (and More Accurate) Analytics: GoAccess and Athena/SQL</title>
    <summary>A privacy and GDPR-friendly approach to producing basic and complex site analytics, and one which is more accurate in an age when adblockers reign supreme.</summary>
    <content type="html"><![CDATA[<p>For years, like most of the rest of the internet, I&rsquo;ve been using Google Analytics to measure traffic to this site without thinking about it too much. To me, it&rsquo;s never been anything more than a glorified hit counter, and as the product&rsquo;s become more complex over the years, I&rsquo;m regularly confused by its expanding interface.</p>

<p>More recently, concerned with Google&rsquo;s seemingly insatiable appetite to track us to every corner of the internet, I started looking into more privacy-friendly alternatives. I landed on <a href="https://www.goatcounter.com/">Goatcounter</a>, which purposely does not set cookies and makes no attempt to track users, and is GDPR-compliant without a notice. It&rsquo;s also got a great, simple interface that&rsquo;s a masterwork of UI design compared to Google&rsquo;s crowded panels. Everything in one place, pretty graphs, and only the information I care about.</p>

<p>But I soon noticed after installing it that although Goatcounter is an independent analytics product with good intentions (to track as little as possible), that doesn&rsquo;t keep it safe from being included in uBlock Origin&rsquo;s block list. Indeed, my own adblocker was blocking analytics for my website:</p>

<figure>
    <img alt="uBlock Origin blocking analytics sites." class="overflowing" loading="lazy" src="/photographs/articles/minimal-analytics/ublock-origin.png" srcset="/photographs/articles/minimal-analytics/ublock-origin@2x.png 2x, /photographs/articles/minimal-analytics/ublock-origin.png 1x">
    <figcaption>uBlock Origin blocking analytics sites.</figcaption>
</figure>

<p>This got me thinking: this site is mostly technical material, and technical people tend to use adblockers. If a big segment of my readership are using adblockers, are my analytics even accurate? If not, how far are they off? After some investigation, the answer: they are absolutely <em>not</em> accurate, and off by <em>a lot</em>. It turns out that if there&rsquo;s any demographic of person who has an adblocker installed &ndash; it&rsquo;s you, dear reader.</p>

<p>I had a post from this site briefly spike to #1 on Hacker News last week, and looked at the difference between the traffic my analytics were showing, and the traffic I was actually receiving.</p>

<p>My estimate is that I got <strong>~2.5x</strong> more unique, non-robot visitors than reported by analytics (~38k versus ~13k), meaning that roughly <strong>60% of users are using an adblocker</strong>. Read to the end to see how I got these numbers.</p>

<p>If analytics products are being blocked at that level, there&rsquo;s a strong argument that it&rsquo;s not worth using them at all &ndash; you&rsquo;re really only getting a sample of traffic rather than an even semi-complete view. So what do you do instead? Well, how about analytic&rsquo;ing like its 1999 by reading log files. This is easier than it sounds because believe it or not, there&rsquo;s some amazing tooling to support it.</p>

<h2 id="goaccess" class="link"><a href="#goaccess">GoAccess</a></h2>

<p><a href="https://goaccess.io/">GoAccess</a> is a very well-written log parsing and visualizing utility, featuring both a curses-based terminal interface as well as a web server that will produce graphical HTML.</p>

<figure>
    <img alt="GoAccess' command line interface." class="overflowing" loading="lazy" src="/photographs/articles/minimal-analytics/goaccess.png" srcset="/photographs/articles/minimal-analytics/goaccess@2x.png 2x, /photographs/articles/minimal-analytics/goaccess.png 1x">
    <figcaption>GoAccess' command line interface.</figcaption>
</figure>

<p>It supports all common web logging formats including those from Apache, Nginx, ELBs, and CloudFront. This site is <a href="/aws-intrinsic-static">hosted on S3 and served via CloudFront</a>, so I&rsquo;m using the latter. Logging from CloudFront is easily configurable under the main settings panel for a distribution:</p>

<figure>
    <img alt="Configuring CloudFront logging." class="overflowing" loading="lazy" src="/photographs/articles/minimal-analytics/cloudfront-logging.png" srcset="/photographs/articles/minimal-analytics/cloudfront-logging@2x.png 2x, /photographs/articles/minimal-analytics/cloudfront-logging.png 1x">
    <figcaption>Configuring CloudFront logging.</figcaption>
</figure>

<p><strong>Tip:</strong> Consider using a log prefix as well so that you can log from multiple sites to the same bucket, and save yourself from configuring the same thing over and over again.</p>

<p>A nice augmentation is to configure the target S3 bucket with an expiration policy. This allows you to say, have logs pruned automatically after 30 days, further protecting your visitors privacy by retaining less, and preventing logs from accumulating forever and eating into your storage costs.</p>

<figure>
    <img alt="Creating an S3 lifecycle rule for expiration." class="overflowing" loading="lazy" src="/photographs/articles/minimal-analytics/s3-lifecycle-rules.png" srcset="/photographs/articles/minimal-analytics/s3-lifecycle-rules@2x.png 2x, /photographs/articles/minimal-analytics/s3-lifecycle-rules.png 1x">
    <figcaption>Creating an S3 lifecycle rule for expiration.</figcaption>
</figure>

<p>(Create a new &ldquo;lifecycle rule&rdquo; under the <code>Management</code> section of a bucket. The settings from there are all straightforward.)</p>

<p>With logging set up, you&rsquo;re ready to sync logs down and start using GoAccess.</p>

<h3 id="git-ergonomics" class="link"><a href="#git-ergonomics">Git ergonomics</a></h3>

<p>I have a <a href="https://github.com/brandur/logs">Git repository</a> that acts as a little analytical test bed for my logs. I don&rsquo;t commit any actual logs, but it contains a variety of scripts that provide easy shortcuts for frequently-used tasks.</p>

<p>Here&rsquo;s one that uses awscli to sync my logging bucket down locally:</p>

<pre><code class="language-sh">#!/bin/bash

aws s3 sync s3://&lt;logs_bucket&gt; logs-brandur/ --delete
</code></pre>

<p>So I can easily run:</p>

<pre><code class="language-sh">bin/sync
</code></pre>

<p>Here&rsquo;s another that starts GoAccess uses my standard logs location, with Gzipped logs streamed into it and filtered through a list of excluded paths that I don&rsquo;t want to see:</p>

<pre><code class="language-sh">#!/bin/bash

if [ &quot;$#&quot; -ne 1 ]; then
    echo &quot;usage: $0 &lt;site&gt;&quot;
    exit 1
fi

NUM_DAYS=30

files=(logs-brandur/$1/*)

# protects against degenerately large numbers of files in the directory
last_files=${files[@]: -3000}

gunzip -c $last_files | grep --line-buffered -v -E -f exclude_list.txt | goaccess - -p conf/goaccess.conf --keep-last $NUM_DAYS
</code></pre>

<p>Now, instead of that convoluted and impossible-to-remember invocation, I run:</p>

<pre><code class="language-sh">bin/terminal
</code></pre>

<h2 id="athena" class="link"><a href="#athena">Deeper introspection with SQL and Athena</a></h2>

<p>GoAccess is great, but it an be a little slow to sync logs down locally and boot up. And while it gives us all of the basic information that we care about, we&rsquo;re still captive to its rails. We can expand our use of analytics-via-logs by using <a href="https://aws.amazon.com/athena/">AWS Athena</a>, which gives us the ability to analyze our log data with arbitrary SQL at relatively low cost.</p>

<p>Athena is built on <a href="https://prestodb.io/">Presto</a>, an SQL engine specializing in large, distributed data. Unlike a traditional data warehouse, Presto doesn&rsquo;t need an online component where data is stored centrally &ndash; it&rsquo;s more than happy to spin itself up ad-hoc and read data as needed out of a collection of files stored on S3, like our CloudFront-generate access logs.</p>

<h3 id="schema" class="link"><a href="#schema">Schema</a></h3>

<p>Start, by creating a new Athena database from <a href="https://console.aws.amazon.com/console/home">AWS console</a>:</p>

<pre><code class="language-sql">CREATE DATABASE brandur_logs;
</code></pre>

<p>(By the way, don&rsquo;t try to use hyphens when naming things, or you will run into some of the most truly horrendous error messages ever written.)</p>

<p>Then, create a table within it that has the same structure as the Cloudfront logging format. Note that <code>LOCATION</code> statement at the end which specifies that the table&rsquo;s source is an S3 path.</p>

<pre><code class="language-sql">CREATE EXTERNAL TABLE IF NOT EXISTS logs_brandur.brandur_org (
  `date` DATE,
  time STRING,
  location STRING,
  bytes BIGINT,
  request_ip STRING,
  method STRING,
  host STRING,
  uri STRING,
  status INT,
  referrer STRING,
  user_agent STRING,
  query_string STRING,
  cookie STRING,
  result_type STRING,
  request_id STRING,
  host_header STRING,
  request_protocol STRING,
  request_bytes BIGINT,
  time_taken FLOAT,
  xforwarded_for STRING,
  ssl_protocol STRING,
  ssl_cipher STRING,
  response_result_type STRING,
  http_version STRING,
  fle_status STRING,
  fle_encrypted_fields INT,
  c_port INT,
  time_to_first_byte FLOAT,
  x_edge_detailed_result_type STRING,
  sc_content_type STRING,
  sc_content_len BIGINT,
  sc_range_start BIGINT,
  sc_range_end BIGINT
)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
LOCATION 's3://logs-brandur/brandur.org/'
TBLPROPERTIES ( 'skip.header.line.count'='2' );
</code></pre>

<p>(This query comes from the <a href="https://docs.aws.amazon.com/athena/latest/ug/cloudfront-logs.html">official Cloudfront-with-Athena docs</a>. Go there for a canonical version in case this one falls out of date.)</p>

<p>One downside is that the Athena interface is rough even by Amazon&rsquo;s low standards, but the fact that someone else will run a Presto cluster so that you don&rsquo;t have to, is a godsend. And we can fix the UI problem.</p>

<figure>
    <img alt="Querying via Athena's UI." class="overflowing" loading="lazy" src="/photographs/articles/minimal-analytics/athena-query.png" srcset="/photographs/articles/minimal-analytics/athena-query@2x.png 2x, /photographs/articles/minimal-analytics/athena-query.png 1x">
    <figcaption>Querying via Athena's UI.</figcaption>
</figure>

<h3 id="query-cli" class="link"><a href="#query-cli">Queries via CLI</a></h3>

<p>One of AWS&rsquo; best features is that it has a complete API for every service, and its reflected into commands in awscli, making it very easy to access and use. I have less-than-zero desire to touch Athena&rsquo;s web UI, so I wrote another <a href="https://github.com/brandur/logs/blob/master/bin/query">little script</a> that creates an Athena query, polls the API until it&rsquo;s finished, then shows the results in simple tabulated form. The script takes an <code>*.sql</code> file as input, so I can write SQL with nice syntax highlighting and completion in Vim, and have it version controlled in Git &ndash; two great features not available if using the vanilla Athena product.</p>

<pre><code class="language-sh">$ bin/query queries/brandur.org/unique_last_month.sql
</code></pre>

<p>Here&rsquo;s a query that maps over my Cloudfront logs to give me unique visitors per day over the last month:</p>

<pre><code class="language-sql">SELECT
    date_trunc('day', date) AS day,
    count(distinct(request_ip)) AS unique_visitors

FROM brandur_logs.brandur_org
WHERE status = 200
  AND date &gt; now() - interval '30' day

  -- filter out static files
  AND uri NOT LIKE '%.%'

  -- filter known robots (list copied from Goaccess and truncated for brevity)
  AND user_agent NOT LIKE '%AdsBot-Google%'
  AND user_agent NOT LIKE '%Googlebot%'
  AND user_agent NOT LIKE '%bingbot%'

GROUP BY 1
ORDER BY 1;

</code></pre>

<p>For a &ldquo;tiny&rdquo; data set like mine (on the order of 100 MB to GBs), Athena replies in seconds:</p>

<pre><code class="language-sh">$ bin/query queries/brandur.org/unique_last_month.sql
query execution id: 65df1113-b206-4fc0-b1d2-8ac8017cbc35

 + ---------- + --------------- +
 | day        | unique_visitors |
 + ---------- + --------------- +
 | 2021-01-17 | 624             |
 | 2021-01-18 | 801             |
 | 2021-01-19 | 820             |
 | 2021-01-20 | 806             |
 | 2021-01-21 | 824             |
 | 2021-01-22 | 866             |
 | 2021-01-23 | 743             |
 | 2021-01-24 | 692             |
 | 2021-01-25 | 947             |
 | 2021-01-26 | 808             |
 | 2021-01-27 | 894             |
 | 2021-01-28 | 860             |
 | 2021-01-29 | 781             |
 | 2021-01-30 | 599             |
 | 2021-01-31 | 627             |
 | 2021-02-01 | 817             |
 | 2021-02-02 | 879             |
 | 2021-02-03 | 835             |
 | 2021-02-04 | 886             |
 | 2021-02-05 | 1232            |
 | 2021-02-06 | 540             |
 | 2021-02-07 | 530             |
 | 2021-02-08 | 19599           |
 | 2021-02-09 | 14626           |
 | 2021-02-10 | 1934            |
 | 2021-02-11 | 1341            |
 | 2021-02-12 | 1148            |
 | 2021-02-13 | 809             |
 | 2021-02-14 | 888             |
 | 2021-02-15 | 901             |
 + ---------- + --------------- +
</code></pre>

<p>Here&rsquo;s one that shows me my most popular articles this month:</p>

<pre><code class="language-sql">SELECT
    uri,
    count(distinct(request_ip)) AS unique_visitors

FROM brandur_logs.brandur_org
WHERE status = 200
  AND date &gt; now() - interval '30' day

  -- filter out static files
  AND uri NOT LIKE '%.%'

  -- filter known robots (list copied from Goaccess and truncated for brevity)
  AND user_agent NOT LIKE '%AdsBot-Google%'
  AND user_agent NOT LIKE '%Googlebot%'
  AND user_agent NOT LIKE '%bingbot%'

GROUP BY 1
ORDER BY 2 DESC
LIMIT 20;
</code></pre>

<p>And again, it executes in seconds:</p>

<pre><code class="language-sh">$ bin/query queries/brandur.org/top_articles_last_month.sql
query execution id: 1830fea4-725d-4e73-ab53-0ffcff3a189f

 + ------------------------------------ + --------------- +
 | uri                                  | unique_visitors |
 + ------------------------------------ + --------------- +
 | /fragments/graceful-degradation-time | 32802           |
 | /                                    | 4854            |
 | /articles                            | 2090            |
 | /logfmt                              | 2016            |
 | /large-database-casualties           | 1726            |
 | /fragments                           | 1448            |
 | /postgres-connections                | 1393            |
 | /about                               | 1227            |
 | /photos                              | 942             |
 | /fragments/rss-abandon               | 821             |
 | /newsletter                          | 811             |
 | /idempotency-keys                    | 797             |
 | /go-worker-pool                      | 724             |
 | /twitter                             | 690             |
 | /fragments/homebrew-m1               | 645             |
 | /now                                 | 633             |
 | /fragments/test-kafka                | 611             |
 | /fragments/ffmpeg-h265               | 598             |
 | /elegant-apis                        | 575             |
 | /postgres-atomicity                  | 412             |
 + ------------------------------------ + --------------- +
</code></pre>

<p>The major outlier at the top shows the HN effect in action.</p>

<p>Athena is currently priced at $5 per TB of data scanned. That makes it quite cheap for a site like mine that generates on the order of 100 MB of logs per month, but it&rsquo;s worth thinking about if you&rsquo;re running something much larger. A side effect of the pricing is that it also means that it&rsquo;s cheaper if you retain data for shorter periods of time, thereby running analytics over less of it (and making your site more privacy-friendly).</p>

<p>(<a href="https://markmcgranaghan.com/cloudfront-analytics">Thanks to Mark</a> for inspiring this section of the post.)</p>

<h2 id="adblockers" class="link"><a href="#adblockers">How many adblockers?</a></h2>

<p>By comparing the results from my online analytic tools and those from these logging-based solutions, I can get a rough idea of how many of my visitors are using adblockers, and therefore invisible to analytics.</p>

<p>I&rsquo;m using my HN spike from last week as a good slice of time to measure across. Note that this analysis isn&rsquo;t perfectly scientific and certainly has some error bars, but I&rsquo;ve done my best to filter out robots, static files, and duplicate visits, so the magnitude should be roughly right.</p>

<figure>
    <img alt="GoatCounter's measurement of an HN traffic peak." class="overflowing" loading="lazy" src="/photographs/articles/minimal-analytics/goatcounter.png" srcset="/photographs/articles/minimal-analytics/goatcounter@2x.png 2x, /photographs/articles/minimal-analytics/goatcounter.png 1x">
    <figcaption>GoatCounter's measurement of an HN traffic peak.</figcaption>
</figure>

<p>Both Google Analytics and Goatcounter agreed that I got <strong>~13k unique visitors</strong> across the couple days where it spiked. GoAccess and my own custom Athena queries agreed that it was more like <strong>~33k unique visitors</strong>, giving me a rough ratio of <strong>2.5x</strong> more visitors than reported by analytics, and meaning that about <strong>60% of my readers are using an adblocker</strong>.</p>

<p>So while analytics tools are still useful for measuring across a  sample of visitors, they&rsquo;re not giving you the whole story, and that in itself is a good reason that you might want to consider dropping them, privacy concerns aside.</p>

<p>Personally, I think it&rsquo;s still fine to use the ones that are making an effort to be privacy-aware like Goatcounter, and they certainly yield benefits over analytics-by-logging like giving you JavaScript-only information like time spent on site and screen size, while being more convenient to look at.</p>
]]></content>
    <published>2021-02-16T15:16:18Z</published>
    <updated>2021-02-16T15:16:18Z</updated>
    <link href="https://brandur.org/minimal-analytics"></link>
    <id>tag:brandur.org,2021-02-16:minimal-analytics</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Feature Casualties of Large Databases</title>
    <summary>Databases shed important RDMS features as they get big. Examining why this tends to be the case, and some ideas for preventing it.</summary>
    <content type="html"><![CDATA[<p>Big data has an unfortunate tendency to get messy. A few years in, a growing database that use to be small, lean, and well-designed, has better odds than not of becoming something large, bloated, and with best practices tossed aside and now considered unsalvageable.</p>

<p>There&rsquo;s a few common reasons that this happens, some better than others:</p>

<ul>
<li><strong>Technological limitation:</strong> The underlying tech doesn&rsquo;t support the scale. Say transactions or referential integrity across partitions in a sharded system.</li>
<li><strong>Stability:</strong> Certain operations come to be considered too risky. e.g. Batch update operations that have unpredictable performance properties.</li>
<li><strong>Cost/effort:</strong> Doing things the right way is too hard or too expensive. e.g. Back-migrating a large amount of existing data.</li>
<li><strong>Convenience:</strong> Similar to the previous point, poor data practice is simply by far the easiest thing to do, and gets your immediate project shipped more quickly, even if it makes future projects more difficult.</li>
</ul>

<p>The loss of these features is unfortunate because they&rsquo;re the major reason we&rsquo;re using sophisticated databases in the first place. In the <a href="https://eng.uber.com/schemaless-part-one-mysql-datastore/">most extreme cases</a>, advanced databases end up as nothing more than glorified key/value stores, and the applications they power lose important foundations for reliability and correctness.</p>

<h2 id="casualties" class="link"><a href="#casualties">The casualties of large applications/data</a></h2>

<h3 id="transactions" class="link"><a href="#transactions">Transactions</a></h3>

<p>ACID transactions tend to be one of the first things to go, especially since the value they provide isn&rsquo;t immediately obvious in a new system that&rsquo;s not yet seeing a lot of traffic or trouble. Between that and the facts that they add some friction in writing code quickly, and can lead to locking problems in production mean that they&rsquo;re often put in the chopping block early, especially when less experienced engineers are involved.</p>

<p>Losing transactions is bad news for an applications future operability, but as this subject&rsquo;s already covered extensively elsewhere (<a href="/acid">including by me</a>), I won&rsquo;t go into depth here.</p>

<h3 id="referential-integrity" class="link"><a href="#referential-integrity">Referential integrity</a></h3>

<p>Referential integrity guarantees that if a key exists somewhere in a database, then the object its referencing does as well. Foreign keys allow developers to control deletions such that if an object is being removed, but is still referenced, than that deletion should be blocked (<code>ON DELETE RESTRICT</code>), or, that referencing objects should be removed with it (<code>ON DELETE CASCADE</code>).</p>

<p>It&rsquo;s a powerful tool for correctness &ndash; having the database enforcing certain rules makes programs easier to write and easier to get right. <em>Not</em> having it tends to bleed out into code. Suddenly anytime a referenced object is loaded <em>anywhere</em>, the case that it came up without a result must be handled:</p>

<pre><code class="language-ruby">user = User.load(api_key.user_id)
if !user
  raise ObjectNotFound, &quot;couldn't find user!&quot;
end
</code></pre>

<p>Sacrificing referential integrity is rationalized away in a number of ways. Sometimes it&rsquo;s due to technological limitation, sometimes due to reliability concerns (a benign-looking delete triggering an unexpectedly large cascade), but more often it&rsquo;s for the simple-and-not-good reason that maintaining good hygiene around foreign key relations takes discipline and work.</p>

<h3 id="nullable" class="link"><a href="#nullable">Nullable, as far as the eye can see</a></h3>

<p>Relations in large databases tend to have a disproportionate number of nullable fields. This is a problem because in application code it&rsquo;s more difficult to work with objects that have a poorly defined schema. Every nullable field needs to be examined independently, and a fallback designed for it in case it didn&rsquo;t have a value. This takes time and introduces new avenues for bugs.</p>

<p>There&rsquo;s a few reasons that nullable-by-default is so common. The simplest is simply that nullable columns are literally the default in DDL &ndash; you&rsquo;ll get one unless you&rsquo;re really thinking about what you&rsquo;re doing and explicitly use <code>NOT NULL</code>.</p>

<p>A more common reason is that non-nullable columns often require that existing data be migrated, which is difficult, time consuming, and maybe even operationally fraught on nodes which are running very hot and which a migration unexpectedly pushes over the edge.</p>

<p>Lastly, there are often technological limitations as well. In Postgres for example, even after running a migration, taking that last step of changing a nullable column to non-nullable (<code>SET NOT NULL</code>) isn&rsquo;t safe. Postgres needs to verify that there are no nulls in the table, requiring a full table scan that blocks other operations. On a small table that&rsquo;ll run in an instant. On a large one, it could be the downfall of production <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>.</p>

<h3 id="indexing" class="link"><a href="#indexing">Suboptimal indexing</a></h3>

<p>Indexes are the easiest thing in the world to work with until they&rsquo;re not. In a large system, they might get complicated because:</p>

<ul>
<li>They need to be built on multiple clusters instead of just one.</li>
<li>Building them on very hot nodes gets risky as the build interferes with production operations. Internal teams may need to build tools to throttle or pause builds.</li>
<li>Data gets so large that building them takes a long time.</li>
<li>Data gets so large that each index is a significant non-trivial cost to store.</li>
</ul>

<p>Reduced performance is the most obvious outcome, but expensive index operations can have less obvious ones too. I worked on a project recently where product design was being driven by whether options would necessitate raising a new index on a particularly enormous collection which would take weeks and cost a large figure every year in storage costs alone.</p>

<h3 id="restricted-apis" class="link"><a href="#restricted-apis">Dangerous queries and restricted APIs</a></h3>

<p>SQL is the most expressive language ever for querying and manipulating data, and in the right hands, that power can make hard things easy.</p>

<p>However, the more complex the SQL statement, the more likely it is to impact production through problems like unpredictable performance or unanticipated locking. A common solution is for storage teams to simply ban non-trivial SQL wholesale, and constrain developers to a vastly simplified API &ndash; e.g. single row select, multi row select with index hint, single row update, single row delete.</p>

<pre><code class="language-ruby"># a simplified storage API
def insert(data:); end
def delete_one(id:); end
def load_many(predicate:, index:, limit:); end
def load_one(id:); end
def update_one(id:, data:); end
</code></pre>

<p>At a previous job, our MySQL DBA banned any database update that affected more than one row, even where it would be vastly beneficial to performance, due to concerns around them delaying replication to secondaries. This might have helped production, but had the predictable effect of reduced productivity along with some truly heinous workarounds for things that should have been trivial, and which instead resulted in considerable tech debt.</p>

<p>Where I work now, even with the comparative unexpressiveness of Mongo compared to SQL, every select in the system must be named and statically defined along with an index it expects to use. This is so that we can verify at build time that the appropriate index is already available in production.</p>

<h2 id="scalability-ideas" class="link"><a href="#scalability-ideas">Ideas for scalability</a></h2>

<p>There&rsquo;s a divide between the engineers who run big production systems and the developers who work on open-source projects in the data space, with neither group having all that much visibility into the other. Engineers who run big databases tend to adopt a nihilist outlook that every large installation inevitably trends towards a key/value store &ndash; at a certain point, the niceties available to smaller databases must get the axe. Open-source developers don&rsquo;t tend to value highly the features that would help big installations.</p>

<p>I don&rsquo;t think the nihilist viewpoint should be the inevitable outcome, and there&rsquo;s cause for optimism in the development of systems like Citus, Spanner, and CockroachDB, which enable previously difficult features like cross shard transactions. We need even more movement in that direction.</p>

<p>There&rsquo;s a variety of possible operations-friendly features that might be possible to counteract the entropic dumbing down of large databases. Some ideas:</p>

<ul>
<li>Make index builds pauseable so that they can be easily throttled in emergencies.</li>
<li>Make it easy to make a nullable field non-nullable, <em>not</em> requiring a problematic and immediate full table scan.</li>
<li>A &ldquo;strict&rdquo; SQL dialect that makes specifying fields as <code>NOT NULL</code> default, and specifying foreign keys required.</li>
<li>A communication protocol that allows the query to signal out-of-band with a query&rsquo;s results that it didn&rsquo;t run particularly efficiently, say that it got results but wasn&rsquo;t able to make use of an index. This would allow a test suite to fail early by signaling the problem to a developer instead of finding out about it in production.</li>
<li>A migrations framework built into the database itself that makes migrations easier and faster to write while also guaranteeing stability by allowing long-lived migration-related queries to be deprioritized and paused if necessary.</li>
</ul>

<p>Ideally, we get to a place where large databases enjoy all the same benefits as smaller ones, and we all get to reap the benefits of software that gets more stable and more reliable as a result.</p>


]]></content>
    <published>2020-12-01T20:06:51Z</published>
    <updated>2020-12-01T20:06:51Z</updated>
    <link href="https://brandur.org/large-database-casualties"></link>
    <id>tag:brandur.org,2020-12-01:large-database-casualties</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Doubling the Sorting Speed of Postgres Network Types with Abbreviated Keys</title>
    <summary>Making the sorting speed of network types in Postgres twice as fast by designing SortSupport abbreviated keys compatible with their existing sort semantics.</summary>
    <content type="html"><![CDATA[<p>A few months ago, I wrote about <a href="/sortsupport">how SortSupport works in
Postgres</a> to vastly speed up sorting on
large data types <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup> like <code>numeric</code> or <code>text</code>, and
<code>varchar</code>. It works by generating <strong>abbreviated keys</strong> for
values that are representative of them for purposes of
sorting, but which fit nicely into the pointer-sized value
(called a &ldquo;<strong>datum</strong>&rdquo;) in memory that Postgres uses for
sorting. Most values can be sorted just based on their
abbreviated key, saving trips to the heap and increasing
sorting throughput. Faster sorting leads to speedup on
common operations like <code>DISTINCT</code>, <code>ORDER BY</code>, and <code>CREATE
INDEX</code>.</p>

<p>A <a href="https://www.postgresql.org/message-id/CABR_9B-PQ8o2MZNJ88wo6r-NxW2EFG70M96Wmcgf99G6HUQ3sw%40mail.gmail.com">patch</a> of mine was recently committed to add
SortSupport for the <code>inet</code> and <code>cidr</code> types, which by my
measurement, a little more than doubles sorting speed on
them. <code>inet</code> and <code>cidr</code> are the types used to store network
addresses or individual hosts and in either IPv4 or IPv6
(they generally look something like <code>1.2.3.0/24</code> or
<code>1.2.3.4</code>).</p>

<p><code>inet</code> and <code>cidr</code> have some important subtleties in how
they&rsquo;re sorted which made designing an abbreviated key that
would be faithful to those subtleties but still efficient,
a non-trivial problem. Because their size is limited,
abbreviated keys are allowed to show equality even for
values that aren&rsquo;t equal (Postgres will fall back to
authoritative comparison to confirm equality or tiebreak),
but they should never falsely indicate inequality.</p>

<h2 id="inet-cidr" class="link"><a href="#inet-cidr">Network type anatomy, and inet vs. cidr</a></h2>

<p>A property that&rsquo;s not necessarily obvious to anyone
unfamiliar with them is that network types (<code>inet</code> or
<code>cidr</code>) can either address a single host (what most people
are used to seeing) or an entire subnetwork of arbitrary
size. For example:</p>

<ul>
<li><p><code>1.2.3.4/32</code> specifies a 32-bit netmask on an IPv4 value,
which is 32 bits wide, which means that it defines
exactly one address: <code>1.2.3.4</code>. <code>/128</code> would work
similarly for IPv6.</p></li>

<li><p><code>1.2.3.0/24</code> specifies a 24-bit netmask. It identifies
the network at <code>1.2.3.*</code>. The last byte may be anywhere
in the range of 0 to 255.</p></li>

<li><p>Similarly, <code>1.0.0.0/8</code> specifies an 8-bit netmask. It
identifies the much larger possible network at <code>1.*</code>.</p></li>
</ul>

<p>We&rsquo;ll establish the following common vocabulary for each
component of an address (and take for example the value
<code>1.2.3.4/24</code>):</p>

<ol>
<li>A <strong>network</strong>, or bits in the netmask (<code>1.2.3.</code>).</li>
<li>A <strong>netmask size</strong> (<code>/24</code> which is 24 bits). Dictates
the number of bits in the network.</li>
<li>A <strong>subnet</strong>, or bits outside of the netmask (<code>.4</code>).
Only <code>inet</code> carries non-zero bits here, and combined
with the network, they identify a single <strong>host</strong>
(<code>1.2.3.4</code>).</li>
</ol>

<p>The netmask size is a little more complex than commonly
understood because while it&rsquo;s most common to see byte-sized
blocks like <code>/8</code>, <code>/16</code>, <code>/24</code>, and <code>/32</code>, it&rsquo;s allowed to
be any number between 0 and 32. It&rsquo;s easy to mentally
extract a byte-sized network out of a value (like <code>1.2.3.</code>
out of <code>1.2.3.4/24</code>) because you can just stop at the
appropriate byte boundary, but when it&rsquo;s not a nice byte
multiple you have to think at the binary level. For
example, if I have the value <code>255.255.255.255/1</code>, the
network is just the leftmost bit. 255 in binary is <code>1111
1111</code>, so the network is the bit <code>1</code> and the subnet is 31
consecutive <code>1</code>s.</p>

<figure>
    <img alt="The anatomy of inet and cidr values." class="overflowing" loading="lazy" src="/assets/images/sortsupport-inet/inet-cidr-anatomy.svg">
    <figcaption>The anatomy of inet and cidr values.</figcaption>
</figure>

<p>The difference between <code>inet</code> and <code>cidr</code> is that <code>inet</code>
allows a values outside of the netmasked bits. The value
<code>1.2.3.4/24</code> is possible in <code>inet</code>, but illegal in <code>cidr</code>
because only zeroes may appear after the network like
<code>1.2.3.0/24</code>. They&rsquo;re nearly identical, with the latter
being more strict.</p>

<p>In the Postgres source code, <code>inet</code> and <code>cidr</code> are
represented by the same C struct. Here it is in
<a href="https://github.com/postgres/postgres/blob/12afc7145c03c212f26fea3a99e016da6a1c919c/src/include/utils/inet.h:23"><code>inet.h</code></a>:</p>

<pre><code class="language-c">/*
 * This is the internal storage format for IP addresses
 * (both INET and CIDR datatypes):
 */
typedef struct
{
    unsigned char family;      /* PGSQL_AF_INET or PGSQL_AF_INET6 */
    unsigned char bits;        /* number of bits in netmask */
    unsigned char ipaddr[16];  /* up to 128 bits of address */
} inet_struct;
</code></pre>

<h2 id="sorting-rules" class="link"><a href="#sorting-rules">Sorting rules</a></h2>

<p>In Postgres, <code>inet</code>/<code>cidr</code> sort according to these rules:</p>

<ol>
<li>IPv4 always appears before IPv6.</li>
<li>The bits in the network are compared (<code>1.2.3.</code>).</li>
<li>Netmask size is compared (<code>/24</code>).</li>
<li>All bits are compared. Having made it here, we know that
the network bits are equal, so we&rsquo;re in effect just
comparing the subnet (<code>.4</code>).</li>
</ol>

<p>These rules combined with the fact that we&rsquo;re working at
the bit level produces ordering that in cases may not be
intuitive. For example, <code>192.0.0.0/1</code> sorts <em>before</em>
<code>128.0.0.0/2</code> despite 192 being the larger number &ndash; when
comparing them, we start by looking at the common bits
available in both networks, which comes out to just one bit
(<code>min(/1, /2)</code>). That bit is the same in the networks of
both values (remember, 192 = <code>1100 0000</code> and 128 = <code>1000
0000</code>), so we fall through to comparing netmask size. <code>/2</code>
is the larger of the two, so <code>128.0.0.0/2</code> is the larger
value.</p>

<h2 id="designing-keys" class="link"><a href="#designing-keys">Designing an abbreviated key</a></h2>

<p>Armed with the structure of <code>inet</code>/<code>cidr</code> and how their
sorting works, we can now design an abbreviated key for
them. Remember that abbreviated keys need to fit into the
pointer-sized Postgres datum &ndash; either 32 or 64 bits
depending on target architecture. The goal is to pack in as
much sorting-relevant information as possible while staying
true to existing semantics.</p>

<p>We&rsquo;ll be breaking the available datum into multiple parts,
with information that we need for higher precedence sorting
rules occupying more significant bits so that it compares
first. This allows us to compare any two keys as integers
&ndash; a very fast operation for CPUs (faster even than
comparing memory byte-by-byte), and also a common technique
in other abbreviated key implementations like the one for
<a href="/sortsupport#uuid">UUIDs</a>.</p>

<h3 id="family" class="link"><a href="#family">1 bit for family</a></h3>

<p>The first part is easy: all IPv4 values always appear
before all IPv6 values. Since there&rsquo;s only two IP families,
so we&rsquo;ll reserve the most significant bit of our key to
represent a value&rsquo;s family. 0 for IPv4 and 1 for IPv6.</p>

<figure>
    <img alt="One bit reserved for IP family." class="overflowing" loading="lazy" src="/assets/images/sortsupport-inet/ip-family.svg">
    <figcaption>One bit reserved for IP family.</figcaption>
</figure>

<p>It might seem short-sighted that we&rsquo;re assuming that only
two IP families will ever exist, but luckily abbreviated
keys are not persisted to disk (only in the memory of a
running Postgres system) and their format is therefore
non-binding. If a new IP family were to ever appear, we
could allocate another bit to account for it.</p>

<h3 id="network" class="link"><a href="#network">As many network bits as we can pack in</a></h3>

<p>The next comparison that needs to be done is against a
value&rsquo;s network bits, so we should include those in the
datum.</p>

<p>The less obvious insight is that we can <em>only</em> include
network bits in this part. Think back to our example of
<code>192.0.0.0/1</code> and <code>128.0.0.0/2</code>: if we included 192&rsquo;s full
bits of <code>1100 0000</code>, then when comparing it to 128&rsquo;s <code>1000
0000</code>, it would sort higher when it needs to come out
lower. In order to guarantee our keys will comply with the
rules, we have to truncate values to just what appears in
the network.</p>

<p>Both <code>192.0.0.0/1</code> and <code>128.0.0.0/2</code> would appear as <code>1000
0000</code> (two of 128&rsquo;s bits were extracted, but it has a 0 in
the second position) and would appear equal when
considering this part of the abbreviated key. In cases
where that&rsquo;s all the space in the key we have to work with,
Postgres will have to fall back to authoritative comparison
(which would be able to move on and compare netmask size)
to break the tie.</p>

<p>The network bits are where we need to stop for most of our
use cases because that&rsquo;s all the space in the datum there
is. An IPv6 value is 128 bits &ndash; after reserving 1 bit in
the datum for family, we have 31 bits left on a 32-bit
machine and 63 bits on a 64-bit machine, which will be
filled entirely with network. An IPv4 value is only 32
bits, but that&rsquo;s still more space than we have left on a
32-bit machine, so again, we&rsquo;ll pack in 31 of them.</p>

<figure>
    <img alt="Number of bits available to store network per datum size and IP family." class="overflowing" loading="lazy" src="/assets/images/sortsupport-inet/network-bits.svg">
    <figcaption>Number of bits available to store network per datum size and IP family.</figcaption>
</figure>

<p>But there is one case where we have some space left over:
IPv4 on a 64-bit machine. Even after storing all 32
possible bits of network, there&rsquo;s still 31 bits available.
Let&rsquo;s see what we can use them for.</p>

<h3 id="ipv4-64bit" class="link"><a href="#ipv4-64bit">IPv4 on 64-bit: network size and a few subnet bits</a></h3>

<p>As datums are being compared for IPv4 on a 64-bit machine,
we can be sure that having looked at the 33 bits that
we&rsquo;ve designed so far &ndash; IP family (1 bit) and
network (32 bits) &ndash; are equal. That leaves us with 31
bits (64 - 33) left to work with, and lets us move onto
the next comparison rule &ndash; netmask size. The largest
possible netmask size for an IPv4 address is 32, which
conveniently fits into only 6 bits (<code>32 = 10 0000</code>) <sup id="footnote-2-source"><a href="#footnote-2">2</a></sup>.</p>

<p>After adding netmask size to the datum we&rsquo;re left with 25
bits (31 - 6), which we can use for the next sorting rule
&ndash; subnet. Subnets can be as large as 32 bits for a <code>/0</code>
value, so we&rsquo;ll have to shift any that are too large to fit
down to the size available. That will only ever happen for
netmask sizes of <code>/6</code> or smaller &ndash; for all commonly seen
netmask sizes like <code>/8</code>, <code>/16</code>, or <code>/24</code> we can fit the
entirety of the subnet into the datum.</p>

<p>With subnet covered, we&rsquo;ve used up all the available key
bits, but also managed to cover every sorting rule &ndash; with
most <sup id="footnote-3-source"><a href="#footnote-3">3</a></sup> real-world data, Postgres should be able to sort
almost entirely with abbreviated keys without falling back
to authoritative comparison. The final key design looks
like this:</p>

<figure>
    <img alt="The design of abbreviated keys for inet and cidr." class="overflowing" loading="lazy" src="/assets/images/sortsupport-inet/key-design.svg">
    <figcaption>The design of abbreviated keys for inet and cidr.</figcaption>
</figure>

<h2 id="gymnastics" class="link"><a href="#gymnastics">Bit gymnastics in C</a></h2>

<p>Now that we have an encoding scheme for each different
case, we can build an implementation that puts everything
into place. This involves the use of many bitwise
operations that are common in C, but which many of us who
program in high-level languages day-to-day aren&rsquo;t as used
to.</p>

<p>I&rsquo;ll go through this implementation step-by-step, but you
may prefer to refer to the completed version in the
<a href="https://github.com/postgres/postgres/blob/12afc7145c03c212f26fea3a99e016da6a1c919c/src/backend/utils/adt/network.c#L561">Postgres source</a>, which we&rsquo;ve made an effort to
comment comprehensively.</p>

<h3 id="integer" class="link"><a href="#integer">Ingesting bytes as an integer</a></h3>

<p>Recall that an IP component is stored as a 16-byte
<code>unsigned char</code> array in the backing network type:</p>

<pre><code class="language-c">typedef struct
{
    ...
    unsigned char ipaddr[16];  /* up to 128 bits of address */
} inet_struct;
</code></pre>

<p>Our abbreviated keys will be compared as if they were
integers (one of the reasons that they&rsquo;re so fast), so the
first step is to extract a datum&rsquo;s worth of bytes from
<code>ipaddr</code> into an intermediate representation that&rsquo;ll be
used to more easily separate out the final components.
We&rsquo;ll use <code>memcpy</code> to copy it out byte-by-byte:</p>

<pre><code class="language-c">Datum ipaddr_datum;
memcpy(&amp;ipaddr_datum, ip_addr(authoritative), sizeof(Datum));
</code></pre>

<p><code>ipaddr</code> is laid out most significant byte first, which
will be fine when representing an integer on a big-endian
machine, but no good on one that&rsquo;s little-endian (like most
of our Intel processors), so do a byte-wise position swap
to re-form it (more detail on this talking about <a href="/sortsupport#uuid"><code>uuid</code>&rsquo;s
abbreviated key implementation</a>:</p>

<pre><code class="language-c">/* Must byteswap on little-endian machines */
ipaddr_datum = DatumBigEndianToNative(ipaddr_datum);
</code></pre>

<p>And for IPv6, make sure to shift a 1 bit into the leftmost
position so that it sorts after all IPv4 values:</p>

<pre><code>Datum res;
res = ((Datum) 1) &lt;&lt; (SIZEOF_DATUM * BITS_PER_BYTE - 1);
</code></pre>

<h3 id="network-bitmask" class="link"><a href="#network-bitmask">Extracting network via bitmask</a></h3>

<p>Next we&rsquo;ll extract the leading <strong>network</strong> component using a
technique called bitmasking. This common technique involves
using a bitwise-AND to extract a desired range of bits:</p>

<pre><code>  1010 1010 1010 1010       (original value)
&amp; 0000 1111 1111 0000       (bitmask)
  -------------------
  0000 1010 1010 0000       (final result)
</code></pre>

<p>We&rsquo;re going to create a bitmask for the <strong>subnet</strong> portion
of the value (reminder: that&rsquo;s the last part <em>after</em> the
network), and it&rsquo;s size depends on how many subnet bits we
expect to see in <code>ipaddr_datum</code>. For example, if the
network component occupies bits equal or greater to the
datum&rsquo;s size, then the subnet bitmask will be zero.</p>

<p>The code&rsquo;s broken into three separate conditionals. This
first section handles the case of no bits in the network
components. The subnet bitmask should be all ones, which we
get by starting with 0, subtracting 1, and allowing the
value to roll over to its maximum value:</p>

<pre><code class="language-c">Datum subnet_bitmask,
      network;

subnet_size = ip_maxbits(authoritative) - ip_bits(authoritative);
Assert(subnet_size &gt;= 0);

if (ip_bits(authoritative) == 0)
{
    /* Fit as many ipaddr bits as possible into subnet */
    subnet_bitmask = ((Datum) 0) - 1;
    network = 0;
}
</code></pre>

<p>The next section is the case where there are some bits for
both the network and subnet. We use a trick to get the
bitmask which involves shifting a 1 left out by the subnet
size, then subtracting one to get 1s in all positions that
were right of it:</p>

<pre><code>  0000 0001 0000 0000       (1 &lt;&lt; 8)
-                   1       (minus one)
  -------------------
  0000 0000 1111 1111       (8-bit mask)
</code></pre>

<p>Getting the network&rsquo;s value then involves ANDing the IP&rsquo;s
datum and the <em>negated</em> form of the subnet bitmask
(<code>ipaddr_datum &amp; ~subnet_bitmask</code>):</p>

<pre><code class="language-c">else if (ip_bits(authoritative) &lt; SIZEOF_DATUM * BITS_PER_BYTE)
{
    /* Split ipaddr bits between network and subnet */
    subnet_bitmask = (((Datum) 1) &lt;&lt; subnet_size) - 1;
    network = ipaddr_datum &amp; ~subnet_bitmask;
}
</code></pre>

<p>The final case represents no bits in the subnet. Set
<code>network</code> to the full value of <code>ipaddr_datum</code>:</p>

<pre><code class="language-c">else
{
    /* Fit as many ipaddr bits as possible into network */
    subnet_bitmask = 0;        /* Unused, but be tidy */
    network = ipaddr_datum;
}
</code></pre>

<h3 id="shifting" class="link"><a href="#shifting">Shifting things into place for IPv4 on 64-bit</a></h3>

<p>Recall that IPv4 on a 64-bit architecture is by far the
most complex case because we have room to fit a lot more
information. This next section involves taking the network
and subnet bitmask that we resolved above and shifting it
all into place.</p>

<p>The order of operations is:</p>

<ol>
<li><code>network</code>: Shift the network left 31 bits to make room
for netmask size and 25 bits worth of subnet.</li>
<li><code>network_size</code>: Shift the network size left 25 bits to
make room for the subnet.</li>
<li><code>subnet</code>: Extract a subnet using the bitmask calculated
above.</li>
<li><code>subnet</code>: If the subnet is longer than 25 bits, shift it
down to just occupy 25 bits.</li>
<li><code>res</code>: Get a final result by ORing the values from (1),
(2), and (4) above.</li>
</ol>

<pre><code class="language-c">#if SIZEOF_DATUM == 8
    if (ip_family(authoritative) == PGSQL_AF_INET)
    {
        /*
         * IPv4 with 8 byte datums: keep all 32 netmasked bits, netmask size,
         * and most significant 25 subnet bits
         */
        Datum        netmask_size = (Datum) ip_bits(authoritative);
        Datum        subnet;

        /* Shift left 31 bits: 6 bits netmask size + 25 subnet bits */
        network &lt;&lt;= (ABBREV_BITS_INET4_NETMASK_SIZE +
                     ABBREV_BITS_INET4_SUBNET);

        /* Shift size to make room for subnet bits at the end */
        netmask_size &lt;&lt;= ABBREV_BITS_INET4_SUBNET;

        /* Extract subnet bits without shifting them */
        subnet = ipaddr_datum &amp; subnet_bitmask;

        /*
         * If we have more than 25 subnet bits, we can't fit everything. Shift
         * subnet down to avoid clobbering bits that are only supposed to be
         * used for netmask_size.
         *
         * Discarding the least significant subnet bits like this is correct
         * because abbreviated comparisons that are resolved at the subnet
         * level must have had equal subnet sizes in order to get that far.
         */
        if (subnet_size &gt; ABBREV_BITS_INET4_SUBNET)
            subnet &gt;&gt;= subnet_size - ABBREV_BITS_INET4_SUBNET;

        /*
         * Assemble the final abbreviated key without clobbering the ipfamily
         * bit that must remain a zero.
         */
        res |= network | netmask_size | subnet;
    }
    else
#endif
</code></pre>

<h3 id="everything-else" class="link"><a href="#everything-else">Everything else</a></h3>

<p>The three other cases (refer to the figure above) are much
simpler because we only have room for network bits. Shift
them right by 1 bit to not clobber our previously set IP
family, then OR with <code>res</code> for the final result:</p>

<pre><code class="language-c">#endif
    {
        /*
         * 4 byte datums, or IPv6 with 8 byte datums: Use as many of the
         * netmasked bits as will fit in final abbreviated key. Avoid
         * clobbering the ipfamily bit that was set earlier.
         */
        res |= network &gt;&gt; 1;
    }
</code></pre>

<h2 id="speed-vs-sustainability" class="link"><a href="#speed-vs-sustainability">Speed vs. sustainability</a></h2>

<p>The abbreviated key implementation here is complex enough
that in most contexts I&rsquo;d probably consider it a poor trade
off &ndash; added speed is nice to have, but there is a cost in
the ongoing maintenance burden of the new code and its
understandability by future contributors.</p>

<p>However, Postgres is a highly leveraged piece of software.
This patch makes sorting and creating indexes on network
types <em>~twice as fast</em>, and that improvement will trickle
down automatically to hundreds of thousands of Postgres
installations around the world as they&rsquo;re upgraded to the
next major version. If there&rsquo;s one place where trading some
more complexity for speed is worth it, it&rsquo;s cases like this
one where only very few have to understand the code, but
very many will reap its benefits. We&rsquo;ve also made sure to
add extensive comments and test cases to keep future code
changes as easy as they can be.</p>

<p>Thanks to Peter Geoghegan for seeding the idea for this
patch, as well as for advice and very thorough
testing/review, and Edmund Horner for review.</p>


]]></content>
    <published>2019-08-07T16:50:44Z</published>
    <updated>2019-08-07T16:50:44Z</updated>
    <link href="https://brandur.org/sortsupport-inet"></link>
    <id>tag:brandur.org,2019-08-07:sortsupport-inet</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Sequences: A Modest, Contra-garden Travel Project</title>
    <summary>In the spirit of the continued production of independent content, I&amp;rsquo;m running a two-week photography and writing project in Berlin called &lt;em&gt;Sequences&lt;/em&gt;.</summary>
    <content type="html"><![CDATA[<p>Many of us in technology-adjacent circles have been
grumbling for years about the continued centralization of
the web. The overwhelming trend has been for content to
gravitate towards the internet&rsquo;s Great Walled Gardens &ndash;
Facebook, YouTube, Medium, and company. As time marches
on, not only are those gardens getting bigger, but their
walls are growing higher. There was a time that those
platforms made motions in the direction of openness &ndash; XMPP
support in Hangouts or Messenger for example &ndash; but
interoperability on their part is a fading relic of a more
idealistic past.</p>

<p>It&rsquo;s not clear what it&rsquo;ll take to break the walls back
down, and we&rsquo;re not likely to bring back the exploratory,
I-have-no-idea-what-I&rsquo;m-doing culture of the 90s &amp; early
2000s, but a necessity to renewed decentralization is the
continued production of content that lives outside those
walls. A fond memory of the earlier days of the internet
was the experimentation &ndash; people building new sites and
projects in every medium from writing to Photoshop to
interactive Flash <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>.</p>

<p>In that spirit <sup id="footnote-2-source"><a href="#footnote-2">2</a></sup>: I just arrived in Berlin, and am
running a tiny photography and writing project called
<strong><em>Sequences</em></strong> while I&rsquo;m here. The format is a photograph
every day, accompanied by a few words that I&rsquo;ll try to keep
interesting. Think of it like an isolate, self-hosted, Ye
Olden Days version of Instagram. No likes, comments, or
react-ji &ndash; just a channel from me to the open web in the
hopes that I can show you some cool stuff.</p>

<p>I&rsquo;m publishing everything via <a href="/sequences.atom" class="feed-icon">feed</a> (empty for the next ~day). For anyone like me who fell off the RSS wagon after the implosion of Google Reader, I&rsquo;ll be <a href="https://twitter.com/brandur" class="twitter-icon">tweeting</a> new entries.</p>

<figure>
    <img alt="A sequences sample page: a large vista with some text accompaniment. All independent." class="overflowing" loading="lazy" src="/assets/images/sequences-project/sample.png" srcset="/assets/images/sequences-project/sample@2x.png 2x, /assets/images/sequences-project/sample.png 1x">
    <figcaption>A sequences sample page: a large vista with some text accompaniment. All independent.</figcaption>
</figure>


]]></content>
    <published>2019-06-04T21:54:26Z</published>
    <updated>2019-06-04T21:54:26Z</updated>
    <link href="https://brandur.org/sequences-project"></link>
    <id>tag:brandur.org,2019-06-04:sequences-project</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Building a Robust Live Reloader with WebSockets and Go</title>
    <summary>A walkthrough of the design of a live reload feature for the static site generator that builds this site, touching on fsnotify, WebSockets, and the curious case of file 4913.</summary>
    <content type="html"><![CDATA[<p>For the last couple weeks I&rsquo;ve been making a few upgrades
to the backend that generates this site
(<a href="/aws-intrinsic-static">previously</a>), with an aim on rebuilding it to
be faster, more stable, and more efficient. The source is
custom, and it&rsquo;d accumulated enough cruft over the years
through a hundred incremental augmentations to justify a
little love.</p>

<p>I recently used <a href="https://gohugo.io/">Hugo</a> for a few
projects, another static site generate well-known for being
one of the first written in Go, and fell in love with one
of its features: live reloading. As a file changes in the
generator&rsquo;s development mode and a build is triggered, live
reloading signals any web browsers open to the site to
reload their content. Here&rsquo;s a video of it in action:</p>

<figure>
  <p>
    <video controls class="overflowing">
      <source src="/assets/images/live-reload/live-reload.h264.mp4" type="video/mp4">
    </video>
  </p>
  <figcaption>A short video of the live reload feature in
    action: changes saved in the editor show up immediately 
    in the browser.</figcaption>
</figure>

<p>It&rsquo;s hard to be convinced just reading about it &ndash; it
doesn&rsquo;t seem like a big deal to just âŒ˜-<code>Tab</code> over to the
browser and âŒ˜-<code>R</code> for a refresh &ndash; but the first time you
try it, it&rsquo;s hard not to get addicted. Although only a tiny
quality of life improvement, it&rsquo;s one that makes the
writing experience much more fluid. And where it&rsquo;s good for
writing, it&rsquo;s <em>wonderful</em> for design, where it&rsquo;s common to
make minor tweaks to CSS properties one at a time by the
hundreds to get everything looking exactly right.</p>

<p>I decided to write my own live reloading implementation and
was pleasantly surprised by how easy it turned out to be.
The libraries available for Go to use as primitives were
robust, and nicely encapsulated complicated concerns into
simple APIs. Browser-level technologies like WebSockets are
now reliable and ubiquitous enough to lend themselves to an
easy client-side implementation with minimal fuss &ndash; just a
few lines of basic JavaScript. No transpiling, no
polyfills, no heavy build pipeline, no mess.</p>

<p>Here&rsquo;s a short tour of the design.</p>

<h2 id="fsnotify" class="link"><a href="#fsnotify">Watching for changes with fsnotify</a></h2>

<p>The first piece of the puzzle is knowing when source files
change so that we can signal a reload. Luckily Go has a
great library for this in <a href="https://github.com/fsnotify/fsnotify">fsnotify</a>, which hooks
into operating system monitoring primitives and notifies a
program over a channel when a change is detected. Basic
usage is as simple as adding directories to a watcher and
listening on a channel:</p>

<pre><code class="language-go">watcher, err := fsnotify.NewWatcher()
...

err = watcher.Add(&quot;./content&quot;)
...

for {
    select {
        case event := &lt;-watcher.Events:
            log.Println(&quot;event:&quot;, event)
    }
}
</code></pre>

<p>When something in the <code>content</code> directory changes, the
program above emits a message like this one:</p>

<pre><code>2019/05/21 11:49:32 event: &quot;./content/hello.md&quot;: WRITE
</code></pre>

<h3 id="vim" class="link"><a href="#vim">Saving files in Vim, and the curious case of 4913</a></h3>

<p>Now things are <em>almost</em> that easy, but a few practical
considerations complicate things a little.</p>

<p>While saving a file in Vim (for example, but other editors
may behave similarly), instead of an ideal single write
event being emitted, instead we see a long stream of events
like this:</p>

<pre><code>2019/05/21 11:49:32 event: &quot;./content/4913&quot;: CREATE
2019/05/21 11:49:32 event: &quot;./content/hello.md~&quot;: CREATE
2019/05/21 11:49:32 event: &quot;./content/hello.md&quot;: RENAME
2019/05/21 11:49:32 event: &quot;./content/hello.md&quot;: CREATE
2019/05/21 11:49:32 event: &quot;./content/hello.md&quot;: CHMOD
2019/05/21 11:49:32 event: &quot;./content/hello.md~&quot;: REMOVE
2019/05/21 11:49:33 event: &quot;./content/hello.md&quot;: CHMOD
</code></pre>

<p>And all of this for one save! What could possibly be going
on? Well, various editors perform some non-intuitive
gymnastics to help protect against edge failures. What
we&rsquo;re seeing here is a Vim concept called a &ldquo;backup file&rdquo;
that exists to protect against the possibility that writing
a change to a file fails midway and leaves a user with lost
data <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>. Here&rsquo;s Vim&rsquo;s full procedure in saving a file:</p>

<ol>
<li><p>Test to see if the editor is allowed to create files in
the target directory by creating a file named <code>4913</code>.
The naming was chosen arbitrarily, but also to minimize
the likelihood of a collision with a real file.</p></li>

<li><p>Move the original file (<code>hello.md</code>) to the backup file,
suffixed by a tilde (<code>hello.md~</code>).</p></li>

<li><p>Write the new contents at the original filename
(<code>hello.md</code>).</p></li>

<li><p>Copy the old permissions to the new file with chmod.</p></li>

<li><p>On successful execution of all of the above, remove the
backup file <code>hello.md~</code>.</p></li>
</ol>

<p>It&rsquo;s good to know that Vim has our back in preventing
corruption, but all these changes aren&rsquo;t particularly
friendly to our build loop because they&rsquo;ll trigger rebuilds
for changes that won&rsquo;t affect the built result. I solved
this ignoring certain filenames in incoming events:</p>

<pre><code class="language-go">// Decides whether a rebuild should be triggered given some input
// event properties from fsnotify.
func shouldRebuild(path string, op fsnotify.Op) bool {
    base := filepath.Base(path)

    // Mac OS' worst mistake.
    if base == &quot;.DS_Store&quot; {
        return false
    }

    // Vim creates this temporary file to see whether it can write
    // into a target directory. It screws up our watching algorithm,
    // so ignore it.
    if base == &quot;4913&quot; {
        return false
    }

    // A special case, but ignore creates on files that look like
    // Vim backups.
    if strings.HasSuffix(base, &quot;~&quot;) {
        return false
    }

    ...
}
</code></pre>

<p>Special-casing byproducts of known editors isn&rsquo;t incredibly
elegant, but it&rsquo;s pragmatic choice. The build would still
work fine without the special cases, but it&rsquo;d be less
efficient. The pace of newly created editors isn&rsquo;t <em>so</em>
frantic so we won&rsquo;t be able to keep up with the new styles
of backup files and the like that they come up with.</p>

<h3 id="build-loop" class="link"><a href="#build-loop">Hardening the build loop</a></h3>

<p>It&rsquo;s a nice feature to trigger a page reload as soon as
possible after a build finishes, so the build loop will
start immediately on changes to non-ignored files. This
introduces a bit of a problem in that there may be
additional changes that arrive in close succession after
the first one <em>while</em> the build is still running.
Time-to-reload is an important feature, but we can&rsquo;t let it
supersede correctness &ndash; every change needs to be captured
to ensure that the final result is correct according the
current state of the source.</p>

<p>We&rsquo;ll cover that case by having two goroutines coordinate.
A <strong><em>watch</em></strong> goroutine watches for file system changes and
sends a signal to a <strong><em>build</em></strong> goroutine upon receiving
one. If however, the build is still ongoing when a new
change comes in, it will accumulate new events until being
signaled that the build completed, at which point it will
trigger a new one with the sum of the accumulated changes.</p>

<figure>
    <img alt="Goroutines coordinating builds even across changes that occur during an active build." class="overflowing" loading="lazy" src="/assets/images/live-reload/build-loop.svg">
    <figcaption>Goroutines coordinating builds even across changes that occur during an active build.</figcaption>
</figure>

<p>Builds are fast (we send just the names of files that
changed to make them incremental), so usually only one
change we&rsquo;re interested will come in at a time, but in case
many do, we&rsquo;ll rebuild until they&rsquo;ve all been accounted
for. Multiple accumulated changes can be pushed into a
single build, so we&rsquo;ll also rebuild as many times as
possible instead of once per change.</p>

<p>The watcher code with an accumulating inner loop looks
something like this (simplified slightly for brevity):</p>

<pre><code class="language-go">for {
    select {
    case event := &lt;-watchEvents:
        lastChangedSources := map[string]struct{}{event.Name: {}}

        if !shouldRebuild(event.Name, event.Op) {
            continue
        }

        for {
            if len(lastChangedSources) &lt; 1 {
                break
            }

            // Start rebuild
            rebuild &lt;- lastChangedSources

            // Zero out the last set of changes and start
            // accumulating.
            lastChangedSources = nil

            // Wait until rebuild is finished. In the meantime,
            // accumulate new events that come in on the watcher's
            // channel and prepare for the next loop.
        INNER_LOOP:
            for {
                select {
                case &lt;-rebuildDone:
                    // Break and start next outer loop
                    break INNER_LOOP

                case event := &lt;-watchEvents:
                    if !shouldRebuild(event.Name, event.Op) {
                        continue
                    }

                    if lastChangedSources == nil {
                        lastChangedSources = make(map[string]struct{})
                    }

                    lastChangedSources[event.Name] = struct{}{}
                }
            }
        }
    }
}
</code></pre>

<h2 id="websockets" class="link"><a href="#websockets">Signaling with WebSockets</a></h2>

<p>To get WebSocket support in the backend we&rsquo;ll use the
<a href="https://github.com/gorilla/websocket">Gorilla WebSocket</a> package, another off-the-shelf
library that abstracts away a lot of gritty details.
Creating a WebSocket connection is as simple as a single
invocation on an <code>Upgrader</code> object from the library:</p>

<pre><code class="language-go">var upgrader = websocket.Upgrader{
    ReadBufferSize:  1024,
    WriteBufferSize: 1024,
}

func handler(w http.ResponseWriter, r *http.Request) {
    conn, err := upgrader.Upgrade(w, r, nil)
    if err != nil {
        log.Println(err)
        return
    }

    ... Use conn to send and receive messages.
}
</code></pre>

<p>There&rsquo;s a little plumbing involved in the HTTP backend that
we&rsquo;ll skip over, but the important part is that the build
goroutine will use a <a href="https://golang.org/pkg/sync/#Cond">condition variable</a> to signal
the goroutines serving open WebSockets when a build
completes. Unlike the much more common channel primitive, a
condition variable allows a single controller to signal
any number of waiting consumers that a change occurred.</p>

<pre><code class="language-go">var buildCompleteMu sync.Mutex
buildComplete := sync.NewCond(&amp;buildCompleteMu)

// Signals all open WebSockets upon the completion of a
// successful build
buildComplete.Broadcast()
</code></pre>

<p>Those goroutines will in turn pass the signal along to
their clients as a JSON-serialized message:</p>

<pre><code class="language-go">// A type representing the extremely basic messages that
// we'll be serializing and sending back over a websocket.
type websocketEvent struct {
    Type string `json:&quot;type&quot;`
}

for {
    select {
    case &lt;-buildCompleteChan:
        err := conn.WriteJSON(websocketEvent{Type: &quot;build_complete&quot;})
        if err != nil {
            c.Log.Errorf(&quot;&lt;Websocket %v&gt; Error writing: %v&quot;,
                conn.RemoteAddr(), writeErr)
        }

    ...
}
</code></pre>

<figure>
    <img alt="The build goroutine broadcasting a completed rebuild to WebSocket goroutines that will message their clients." class="overflowing" loading="lazy" src="/assets/images/live-reload/signaling-rebuilds.svg">
    <figcaption>The build goroutine broadcasting a completed rebuild to WebSocket goroutines that will message their clients.</figcaption>
</figure>

<h3 id="client" class="link"><a href="#client">Client-side JavaScript</a></h3>

<p>The browser API for WebSockets is dead simple &ndash; a
<code>WebSocket</code> object and a single callback. Upon receiving
<code>build_complete</code> message from the server, we&rsquo;ll close the
WebSocket connection and reload the page.</p>

<p>Here&rsquo;s the minimum viable implementation:</p>

<pre><code class="language-js">var socket = new WebSocket(&quot;ws://localhost:5002/websocket&quot;);

socket.onmessage = function(event) {
  var data = JSON.parse(event.data);
  switch(data.type) {
    case &quot;build_complete&quot;:
      // 1000 = &quot;Normal closure&quot; and the second parameter is a
      // human-readable reason.
      socket.close(1000, &quot;Reloading page after receiving build_complete&quot;);

      console.log(&quot;Reloading page after receiving build_complete&quot;);
      location.reload(true);

      break;

    default:
      console.log(`Don't know how to handle type '${data.type}'`);
  }
}
</code></pre>

<h3 id="alive" class="link"><a href="#alive">Keeping connections alive</a></h3>

<p>We want to keep the amount of JavaScript we write to a
minimum, but it&rsquo;d be nice to make sure that client
connections are as robust as possible. In the event that a
WebSocket terminates unexpectedly, or the build server
restarts, they should try and reconnect so that the live
reload feature stays alive.</p>

<p>Here we use a WebSocket&rsquo;s <code>onclose</code> callback to set a
timeout that tries to reconnect after five seconds.
<code>onclose</code> is called even in the event of a connection
failure, so this code will continually try to reconnect
until either its tab is closed, or it&rsquo;s successful.</p>

<pre><code class="language-js">function connect() {
  var socket = new WebSocket(&quot;ws://localhost:5002/websocket&quot;);

  socket.onclose = function(event) {
    console.log(&quot;Websocket connection closed or unable to connect; &quot; +
      &quot;starting reconnect timeout&quot;);

    // Allow the last socket to be cleaned up.
    socket = null;

    // Set an interval to continue trying to reconnect
    // periodically until we succeed.
    setTimeout(function() {
      connect();
    }, 5000)
  }

  socket.onmessage = function(event) {
    ...
  }
}

connect();
</code></pre>

<p>This implementation, although still quite simple, ends up
working very reliably. It&rsquo;s common for me to shut down my
build server as I&rsquo;m changing Go code in the backend, and
with these few extra lines for resilience, the next time I
restart it all background tabs that I had open immediately
find the new server and start listening again almost
immediately. The server could&rsquo;ve been down for hours (or
days!) and it still works just fine.</p>

<h2 id="black-boxes" class="link"><a href="#black-boxes">Black boxes and solid foundations</a></h2>

<p>Building live reloading reminded me of the importance of
good foundational layers that are well-abstracted. Fsnotify
connects into one of three different OS-level monitoring
APIs depending on the operating system (<code>inotify</code>,
<code>kqueue</code>, or <code>ReadDirectoryChangesW</code>), and if you look at
its implementation, does quite a lot of legwork to make
that possible. But for us as the end user, it&rsquo;s all hidden
behind a couple function calls and two channels:</p>

<pre><code class="language-go">watcher, err := fsnotify.NewWatcher()
...

err = watcher.Add(&quot;/tmp/foo&quot;)
...

for {
    select {
        case event := &lt;-watcher.Events:
            ...

        case err := &lt;-watcher.Errors:
            ...
    }
}
</code></pre>

<p>None of the package&rsquo;s underlying complexity leaks into my
program, which leaves me with a lot less to worry about.</p>

<p>Likewise with WebSockets, the most basic client
implementation of live reload is about five lines of code,
despite the behind-the-scenes work involved in getting a
WebSocket open and connected. This is exactly what the road
to reliable software looks like: layering on top of <strong>black
boxes</strong> that expose a minimal API and whose walls are
opaque &ndash; they can be expected to &ldquo;just&rdquo; work, so there&rsquo;s
no need to think too hard about what&rsquo;s inside them.</p>


]]></content>
    <published>2019-05-28T13:57:17Z</published>
    <updated>2019-05-28T13:57:17Z</updated>
    <link href="https://brandur.org/live-reload"></link>
    <id>tag:brandur.org,2019-05-28:live-reload</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>SortSupport: Sorting in Postgres at Speed</title>
    <summary>How Postgres makes sorting really fast by comparing small, memory-friendly abbreviated keys as proxies for arbitrarily large values on the heap.</summary>
    <content type="html"><![CDATA[<p>Most often, there&rsquo;s a trade off involved in optimizing
software. The cost of better performance is the opportunity
cost of the time that it took to write the optimization,
and the additional cost of maintenance for code that
becomes more complex and more difficult to understand.</p>

<p>Many projects prioritize product development over improving
runtime speed. Time is spent building new things instead of
making existing things faster. Code is kept simpler and
easier to understand so that adding new features and fixing
bugs stays easy, even as particular people rotate in and
out and institutional knowledge is lost.</p>

<p>But that&rsquo;s certainly not the case in all domains. Game code
is often an interesting read because it comes from an
industry where speed is a competitive advantage, and it&rsquo;s
common practice to optimize liberally even at some cost to
modularity and maintainability. One technique for that is
to inline code in critical sections even to the point of
absurdity. CryEngine, open-sourced a few years ago, has a
few examples of this, with <a href="https://github.com/CRYTEK/CRYENGINE/blob/release/Code/CryEngine/CryPhysics/livingentity.cpp#L1275">&ldquo;tick&rdquo; functions like this
one</a> that are 800+ lines long with 14 levels of
indentation.</p>

<p>Another common place to find optimizations is in databases.
While games optimize because they have to, databases
optimize because they&rsquo;re an example of software that&rsquo;s
extremely leveraged &ndash; if there&rsquo;s a way to make running
select queries or building indexes 10% faster, it&rsquo;s not an
improvement that affects just a couple users, it&rsquo;s one
that&rsquo;ll potentially invigorate millions of installations
around the world. That&rsquo;s enough of an advantage that the
enhancement is very often worth it, even if the price is a
challenging implementation or some additional code
complexity.</p>

<p>Postgres contains a wide breadth of optimizations, and
happily they&rsquo;ve been written conscientiously so that the
source code stays readable. The one that we&rsquo;ll look at
today is <strong>SortSupport</strong>, a technique for localizing the
information needed to compare data into places where it can
be accessed very quickly, thereby making sorting data much
faster. Sorting for types that have had Sortsupport
implemented usually gets twice as fast or more, a speedup
that transfers directly into common database operations
like <code>ORDER BY</code>, <code>DISTINCT</code>, and <code>CREATE INDEX</code>.</p>

<h2 id="abbreviated-keys" class="link"><a href="#abbreviated-keys">Sorting with abbreviated keys</a></h2>

<p>While sorting, Postgres builds a series of tiny structures
that represent the data set being sorted. These tuples have
space for a value the size of a native pointer (i.e. 64
bits on a 64-bit machine) which is enough to fit the
entirety of some common types like booleans or integers
(known as pass-by-value types), but not for others that are
larger than 64 bits or arbitrarily large. In their case,
Postgres will follow a references back to the heap when
comparing values (they&rsquo;re appropriately called
pass-by-reference types). Postgres is very fast, so that
still happens quickly, but it&rsquo;s slower than comparing
values readily available in memory.</p>

<figure>
    <img alt="An array of sort tuples." class="overflowing" loading="lazy" src="/assets/images/sortsupport/sort-tuples.svg">
    <figcaption>An array of sort tuples.</figcaption>
</figure>

<p>SortSupport augments pass-by-reference types by bringing a
representative part of their value into the sort tuple to
save trips to the heap. Because sort tuples usually don&rsquo;t
have the space to store the entirety of the value,
SortSupport generates a digest of the full value called an
<strong>abbreviated key</strong>, and stores it instead. The contents of
an abbreviated key vary by type, but they&rsquo;ll aim to store
as much sorting-relevant information as possible while
remaining faithful to pre-existing sorting rules.</p>

<p>Abbreviated keys should never produce an incorrect
comparison, but it&rsquo;s okay if they can&rsquo;t fully resolve one.
If two abbreviated keys look equal, Postgres will fall back
to comparing their full heap values to make sure it gets
the right result (called an &ldquo;authoritative comparison&rdquo;).</p>

<figure>
    <img alt="A sort tuple with an abbreviated key and pointer to the heap." class="overflowing" loading="lazy" src="/assets/images/sortsupport/abbreviated-keys.svg">
    <figcaption>A sort tuple with an abbreviated key and pointer to the heap.</figcaption>
</figure>

<p>Implementing an abbreviated key is straightforward in many
cases. UUIDs are a good example of that: at 128 bits long
they&rsquo;re always larger than the pointer size even on a
64-bit machine, but we can get a very good proxy of their
full value just by sampling their first 64 bits (or 32 on a
32-bit machine). Especially for V4 UUIDs which are almost
entirely random <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>, the first 64 bits will be enough to
definitively determine the order for all but unimaginably
large data sets. Indeed, <a href="https://www.postgresql.org/message-id/CAM3SWZR4avsTwwNVUzRNbHk8v36W-QBqpoKg%3DOGkWWy0dKtWBA%40mail.gmail.com">the patch that brought in
SortSupport for UUIDs</a> made sorting them about
twice as fast!</p>

<p>String-like types (e.g. <code>text</code>, <code>varchar</code>) aren&rsquo;t too much
harder: just pack as many characters from the front of the
string in as possible (although made somewhat more
complicated by locales). Adding SortSupport for them made
operations like <code>CREATE INDEX</code> <a href="http://pgeoghegan.blogspot.com/2015/01/abbreviated-keys-exploiting-locality-to.html">about three times
faster</a>. My only ever patch to Postgres was
implementing SortSupport for the <code>macaddr</code> type, which was
fairly easy because although it&rsquo;s pass-by-reference, its
values are only six bytes long <sup id="footnote-2-source"><a href="#footnote-2">2</a></sup>. On a 64-bit machine we
have room for all six bytes, and on 32-bit we sample the
MAC address&rsquo; first four bytes.</p>

<p>Some abbreviated keys are more complex. The implementation
for the <code>numeric</code> type, which allows arbitrary scale and
precision, involves <a href="https://en.wikipedia.org/wiki/Offset_binary">excess-K coding</a> and breaking
available bits into multiple parts to store sort-relevant
fields.</p>

<h2 id="implementation" class="link"><a href="#implementation">A glance at the implementation</a></h2>

<p>Let&rsquo;s try to get a basic idea of how SortSupport is
implemented by examining a narrow slice of source code.
Sorting in Postgres is extremely complex and involves
thousands of lines of code, so fair warning that I&rsquo;m going
to simplify some things and skip <em>a lot</em> of others.</p>

<p>A good place start is with <code>Datum</code>, the pointer-sized type
(32 or 64 bits, depending on the CPU) used for sort
comparisons. It stores entire values for pass-by-value
types, abbreviated keys for pass-by-reference types that
implement SortSupport, and a pointer for those that don&rsquo;t.
You can find it defined in <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/include/postgres.h#L367"><code>postgres.h</code></a>:</p>

<pre><code class="language-c">/*
 * A Datum contains either a value of a pass-by-value type or a pointer
 * to a value of a pass-by-reference type.  Therefore, we require:
 *
 * sizeof(Datum) == sizeof(void *) == 4 or 8
 */

typedef uintptr_t Datum;

#define SIZEOF_DATUM SIZEOF_VOID_P
</code></pre>

<h3 id="uuid" class="link"><a href="#uuid">Building abbreviated keys for UUID</a></h3>

<p>The format of abbreviated keys for the <code>uuid</code> type is one
of the easiest to understand, so let&rsquo;s look at that. In
Postgres, the struct <code>pg_uuid_t</code> defines how UUIDs are
physically stored in the heap (from <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/include/utils/uuid.h#L20"><code>uuid.h</code></a>):</p>

<pre><code class="language-c">/* uuid size in bytes */
#define UUID_LEN 16

typedef struct pg_uuid_t
{
    unsigned char data[UUID_LEN];
} pg_uuid_t;
</code></pre>

<p>You might be used to seeing UUIDs represented in string
format like <code>123e4567-e89b-12d3-a456-426655440000</code>, but
remember that this is Postgres which likes to be as
efficient as possible! A UUID contains 16 bytes worth of
information, so <code>pg_uuid_t</code> above defines an array of
exactly 16 bytes. No wastefulness to be found.</p>

<p>SortSupport implementations define a conversion routine
which takes the original value and produces a datum
containing an abbreviated key. Here&rsquo;s the one for UUIDs
(from <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/backend/utils/adt/uuid.c#L367"><code>uuid.c</code></a>):</p>

<pre><code class="language-c">static Datum
uuid_abbrev_convert(Datum original, SortSupport ssup)
{
    pg_uuid_t *authoritative = DatumGetUUIDP(original);
    Datum      res;

    memcpy(&amp;res, authoritative-&gt;data, sizeof(Datum));

    ...

    /*
     * Byteswap on little-endian machines.
     *
     * This is needed so that uuid_cmp_abbrev() (an unsigned integer 3-way
     * comparator) works correctly on all platforms.  If we didn't do this,
     * the comparator would have to call memcmp() with a pair of pointers to
     * the first byte of each abbreviated key, which is slower.
     */
    res = DatumBigEndianToNative(res);

    return res;
}
</code></pre>

<p><code>memcpy</code> (&ldquo;memory copy&rdquo;) extracts a datum worth of bytes
from a <code>pg_uuid_t</code> and places it into <code>res</code>. We can&rsquo;t take
the whole UUID, but we&rsquo;ll be taking its 4 or 8 most
significant bytes, which will be enough information for
most comparisons.</p>

<figure>
    <img alt="Abbreviated key formats for the `uuid` type." class="overflowing" loading="lazy" src="/assets/images/sortsupport/uuid.svg">
    <figcaption>Abbreviated key formats for the `uuid` type.</figcaption>
</figure>

<p>The call <code>DatumBigEndianToNative</code> is there to help with an
optimization. When comparing our abbreviated keys, we could
do so with <code>memcmp</code> (&ldquo;memory compare&rdquo;)  which would compare
each byte in the datum one at a time. That&rsquo;s perfectly
functional of course, but because our datums are the same
size as native integers, we can instead choose to take
advantage of the fact that CPUs are optimized to compare
integers really, really quickly, and arrange the datums in
memory as if they were integers. You can see this integer
comparison taking place in the UUID abbreviated key
comparison function:</p>

<pre><code class="language-c">static int
uuid_cmp_abbrev(Datum x, Datum y, SortSupport ssup)
{
    if (x &gt; y)
        return 1;
    else if (x == y)
        return 0;
    else
        return -1;
}
</code></pre>

<p>However, pretending that some consecutive bytes in memory
are integers introduces some complication. Integers might
be stored like <code>data</code> in <code>pg_uuid_t</code> with the most
significant byte first, but that depends on the
architecture of the CPU. We call architectures that store
numerical values this way <strong>big-endian</strong>. Big-endian
machines exist, but the chances are that the CPU you&rsquo;re
using to read this article stores bytes in the reverse
order of their significance, with the most significant at
the highest address. This layout is called
<strong>little-endian</strong>, and is in use by Intel&rsquo;s X86, as well as
being the default mode for ARM chips like the ones in
Android and iOS devices.</p>

<p>If we left the big-endian result of the <code>memcpy</code> unchanged
on little-endian systems, the resulting integer would be
wrong. The answer is to byteswap, which reverses the order
of the bytes, and corrects the integer.</p>

<figure>
    <img alt="Example placement of integer bytes on little and big endian architectures." class="overflowing" loading="lazy" src="/assets/images/sortsupport/endianness.svg">
    <figcaption>Example placement of integer bytes on little and big endian architectures.</figcaption>
</figure>

<p>You can see in <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/include/port/pg_bswap.h#L143"><code>pg_bswap.h</code></a> that
<code>DatumBigEndianToNative</code> is defined as a no-op on a
big-endian machine, and is otherwise connected to a
byteswap (&ldquo;bswap&rdquo;) routine of the appropriate size:</p>

<pre><code class="language-c">#ifdef WORDS_BIGENDIAN

        #define        DatumBigEndianToNative(x)    (x)

#else

    #if SIZEOF_DATUM == 8
        #define        DatumBigEndianToNative(x)    pg_bswap64(x)
    #else
        #define        DatumBigEndianToNative(x)    pg_bswap32(x)
    #endif

#endif
</code></pre>

<h4 id="abort" class="link"><a href="#abort">Conversion abort & HyperLogLog</a></h4>

<p>Let&rsquo;s touch upon one more feature of <code>uuid_abbrev_convert</code>.
In data sets with very low cardinality (i.e, many
duplicated items) SortSupport introduces some danger of
worsening performance. With so many duplicates, the
contents of abbreviated keys would often show equality, in
which cases Postgres would often have to fall back to the
authoritative comparator. In effect, by adding SortSupport
we would have added a useless additional comparison that
wasn&rsquo;t there before.</p>

<p>To protect against performance regression, SortSupport has
a mechanism for aborting abbreviated key conversion. If the
data set is found to be below a certain cardinality
threshold, Postgres stops abbreviating, reverts any keys
that were already abbreviated, and disables further
abbreviation for the sort.</p>

<p>Cardinality is estimated with the help of
<a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>, an algorithm that estimates the
distinct count of a data set in a very memory-efficient
way. Here you can see the conversion routine adding new
values to the HyperLogLog if an abort is still possible:</p>

<pre><code class="language-c">uss-&gt;input_count += 1;

if (uss-&gt;estimating)
{
    uint32        tmp;

#if SIZEOF_DATUM == 8
    tmp = (uint32) res ^ (uint32) ((uint64) res &gt;&gt; 32);
#else
    tmp = (uint32) res;
#endif

    addHyperLogLog(&amp;uss-&gt;abbr_card, DatumGetUInt32(hash_uint32(tmp)));
}
</code></pre>

<p>And where it makes an abort decision (from <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/backend/utils/adt/uuid.c#L301"><code>uuid.c</code></a>):</p>

<pre><code class="language-c">static bool
uuid_abbrev_abort(int memtupcount, SortSupport ssup)
{
    ...

    abbr_card = estimateHyperLogLog(&amp;uss-&gt;abbr_card);

    /*
     * If we have &gt;100k distinct values, then even if we were
     * sorting many billion rows we'd likely still break even,
     * and the penalty of undoing that many rows of abbrevs would
     * probably not be worth it. Stop even counting at that point.
     */
    if (abbr_card &gt; 100000.0)
    {
        uss-&gt;estimating = false;
        return false;
    }

    /*
     * Target minimum cardinality is 1 per ~2k of non-null inputs.
     * 0.5 row fudge factor allows us to abort earlier on genuinely
     * pathological data where we've had exactly one abbreviated
     * value in the first 2k (non-null) rows.
     */
    if (abbr_card &lt; uss-&gt;input_count / 2000.0 + 0.5)
    {
        return true;
    }

    ...
}
</code></pre>

<p>It also covers aborting the case where we have a data set
that&rsquo;s poorly suited to the abbreviated key format. For
example, imagine a million UUIDs that all shared a common
prefix in their first eight bytes, but were distinct in
their last eight <sup id="footnote-3-source"><a href="#footnote-3">3</a></sup>. Realistically this will be extremely
unusual, so abbreviated key conversion will rarely abort.</p>

<h3 id="tuples" class="link"><a href="#tuples">Tuples and data types</a></h3>

<p><strong>Sort tuples</strong> are the tiny structures that Postgres sorts
in memory. They hold a reference to the &ldquo;true&rdquo; tuple, a
datum, and a flag to indicate whether or not the first
value is <code>NULL</code> (which has its own special sorting
semantics). The latter two are named with a <code>1</code> suffix as
<code>datum1</code> and <code>isnull1</code> because they represent only one
field worth of information. Postgres will need to fall back
to different values in the event of equality in a
multi-column comparison. From <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/backend/utils/sort/tuplesort.c#L169"><code>tuplesort.c</code></a>:</p>

<pre><code class="language-c">/*
 * The objects we actually sort are SortTuple structs.  These contain
 * a pointer to the tuple proper (might be a MinimalTuple or IndexTuple),
 * which is a separate palloc chunk --- we assume it is just one chunk and
 * can be freed by a simple pfree() (except during merge, when we use a
 * simple slab allocator).  SortTuples also contain the tuple's first key
 * column in Datum/nullflag format, and an index integer.
 */
typedef struct
{
    void       *tuple;          /* the tuple itself */
    Datum       datum1;         /* value of first key column */
    bool        isnull1;        /* is first key column NULL? */
    int         tupindex;       /* see notes above */
} SortTuple;
</code></pre>

<p>In the code we&rsquo;ll look at below, <code>SortTuple</code> may reference
a <strong>heap tuple</strong>, which has a variety of different struct
representations. One used by the sort algorithm is
<code>HeapTupleHeaderData</code> (from <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/include/access/htup_details.h#L152"><code>htup_details.h</code></a>):</p>

<pre><code class="language-c">struct HeapTupleHeaderData
{
    union
    {
        HeapTupleFields t_heap;
        DatumTupleFields t_datum;
    }            t_choice;

    ItemPointerData t_ctid; /* current TID of this or newer tuple (or a
                             * speculative insertion token) */

    ...
}
</code></pre>

<p>Heap tuples have a pretty complex structure which we won&rsquo;t
cover, but you can see that it contains an
<code>ItemPointerData</code> value. This struct is what gives Postgres
the precise information it needs to find data in the heap
(from <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/include/storage/itemptr.h#L36"><code>itemptr.h</code></a>):</p>

<pre><code class="language-c">/*
 * ItemPointer:
 *
 * This is a pointer to an item within a disk page of a known file
 * (for example, a cross-link from an index to its parent table).
 * blkid tells us which block, posid tells us which entry in the linp
 * (ItemIdData) array we want.
 */
typedef struct ItemPointerData
{
    BlockIdData ip_blkid;
    OffsetNumber ip_posid;
}
</code></pre>

<h3 id="comparison" class="link"><a href="#comparison">Tuple comparison</a></h3>

<p>The algorithm to compare abbreviated keys is duplicated in
the Postgres source in a number of places depending on the
sort operation being carried out. We&rsquo;ll take a look at
<code>comparetup_heap</code> (from <a href="https://github.com/postgres/postgres/blob/08ecdfe7e5e0a31efbe1d58fefbe085b53bc79ca/src/backend/utils/sort/tuplesort.c#L3508"><code>tuplesort.c</code></a>) which
is used when sorting based on the heap. This would be
invoked for example if you ran an <code>ORDER BY</code> on a field
that doesn&rsquo;t have an index on it.</p>

<pre><code class="language-c">static int
comparetup_heap(const SortTuple *a, const SortTuple *b, Tuplesortstate *state)
{
    SortSupport sortKey = state-&gt;sortKeys;
    HeapTupleData ltup;
    HeapTupleData rtup;
    TupleDesc     tupDesc;
    int           nkey;
    int32         compare;
    AttrNumber    attno;
    Datum         datum1,
                  datum2;
    bool          isnull1,
                  isnull2;


    /* Compare the leading sort key */
    compare = ApplySortComparator(a-&gt;datum1, a-&gt;isnull1,
                                  b-&gt;datum1, b-&gt;isnull1,
                                  sortKey);
    if (compare != 0)
        return compare;
</code></pre>

<p><code>ApplySortComparator</code> gets a comparison result between two
datum values. It&rsquo;ll compare two abbreviated keys where
appropriate and handles <code>NULL</code> sorting semantics. The
return value of a comparison follows the spirit of C&rsquo;s
<code>strcmp</code>: when comparing <code>(a, b)</code>, -1 indicates <code>a &lt; b</code>,
0 indicates equality, and 1 indicates <code>a &gt; b</code>.</p>

<p>The algorithm returns immediately if inequality (<code>!= 0</code>)
was detected. Otherwise, it checks to see if abbreviated
keys were used, and if so applies the authoritative
comparison if they were. Because space in abbreviated keys
is limited, two being equal doesn&rsquo;t necessarily indicate
that the values that they represent are.</p>

<pre><code class="language-c">if (sortKey-&gt;abbrev_converter)
{
    attno = sortKey-&gt;ssup_attno;

    datum1 = heap_getattr(&amp;ltup, attno, tupDesc, &amp;isnull1);
    datum2 = heap_getattr(&amp;rtup, attno, tupDesc, &amp;isnull2);

    compare = ApplySortAbbrevFullComparator(datum1, isnull1,
                                            datum2, isnull2,
                                            sortKey);
    if (compare != 0)
        return compare;
}
</code></pre>

<p>Once again, the algorithm returns if inequality was
detected. If not, it starts to look beyond the first field
(in the case of a multi-column sort):</p>

<pre><code class="language-c">    ...

    sortKey++;
    for (nkey = 1; nkey &lt; state-&gt;nKeys; nkey++, sortKey++)
    {
        attno = sortKey-&gt;ssup_attno;

        datum1 = heap_getattr(&amp;ltup, attno, tupDesc, &amp;isnull1);
        datum2 = heap_getattr(&amp;rtup, attno, tupDesc, &amp;isnull2);

        compare = ApplySortComparator(datum1, isnull1,
                                      datum2, isnull2,
                                      sortKey);
        if (compare != 0)
            return compare;
    }

    return 0;
}
</code></pre>

<p>After finding abbreviated keys to be equal, full values to
be equal, and all additional sort fields to be equal, the
last step is to <code>return 0</code>, indicating in classic libc
style that the two tuples are really, fully equal.</p>

<h2 id="leverage" class="link"><a href="#leverage">Fast code and leveraged software</a></h2>

<p>SortSupport is a good example of the type of low-level
optimization that most of us probably wouldn&rsquo;t bother with
in our projects, but which makes sense in an extremely
leveraged system like a database. As implementations are
added for it and Postgres&rsquo; tens of thousands of users like
myself upgrade, common operations like <code>DISTINCT</code>, <code>ORDER
BY</code>, and <code>CREATE INDEX</code> get twice as fast, for free.</p>

<p>Credit to Peter Geoghegan for some of the original
exploration of this idea and implementations for UUID and a
generalized system for SortSupport on variable-length
string types, Robert Haas and Tom Lane for adding the
<a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=c6e3ac11b60ac4a8942ab964252d51c1c0bd8845">necessary infrastructure</a>, and Andrew
Gierth for a <a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=abd94bcac4582903765be7be959d1dbc121df0d0">difficult implementation</a> for
<code>numeric</code>. (I hope I got all that right.)</p>


]]></content>
    <published>2019-02-04T16:56:52Z</published>
    <updated>2019-02-04T16:56:52Z</updated>
    <link href="https://brandur.org/sortsupport"></link>
    <id>tag:brandur.org,2019-02-04:sortsupport</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>How to Manage Connections Efficiently in Postgres, or Any Database</title>
    <summary>Hitting the limit for maximum allowed connections is a common operational problem in Postgres. Here we look at a few techniques for managing connections and making efficient use of those that are available.</summary>
    <content type="html"><![CDATA[<p>You start building your new project. You&rsquo;ve heard good
things about Postgres, so you choose it as your database.
As advertised, it proves to be a satisfying tool and
progress is good. You put your project into production for
the first time and like you&rsquo;d hoped, things go smoothly as
Postgres turns out to be well-suited for production use as
well.</p>

<p>The first few months go well and traffic continues to ramp
up, when suddenly a big spike of failures appears. You dig
into the cause and see that your application is failing to
open database connections. You find this chilling artifact
littered throughout your logs:</p>

<pre><code>FATAL: remaining connection slots are reserved for
non-replication superuser connections
</code></pre>

<p>This is one of the first major operational problems that
new users are likely to encounter with Postgres, and one
that might prove to be frustratingly persistent. Like the
error suggests, the database is indicating that its total
number of connection slots are limited, and that the limit
has been reached.</p>

<p>The ceiling is controlled by the <code>max_connections</code> key in
Postgres&rsquo; configuration, which defaults to 100. Almost
every cloud Postgres provider like Google Cloud Platform or
Heroku limit the number pretty carefully, with the largest
databases topping out at 500 connections, and the smaller
ones at much lower numbers like 20 or 25.</p>

<p>At first sight this might seem a little counterintuitive.
If the connection limit is a known problem, why not just
configure a huge maximum to avoid it? As with many things
in computing, the solution isn&rsquo;t as simple as it might seem
at first glance, and there are a number of factors that
will limit the maximum number of connections that it&rsquo;s
practical to have; some obvious, and some not. Let&rsquo;s take a
closer look.</p>

<h2 id="concurrency-limits" class="link"><a href="#concurrency-limits">The practical limits of concurrency</a></h2>

<p>The most direct constraint, but also probably the least
important, is memory. Postgres is designed around a process
model where a central Postmaster accepts incoming
connections and forks child processes to handle them. Each
of these &ldquo;backend&rdquo; processes starts out at around 5 MB in
size, but may grow to be much larger depending on the data
they&rsquo;re accessing <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>.</p>

<figure>
    <img alt="A simplified view of Postgres' forking process model." class="overflowing" loading="lazy" src="/assets/images/postgres-connections/process-model.svg">
    <figcaption>A simplified view of Postgres' forking process model.</figcaption>
</figure>

<p>Since these days it&rsquo;s pretty easy to procure a system where
memory is abundant, the absolute memory ceiling often isn&rsquo;t
a main limiting factor. One that&rsquo;s more subtle and more
important is that the Postmaster and its backend processes
use shared memory for communication, and parts of that
shared space are global bottlenecks. For example, here&rsquo;s
the structure that tracks every ongoing process and
transaction:</p>

<pre><code class="language-c">typedef struct PROC_HDR
{
    /* Array of PGPROC structures (not including dummies for prepared txns) */
    PGPROC       *allProcs;
    /* Array of PGXACT structures (not including dummies for prepared txns) */
    PGXACT       *allPgXact;

    ...
}

extern PGDLLIMPORT PROC_HDR *ProcGlobal;
</code></pre>

<p>Operations that might happen in any backend requires
walking the entire list of processes or transactions.
Adding a new process to the proc array necessitates taking
an exclusive lock:</p>

<pre><code class="language-c">void
ProcArrayAdd(PGPROC *proc)
{
    ProcArrayStruct *arrayP = procArray;
    int            index;

    LWLockAcquire(ProcArrayLock, LW_EXCLUSIVE);

    ...
}
</code></pre>

<p>Likewise, <code>GetSnapshotData</code> is often called multiple times
for any operation and needs to loop through every other
process in the system:</p>

<pre><code class="language-c">Snapshot
GetSnapshotData(Snapshot snapshot)
{
    ProcArrayStruct *arrayP = procArray;

    ...

    /*
     * Spin over procArray checking xid, xmin, and subxids.  The goal is
     * to gather all active xids, find the lowest xmin, and try to record
     * subxids.
     */
    numProcs = arrayP-&gt;numProcs;
    for (index = 0; index &lt; numProcs; index++)
    {
        ...
    }
}
</code></pre>

<p>There are a few such bottlenecks throughout the normal
paths that Postgres uses to work, and they are of course in
addition to the normal contention you&rsquo;d expect to find
around system resources like I/O or CPU.</p>

<p>The cumulative effect is that within any given backend,
performance is proportional to the number of all active
backends in the wider system. I wrote a <a href="https://github.com/brandur/connections-test">benchmark</a> to
demonstrate this effect: it spins up a cluster of parallel
workers that each use their own connection to perform a
transaction that inserts ten times, selects ten times, and
deletes ten times before committing <sup id="footnote-2-source"><a href="#footnote-2">2</a></sup>. Parallelism starts
at 1, ramps up to 1000, and timing is measured for every
transaction. You can see from the results that performance
degrades slowly but surely as more active clients are
introduced:</p>

<figure>
    <img alt="Performance of a simple task degrading as the number of active connections in the database increases." class="overflowing" loading="lazy" src="/assets/images/postgres-connections/contention.png" srcset="/assets/images/postgres-connections/contention@2x.png 2x, /assets/images/postgres-connections/contention.png 1x">
    <figcaption>Performance of a simple task degrading as the number of active connections in the database increases.</figcaption>
</figure>

<p>So while it might be a little irking that platforms like
Google Cloud and Heroku limit the total connections even on
very big servers, they&rsquo;re actually trying to help you.
Performance in Postgres isn&rsquo;t reliable when it&rsquo;s scaled up
to huge numbers of connections. Once you start brushing up
against a big connection limit like 500, the right answer
probably isn&rsquo;t to increase it &ndash; it&rsquo;s to re-evaluate how
those connections are being used to and try to manage them
more efficiently.</p>

<h2 id="techniques" class="link"><a href="#techniques">Techniques for efficient connection use</a></h2>

<h3 id="connection-pool" class="link"><a href="#connection-pool">Connection pools</a></h3>

<p>A connection pool is a cache of database connections,
usually local to a specific process. Its main advantage is
improved performance &ndash; there&rsquo;s a certain amount of
overhead inherent to opening a new database connection in
both the client and the server. After finishing with a
connection, by checking it back into a pool instead of
discarding it, the connection can be reused next time one
is needed within the application. Connection pooling is
built into many database adapters including Go&rsquo;s
<a href="https://godoc.org/database/sql"><code>database/sql</code></a>, Java&rsquo;s <a href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC</a>, or
Active Record in Ruby.</p>

<figure>
    <img alt="A deployment with a number of nodes, each of which maintains a local pool of connections for their workers to use." class="overflowing" loading="lazy" src="/assets/images/postgres-connections/connection-pooling.svg">
    <figcaption>A deployment with a number of nodes, each of which maintains a local pool of connections for their workers to use.</figcaption>
</figure>

<p>Connection pools also help manage connections more
efficiently. They&rsquo;re configured with a maximum number of
connections that the pool can hold which makes the total
number of connections that you can expect a single deployed
node to use deterministic. By writing application workers
to only acquire a connection when they&rsquo;re serving a
request, those per-node pools of connections can be shared
between a much larger pool of workers.</p>

<p>A limitation of connection pools is that they&rsquo;re usually
only effective in languages that can be deployed within a
single process. Rails implements a connection pool in
Active Record, but because Ruby isn&rsquo;t capable of real
parallelism, it&rsquo;s common to use forking servers like
Unicorn or Puma. This makes those connection pools much
less effective because each process needs to maintain its
own <sup id="footnote-3-source"><a href="#footnote-3">3</a></sup>.</p>

<h3 id="mvc" class="link"><a href="#mvc">Minimum viable checkouts</a></h3>

<p>For any given span of work, very often it&rsquo;s possible to
identify a critical span in the middle where core domain
logic is being run, and where a database connection needs
to be held. To take an HTTP request for example, there&rsquo;s
usually a phase at the beginning where a worker is reading
a request&rsquo;s body, decoding and validating its payload, and
performing other peripheral operations like rate limiting
before moving on to the application&rsquo;s core logic. After
that logic is executed there&rsquo;s a similar phase at the end
where it&rsquo;s serializing and sending the response, emitting
metrics, performing logging, and so on.</p>

<figure>
    <img alt="Workers should only hold connections as long as they're needed. There's work before and after core application logic where no connection is needed." class="overflowing" loading="lazy" src="/assets/images/postgres-connections/minimum-viable-checkout.svg">
    <figcaption>Workers should only hold connections as long as they're needed. There's work before and after core application logic where no connection is needed.</figcaption>
</figure>

<p>Workers should only have a connection checked out of the
pool while that core logic is executing. This <strong>minimum
viable checkout</strong> technique maximizes the efficient use of
connections by minimizing the amount of time any given
worker holds one, allowing a pool of connections to be
feasibly shared amongst a much larger pool of workers. Idle
workers don&rsquo;t hold any connections at all.</p>

<h4 id="foreign-mutations" class="link"><a href="#foreign-mutations">Releasing connections around foreign mutations</a></h4>

<p>I&rsquo;ve written previously about breaking units of application
work into <a href="/idempotency-keys#atomic-phases">atomic phases</a> around where an
application is making requests to foreign APIs. Utilization
can be made even more efficient by making sure to release
connections back to the pool while that slow network I/O is
in flight (an application should not be in a transaction
while mutating foreign state anyway), and reacquire
them afterwards.</p>

<h3 id="pgbouncer" class="link"><a href="#pgbouncer">PgBouncer & inter-node pooling</a></h3>

<p>Connection pools and minimum viable checkouts will go a
long way, but you may still reach a point where a hammer is
needed. When an application is scaled out to many nodes,
connection pools maximize the efficient use of connections
local to any of them, but can&rsquo;t do so between nodes. In
most systems work should be distributed between nodes
roughly equally, but because it&rsquo;s normal to use randomness
to do that (through something like HAProxy or another load
balancer), and because work durations vary, an equal
distribution of work across the whole cluster at any given
time isn&rsquo;t likely.</p>

<p>If we have <em>N</em> nodes and <em>M</em> maximum connections per node,
we may have a configuration where <em>N</em> Ã— <em>M</em> is greater than
the database&rsquo;s <code>max_connections</code> to protect against the
case where a single node is handling an outsized amount of
work and needs more connections. Because nodes aren&rsquo;t
coordinating, if the whole cluster is running close to
capacity, it&rsquo;s possible for a node trying to get a new
connection to go over-limit and get an error back from
Postgres.</p>

<p>In this case it&rsquo;s possible to install
<a href="https://pgbouncer.github.io/">PgBouncer</a> to act as a global pool by proxying
all connections through it to Postgres. It functions almost
exactly like a connection pool and has a few modes of
operation:</p>

<ul>
<li><p><strong>Session pooling:</strong> A connection is assigned when a
client opens a connection and unassigned when the client
closes it.</p></li>

<li><p><strong>Transaction pooling:</strong> Connections are assigned only
for the duration of a transaction, and may be shared
around them. This comes with a limitation that
applications cannot use features that change the &ldquo;global&rdquo;
state of a connection like <code>SET</code>, <code>LISTEN</code>/<code>NOTIFY</code>, or
prepared statements <sup id="footnote-4-source"><a href="#footnote-4">4</a></sup>.</p></li>

<li><p><strong>Statement pooling:</strong> Connections are assigned only
around individual statements. This only works of course
if an application gives up the use of transactions, at
which point it&rsquo;s losing a big advantage of using
Postgres in the first place.</p></li>
</ul>

<figure>
    <img alt="Using PgBouncer to maintain a global connection pool to optimize connection use across all nodes." class="overflowing" loading="lazy" src="/assets/images/postgres-connections/pgbouncer.svg">
    <figcaption>Using PgBouncer to maintain a global connection pool to optimize connection use across all nodes.</figcaption>
</figure>

<p>Transaction pooling is the best strategy for applications
that are already making effective use of a node-local
connection pool, and will allow such an application that&rsquo;s
configured with an <em>N</em> Ã— <em>M</em> greater than <code>max_connections</code>
to closely approach the maximum possible theoretical
utilization of available connections, and to also avoid
connection errors caused by going over-limit (although
delaying requests while waiting for a connection to become
available from PgBouncer is still possible).</p>

<p>Probably the more common use of PgBouncer is to act as a
node-local connection pool for applications that can&rsquo;t do a
good job of implementing their own, like a Rails app
deployed with Unicorn. Heroku, for example, provides and
recommends the use of a standardized buildpack that deploys
a per-dyno PgBouncer to accomplish this. It&rsquo;s a handy tool
to cover this case, but it&rsquo;s advisable to use a more
sophisticated technique if possible.</p>

<h2 id="resource" class="link"><a href="#resource">Connections as a resource</a></h2>

<p>There was a trend in frameworks for some time to try and
simplify software development for their users by
abstracting away the details of connection management. This
might work for a time, but in the long run anyone
deploying a large application on Postgres will have to
understand what&rsquo;s happening or they&rsquo;re likely to run into
trouble. It&rsquo;ll usually pay to understand them earlier so
that applications can be architected smartly to maximize
the efficient use of a scarce resource.</p>

<p>Developers should be aware of how many connections each
node can use, how many connections a cluster can use by
multiplying that number by the number of nodes, and where
that total sits relative to Postgres&rsquo; <code>max_connections</code>.
It&rsquo;s common to hit limits during a deploy because a
graceful restart spins up new workers or nodes before
shutting down old ones, so know expected connection numbers
during deployments as well.</p>

<p>Finally, although we&rsquo;ve talked mostly about Postgres here,
there will be practical bottlenecks like the ones described
here in any database, so these techniques for managing
connections should be widely portable.</p>


]]></content>
    <published>2018-10-15T15:42:51Z</published>
    <updated>2018-10-15T15:42:51Z</updated>
    <link href="https://brandur.org/postgres-connections"></link>
    <id>tag:brandur.org,2018-10-15:postgres-connections</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>A Missing Link in Postgres 11: Fast Column Creation with Defaults</title>
    <summary>How a seemingly minor enhancement in Postgres 11 fills one of the system&amp;rsquo;s biggest operational holes.</summary>
    <content type="html"><![CDATA[<p>If you read through the release notes for <a href="https://www.postgresql.org/docs/11/static/release-11.html">upcoming
Postgres 11</a>, you might see a somewhat
inconspicuous addition tucked away at the bottom of the
enhancements list:</p>

<blockquote>
<p>Many other useful performance improvements, including
making <code>ALTER TABLE .. ADD COLUMN</code> with a non-null column
default faster</p>
</blockquote>

<p>It&rsquo;s not a flagship feature of the new release, but it&rsquo;s
still one of the more important operational improvements
that Postgres has made in years, even though it might not
be immediately obvious why. The short version is that it&rsquo;s
eliminated a limitation that used to make correctness in
schema design difficult, but let&rsquo;s take a look at the
details.</p>

<h2 id="alterations" class="link"><a href="#alterations">Alterations and exclusive locks</a></h2>

<p>Consider for a moment one of the simplest database
statements possible, one that adds a new column to a table:</p>

<pre><code class="language-sql">ALTER TABLE users
    ADD COLUMN credits bigint;
</code></pre>

<p>Although it&rsquo;s altering the table&rsquo;s schema, any modern
database is sophisticated enough to make this operation
practically instantaneous. Instead of rewriting the
existing representation of the table (thereby forcing all
existing data to be copied over at great expense),
information on the new column is added to the system
catalog, which is cheap. That allows new rows to be written
with values for the new column, and the system is smart
enough to return <code>NULL</code> for current rows where no value
previously existed.</p>

<p>But things get complicated when we add a <code>DEFAULT</code> clause
to the same statement:</p>

<pre><code class="language-sql">ALTER TABLE users
    ADD COLUMN credits bigint NOT NULL DEFAULT 0;
</code></pre>

<p>The SQL looks so similar as to be almost identical, but
where the previous operation was trivial, this one is
infinitely more expensive in that it now requires a full
rewrite of the table and all its indexes. Because there&rsquo;s
now a non-null value involved, the database ensures data
integrity by going back and injecting it into every
existing row.</p>

<p>Despite that expense, Postgres is still capable of doing
the rewrite efficiently, and on smaller databases it&rsquo;ll
appear to happen instantly.</p>

<p>It&rsquo;s bigger installations where it becomes a problem.
Rewriting a table with a large body of existing data will
take about as long as you&rsquo;d expect, and in the meantime,
the rewrite will take an <a href="https://www.postgresql.org/docs/current/static/explicit-locking.html"><code>ACCESS EXCLUSIVE</code> lock</a>
on the table. <code>ACCESS EXCLUSIVE</code> is the coarsest
granularity of table lock possible, and it&rsquo;ll block <em>every</em>
other operation until it&rsquo;s released; even simple <code>SELECT</code>
statements have to wait. In any system with a lot of
ongoing access to the table, that&rsquo;s a huge problem.</p>

<figure>
    <img alt="Transactions blocking during a table rewrite." class="overflowing" loading="lazy" src="/assets/images/postgres-default/blocking.svg">
    <figcaption>Transactions blocking during a table rewrite.</figcaption>
</figure>

<p>Historically, accidentally locking access to a table when
adding a column has been a common pitfall for new Postgres
operators because there&rsquo;s nothing in the SQL to tip them
off to the additional expense of adding that <code>DEFAULT</code>
clause. It takes a close reading of <a href="https://www.postgresql.org/docs/10/static/sql-altertable.html">the
manual</a> to find out, or the pyrrhic wisdom
acquired by causing a minor operational incident.</p>

<h2 id="constraints" class="link"><a href="#constraints">Constraints, relaxed by necessity</a></h2>

<p>Because it&rsquo;s not possible to cheaply add a <code>DEFAULT</code>
column, it&rsquo;s also not possible to add a column set to <code>NOT
NULL</code>. By definition non-null columns need to have values
for every row, and you can&rsquo;t add one to a non-empty table
without specifying what values the existing data should
have, and that takes <code>DEFAULT</code>.</p>

<p>You can still get a non-null column by first adding it as
nullable, running a migration to add values to every
existing row, then altering the table with <code>SET NOT NULL</code>,
but even that&rsquo;s not perfectly safe because <code>SET NOT NULL</code>
requires a full stable scan as it verifies the new
constraint across all existing data. The scan is faster
than a rewrite, but still needs an <code>ACCESS EXCLUSIVE</code> lock.</p>

<p>The amount of effort involved in getting a new non-null
column into any large relation means that in practice you
often don&rsquo;t bother. It&rsquo;s either too dangerous, or too time
consuming.</p>

<h2 id="why-bother" class="link"><a href="#why-bother">Why bother with non-null anyway?</a></h2>

<p>One of the biggest reasons to prefer relational databases
over document stores, key/value stores, and other less
sophisticated storage technology is data integrity. Columns
are strongly typed with the likes of <code>INT</code>, <code>DECIMAL</code>, or
<code>TIMESTAMPTZ</code>. Values are constrained with <code>NOT NULL</code>,
<code>VARCHAR</code> (length), or <a href="https://www.postgresql.org/docs/current/static/ddl-constraints.html#DDL-CONSTRAINTS-CHECK-CONSTRAINTS"><code>CHECK</code> constraints</a>.
Foreign key constraints guarantee <a href="https://en.wikipedia.org/wiki/Referential_integrity">referential
integrity</a>.</p>

<p>With a good schema design you can rest assured that your
data is in a high quality state because the very database
is ensuring it. This makes querying or changing it easier,
and prevents an entire class of application-level bugs
caused by data existing in an unexpected state. Enthusiasts
like me have always argued in favor of strong data
constraints, but knew also that new non-null fields often
weren&rsquo;t possible in Postgres when it was running at scale.</p>

<p>Postgres 11 brings in a change that makes <code>ADD COLUMN</code> with
<code>DEFAULT</code> values fast by marshaling them for existing rows
only as necessary. The expensive table rewrite and long
hold on <code>ACCESS EXCLUSIVE</code> are eliminated, and a gaping
hole in Postgres&rsquo; operational story is filled. It will now
be possible to have both strong data integrity and strong
operational guarantees.</p>

<h2 id="under-the-hood" class="link"><a href="#under-the-hood">Appendix: Under the hood</a></h2>

<p>The change adds two new fields to
<a href="https://www.postgresql.org/docs/current/static/catalog-pg-attribute.html"><code>pg_attribute</code></a>, a system table that tracks
information on every column in the database:</p>

<ul>
<li><code>atthasmissing</code>: Set to <code>true</code> when there are missing
default values.</li>
<li><code>attmissingval</code>: Contains the missing value.</li>
</ul>

<p>As scans are returning rows, they check these new fields
and return missing values where appropriate. New rows
inserted into the table pick up the default values as
they&rsquo;re created so that there&rsquo;s no need to check
<code>atthasmissing</code> when returning their contents.</p>

<figure>
    <img alt="Fast column creation with existing rows loading defaults from pg_attribute." class="overflowing" loading="lazy" src="/assets/images/postgres-default/implementation.svg">
    <figcaption>Fast column creation with existing rows loading defaults from pg_attribute.</figcaption>
</figure>

<p>The <code>pg_attribute</code> fields are only used as long as they
have to be. If at any point the table is rewritten,
Postgres takes the opportunity to insert the default value
for every row and unset <code>atthasmissing</code> and
<code>attmissingval</code>.</p>

<p>Due to the relative simplicity of <code>attmissingval</code>, this
optimization only works for default values and function
calls that are <em>non-volatile</em> <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>. Using it with a volatile
function like <code>random()</code> won&rsquo;t set <code>atthasmissing</code> and
adding the default will have to rewrite the table like it
did before. Non-volatile function calls work fine though.
For example, adding <code>DEFAULT now()</code> will put the
transaction&rsquo;s current value of <code>now()</code> into <code>atthasmissing</code>
and all existing rows will inherit it, but any newly
inserted rows will get a current value of <code>now()</code> as you&rsquo;d
expect.</p>

<p>There&rsquo;s nothing all that difficult conceptually about this
change, but its implementation wasn&rsquo;t easy because the
system is complex enough that there&rsquo;s a lot of places where
the new missing values have to be considered. See <a href="https://github.com/postgres/postgres/commit/16828d5c0273b4fe5f10f42588005f16b415b2d8">the
patch</a> that brought it in for full details.</p>


]]></content>
    <published>2018-08-28T16:46:39Z</published>
    <updated>2018-08-28T16:46:39Z</updated>
    <link href="https://brandur.org/postgres-default"></link>
    <id>tag:brandur.org,2018-08-28:postgres-default</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Tweeting for 10,000 Years: An Experiment in Autonomous Software</title>
    <summary>Using the design principles of a 10,000 year clock to build a program that&amp;rsquo;s intended to run on a macro timescale.</summary>
    <content type="html"><![CDATA[<p>Deep inside a mountain in Texas, a clock is being built.
Unlike other clocks, this one is designed to outlast every
other invention of humankind, carefully engineered to
maximize longevity on a scale of time that&rsquo;s
incompatible with our most fundamental intuitions.</p>

<p>The counterweight for its drive mechanism is housed in a
hollowed out shaft that&rsquo;s 500 feet high and 12 feet in
diameter. It&rsquo;s the size of a small car and weighs an
unbelievable 10,000 pounds. The clock&rsquo;s periodic chimes are
controlled by 20 huge gears stacked on top of one another
&ndash; each of which is 8 feet in diameter. It keeps time
through a 6-foot pendulum assembly terminating with
football-sized titanium weights that swing as unhurriedly
as one might imagine from such a leviathan, taking a full
ten seconds to move all the way back and forth. Components
are machined to within tolerances of a fraction of an inch,
rather than thousandths of an inch common in similar
devices, so that they&rsquo;ll keep working as time takes its
inevitable toll through expansion and rust.</p>

<figure>
    <img alt="The design of the orrery to be used in the 10,000 year clock. It shows the relative position of six human-eye visible planets in our solar system." class="overflowing" loading="lazy" src="/assets/images/10000-years/orrery.jpg" srcset="/assets/images/10000-years/orrery@2x.jpg 2x, /assets/images/10000-years/orrery.jpg 1x">
    <figcaption>The design of the orrery to be used in the 10,000 year clock. It shows the relative position of six human-eye visible planets in our solar system.</figcaption>
</figure>

<p>If all goes well, the clock will keep time for 10,000
years. It&rsquo;s called the &ldquo;<a href="https://en.wikipedia.org/wiki/Clock_of_the_Long_Now">Clock of the Long Now</a>&rdquo; and
is a project of the <a href="https://en.wikipedia.org/wiki/Long_Now_Foundation">Long Now Foundation</a>, who aim
to foster that values long-term planning and
responsibility, and counteract what seems to be an
accelerating trend towards an ever shortening attention
span that we see in society today. Their scale is one of
centuries and millennia, and they aim to construct
frameworks that will be functional for 10,000 years and
beyond. As a reminder of this charter, the Long Now
represents years in five digits instead of four &ndash; under
their calendaring system, it&rsquo;s the year 02018.</p>

<h2 id="tweeting" class="link"><a href="#tweeting">How to tweet as long as possible</a></h2>

<p>Software may not be as well suited as a finely engineered
clock to operate on these sorts of geological scales, but
that doesn&rsquo;t mean we can&rsquo;t try to put some of the 10,000
year clock&rsquo;s design principles to work. As seen by the
short functional lifetime of most software, and its
tendency to continually complexify and bloat, our industry
is one that&rsquo;s reliably short-sighted when it comes to
building products that will last.</p>

<p>Software does have some advantages for longevity compared
to a mechanical apparatus. Especially in the age of the
cloud, a well-designed program isn&rsquo;t dependent on any
single host. It can be moved around as the hardware below
it succumbs to the physical realities of entropy, and rely
on its underlying platform to stay stable thanks to the
efforts of human maintainers.</p>

<p>I wanted to write a little experiment inspired by the
10,000 year clock to see how long I could make a simple
program last without my intervention. It&rsquo;s called
<a href="https://github.com/brandur/perpetual">Perpetual</a>, and it has the simple task of
posting a total of ten pre-configured tweets to my timeline
on something close to an exponential scale; the last being
very optimistically scheduled to fire 10,000 years from
now. The first of them went out just a few minutes after
this article was published.</p>

<p><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">LHI000: I put together a few words on what we can learn in software about resilience and long term thinking from the design principles of a 10,000 year clock.<a href="https://t.co/lUOK8IJpsV">https://t.co/lUOK8IJpsV</a></p>&amp;mdash; Brandur (@brandur) <a href="https://twitter.com/brandur/status/1020320298569293824?ref_src=twsrc%5Etfw">July 20, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>Each tweet, or &ldquo;interval&rdquo;, is prefixed with a magic string
and number like <strong>LHI001</strong> (LHI stands for &ldquo;long heartbeat
interval&rdquo;) so that the scheduled tweets are recognizable,
and so that the program can easily find the last one that
it published. Here&rsquo;s the intended timeline:</p>

<figure>
  <div class="table-container">
    <table class="overflowing">
      <tr>
        <th>Interval #</th>
        <th>Tweet prefix</th>
        <th>Scheduled time</th>
      </tr>
      <tr>
        <td align="center">0</td>
        <td align="center">LHI000</td>
        <td align="center">Today</td>
      </tr>
      <tr>
        <td align="center">1</td>
        <td align="center">LHI001</td>
        <td align="center">1 day (from now)</td>
      </tr>
      <tr>
        <td align="center">2</td>
        <td align="center">LHI002</td>
        <td align="center">1 week</td>
      </tr>
      <tr>
        <td align="center">3</td>
        <td align="center">LHI003</td>
        <td align="center">1 month</td>
      </tr>
      <tr>
        <td align="center">4</td>
        <td align="center">LHI004</td>
        <td align="center">1 year</td>
      </tr>
      <tr>
        <td align="center">5</td>
        <td align="center">LHI005</td>
        <td align="center">5 years</td>
      </tr>
      <tr>
        <td align="center">6</td>
        <td align="center">LHI006</td>
        <td align="center">10 years</td>
      </tr>
      <tr>
        <td align="center">7</td>
        <td align="center">LHI007</td>
        <td align="center">100 years</td>
      </tr>
      <tr>
        <td align="center">8</td>
        <td align="center">LHI008</td>
        <td align="center">1,000 years</td>
      </tr>
      <tr>
        <td align="center">9</td>
        <td align="center">LHI009</td>
        <td align="center">10,000 years</td>
      </tr>
    </table>
  </div>
  <figcaption>The scheduled publication time for each tweet/interval.</figcaption>
</figure>

<p>And here&rsquo;s the code that checks for old intervals and
decides whether a new one should be posted (somewhat
simplified for brevity):</p>

<pre><code class="language-go">func Update(api TwitterAPI, intervals []*Interval, now time.Time)
        (int, error) {

    it := api.ListTweets()

    for it.Next() {
        lastTweet = it.Value()

        id, ok = extractIntervalID(lastTweet.Message)
        if ok {
            break
        }
    }

    if it.Err() != nil {
        return -1, it.Err()
    }

    var nextIntervalID int
    if ok {
        // Pick the next interval in the series
        nextIntervalID = id + 1
    } else {
        // If ok is false, we never extracted an interval ID, which
        // means that this program has never posted before. Pick the
        // first interval ID in the series.
        nextIntervalID = 0
    }

    if nextIntervalID &gt;= len(intervals) {
        return -1, nil
    }

    interval := intervals[nextIntervalID]

    if interval.Target.After(now) {
        fmt.Printf(&quot;Interval not ready, target: %v\n&quot;, interval.Target)
        return -1, nil
    }

    tweet, err := api.PostTweet(
        formatInterval(nextIntervalID, interval.Message))
    if err != nil {
        return -1, err
    }

    return nextIntervalID, nil
}
</code></pre>

<h2 id="time" class="link"><a href="#time">Time and scale</a></h2>

<p>It&rsquo;s a cute idea, but as you may have already guessed, my
program won&rsquo;t be tweeting for 10,000 years. It&rsquo;ll be lucky
if it makes it to 10 years, and lucky beyond all reason if
it makes it to 100 (more on this in <a href="#threats">Existential
threats</a> below). Humans tend to have a hard time
imagining increasing orders of magnitude, a fact that&rsquo;s
demonstrated by the well-documented cognitive bias of
<a href="https://en.wikipedia.org/wiki/Scope_neglect">scope insensitivity</a>. We can all do the
basic arithmetic that tells us there are 1,000 ten year
segments in 10,000, but it&rsquo;s difficult to appreciate how
much more time that really is. After some size, all
numbers, whether they&rsquo;re a thousand, ten thousand, a
million, or ten million, are just <em>really big</em>.</p>

<p>Consider that the oldest pyramid, the Pyramid of Djoser at
Saqqara, isn&rsquo;t quite 5,000 years old, and that&rsquo;s <em>ancient</em>.
As young Cleopatra, and who lived contemporaneously with
some of history&rsquo;s other most famous figures like Julius
Caesar, Mark Antony, and Augustus, looked up the huge stone
monuments that were her country&rsquo;s legacy, consider that
they&rsquo;d been constructed further back in history for her
(she was born 69 BC) than she is back in history for us in
2018. There are a few human artifacts from as far back as
10,000 years ago, but they mostly amount to nothing more
than fragments of pots.</p>

<p>But just because the program is unlikely to succeed on its
10,000 year mission doesn&rsquo;t mean that we can&rsquo;t try to
improve its chances.</p>

<figure>
    <img alt="We have many artifacts from ancient humanity, but 10,000 years predates almost all of them." class="overflowing" loading="lazy" src="/assets/images/10000-years/monolith.jpg" srcset="/assets/images/10000-years/monolith@2x.jpg 2x, /assets/images/10000-years/monolith.jpg 1x">
    <figcaption>We have many artifacts from ancient humanity, but 10,000 years predates almost all of them.</figcaption>
</figure>

<h2 id="hedging" class="link"><a href="#hedging">Hedging against failure</a></h2>

<p>The program&rsquo;s goal for longevity is extremely ambitious, so
it&rsquo;s engineered with a number of features that aim to
protect it against the decaying forces of time and make it
as minimally prone to failure:</p>

<ul>
<li><p>It runs on a <strong>serverless</strong> architecture to insulate it
against failures in underlying infrastructure. If a
single server were to die, it would just be run somewhere
else. Its platform will also get regular updates for
security and stability.</p></li>

<li><p>That platform is <strong>AWS Lambda</strong>, a service provided by a
big company (Amazon) that&rsquo;s more likely than others to be
long-lived. It also has a reliable history of <em>not</em>
retiring products, and making relatively few breaking
changes.</p></li>

<li><p>It has <strong>no persistent state</strong> of its own, and instead
relies entirely on state returned from Twitter&rsquo;s API.
Databases are especially prone to aging and operational
problems, and not including one improves the program&rsquo;s
chances.</p></li>

<li><p>In the spirit of <a href="/minimalism">production minimalism</a>,
there are <strong>very few moving parts</strong>: just the program
itself, Twitter&rsquo;s API, and the underlying serverless
platform.</p></li>

<li><p><strong>I&rsquo;m using Go</strong>. As described in <a href="/go-lambda#tenacity">Go on
Lambda</a>, its 1.x series has a
remarkable history of longevity and near perfect
backwards compatibility. Even if Go 2 were to be
released, I expect that there&rsquo;s a good chance that my
program would work with it.</p></li>

<li><p>Relatedly, Go is a <strong>statically typed language</strong> which
means that the code I wrote is more likely to actually
work compared to if it&rsquo;d been written in an interpreted
language where many problems only appear at runtime. I&rsquo;ve
also included a <strong>comprehensive test suite</strong>.</p></li>

<li><p>The program compiles down to a <strong>self-contained binary</strong>
and won&rsquo;t be as susceptible to breakage by a change in
its underlying bootstrap or dependencies (compared to say
Ruby, where an eventual upgrade to Bundler could mean
that your program no longer starts).</p></li>
</ul>

<h2 id="threats" class="link"><a href="#threats">Existential threats</a></h2>

<p>Over this kind of timeline, the program faces many
existential threats. One of them will knock it offline
eventually, with the only question being: which one?</p>

<ul>
<li><p>Maybe the most common of all failures is an
<strong>application bug</strong>. I&rsquo;ve tried to protect against this
pitfall through testing, but I could&rsquo;ve easily overlooked
a subtle edge case.</p></li>

<li><p>Changes in <strong>Twitter&rsquo;s API</strong> could spell the end. This
would take the form of a backwards-incompatible change
like a new required parameter, change in the structure of
responses, or adjustment to how applications authenticate.</p></li>

<li><p>Relatedly, changes in <strong>Twitter&rsquo;s product</strong> are also
dangerous. They could move to a new pricing model,
remodel the product&rsquo;s core design, or fold as a company.</p></li>

<li><p>Risks on <strong>AWS</strong> are similar. There&rsquo;s a minimal API that Go
programs on Lambda use to communicate with the service,
and that could change. The Lambda product could be
retired. I&rsquo;ve set up the program to be able to run only
on free tier, but that could change, or the account it&rsquo;s
running under could become otherwise delinquent.</p></li>

<li><p>If left running long enough, <strong>the binary</strong> I&rsquo;ve upload
to Lambda might become incompatible with the underlying
virtual and hardware infrastructure through changes in
machine code or low level operating system APIs. It would
need to be recompiled with a newer version of Go to work
again.</p></li>
</ul>

<p>I&rsquo;d personally bet that it will be changes in Twitter&rsquo;s API
that will take the program down in the end. Their API has
been stable for some time, but has accumulated its share of
rough edges over the years. It stands to reason that
Twitter eventually undertake a project to revitalize it,
and the chances are that will be the end of the current API
after some deprecation period that&rsquo;s likely to span a
maximum of a handful of years.</p>

<h2 id="learn" class="link"><a href="#learn">What we can learn from a clock</a></h2>

<p>A core set of <a href="http://longnow.org/clock/principles/">guiding principles</a> were devised
to help design the 10,000 year clock:</p>

<ul>
<li><p><strong>Longevity:</strong> The clock should be accurate even after
10,000 years, and must not contain valuable parts (such
as jewels, expensive metals, or special alloys) that
might be looted.</p></li>

<li><p><strong>Maintainability:</strong> Future generations should be able
to keep the clock working, if necessary, with nothing
more advanced than Bronze Age tools and materials.</p></li>

<li><p><strong>Transparency:</strong> The clock should be understandable
without stopping or disassembling it; no functionality
should be opaque.</p></li>

<li><p><strong>Evolvability:</strong> It should be possible to improve the
clock over time.</p></li>

<li><p><strong>Scalability:</strong> It should be possible to build working
models of the clock from table-top to monumental size
using the same design.</p></li>
</ul>

<h3 id="software" class="link"><a href="#software">Rethought for software</a></h3>

<p>The Long Now describe the principles above as &ldquo;generally
good for designing anything to last a long time,&rdquo; and they
are, even when it comes to software. It doesn&rsquo;t take much
creativity to rethink them as a set of values that could
help guide our industry. I&rsquo;d phrase them like this:</p>

<ul>
<li><p><strong>Longevity:</strong> Software should be written as robustly as
possible to maximize longevity. Consider edge cases, test
comprehensively, and use statically typed languages.
Avoid dependencies that are complex or brittle.</p></li>

<li><p><strong>Maintainability:</strong> Use frameworks that will make
software easily maintainable by developers who come after
you. Development should only require a minimal toolchain,
and one that&rsquo;s demonstrated a good history of stability
and support.</p></li>

<li><p><strong>Transparency:</strong> Write code simply and elegantly. Use
abstractions, but don&rsquo;t abstract so heavily as to
obfuscate. It should be obvious how code works not only
to you, but for any others who might read it in the
future.</p></li>

<li><p><strong>Evolvability:</strong> It should be possible to improve
software over time. A good compiler and test suite should
let future developers who aren&rsquo;t deeply familiar with the
existing code make those improvements safely.</p></li>

<li><p><strong>Scalability:</strong> To ensure that production software
will work properly, write an extensive test suite and
deploy the prototype in high-fidelity pre-production
environments before taking it live.</p></li>
</ul>

<p>Software tends to stay in operation longer than we think it
will when we first wrote it, and the wearing effects of
entropy within it and its ecosystem often take their toll
more quickly and more destructively than we could imagine.
You don&rsquo;t need to be thinking on a scale of 10,000 years to
make applying these principles a good idea.</p>

<h2 id="post-mortem" class="link"><a href="#post-mortem">Post-mortem analysis</a></h2>

<p>Updated <strong>April 14, 2023</strong>: entropy won. The official time in operation of this
experiment was 4 years, 8 months. Not half bad, but a little short of the stated
goal.</p>

<p>I got this email from Twitter today:</p>

<blockquote>
<p>This is a notice that your app - 10000-years - has been suspended from
accessing the Twitter API.</p>

<p>Please visit developer.twitter.com to sign up to our new Free, Basic or
E&gt; nterprise access tiers.</p>
</blockquote>

<p>Free access to Twitter&rsquo;s API is being disabled for the vast majority of
applications. A free account is available for write-only operations, but the
program above needs read access to make sure it doesn&rsquo;t double-post. And
updating the project in any way would contradict the spirit of the experiment
anyway.</p>

<p>It turns out that writing software that can stand the test of time isn&rsquo;t easy.</p>
]]></content>
    <published>2018-07-20T13:41:22Z</published>
    <updated>2018-07-20T13:41:22Z</updated>
    <link href="https://brandur.org/10000-years"></link>
    <id>tag:brandur.org,2018-07-20:10000-years</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Living APIs, and the Case for GraphQL</title>
    <summary>Why it makes sense to model APIs as graphs, and what GraphQL can do for us to help with discovery, batch operations, and gradual enhancement.</summary>
    <content type="html"><![CDATA[<p>It&rsquo;s hard to read exactly where GraphQL stands in the API
world right now. Available publicly since 2015, trends in
APIs aren&rsquo;t obviously moving in its favor, but not
obviously moving against it either. Interest from the
developer community has been steady throughout even if the
technology isn&rsquo;t spreading like wildfire.</p>

<p>Its biggest third party proponent is GitHub, who released
the fourth version of their API as GraphQL in 2016 with an
<a href="https://github.blog/2016-09-14-the-github-graphql-api/">engineering post</a> speaking about it very
favorably. It also has a other vocal users in the form of
Shopify and Yelp, both of whom offer public GraphQL APIs.
But beyond those big three, other big providers are
somewhat harder to find. <a href="https://github.com/APIs-guru/graphql-apis">This repository</a>
keeps a list of publicly available GraphQL APIs, and most
well-known API providers are notably absent, including
Facebook themselves <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>.</p>

<p>Most publicly proffered APIs are still &ldquo;REST-ish&rdquo; &ndash; with
resources and actions offered over HTTP &ndash; including those
from almost every name you&rsquo;d recognize in the space:
Amazon, Dropbox, Google, Microsoft, Stripe, and Twilio.
Momentum plays a huge part in that the pattern is
widespread and developers are used to it both on the parts
of integrators using APIs, and those who are building them.
Some arguments are still made that strict adherence to REST
and hypermedia will open a wide world of automatic
discoverability and adaptation, but lack of real world
precedent despite years of opportunity seems to be a strong
empirical suggestion that this vision is a
will-o&rsquo;-the-wisp.</p>

<p>GraphQL&rsquo;s biggest problem may be that although it&rsquo;s better,
it&rsquo;s not &ldquo;better enough&rdquo;. The bar set by REST is low, but
it&rsquo;s high enough to work, and is adequate for most
purposes.</p>

<p>I&rsquo;ve been doing a lot of thinking about what a new
generation of web APIs would look like (or if there will be
one at all), and I for one, would like to see more GraphQL.
I&rsquo;ll try to articulate a few arguments for why it&rsquo;s a good
idea that go beyond the common surface-level selling
points.</p>

<h2 id="surface" class="link"><a href="#surface">The surface</a></h2>

<p>I&rsquo;ll defer to the <a href="https://graphql.org/learn/">official introduction</a> as a good
resource to get familiar with GraphQL&rsquo;s basics, but it has
a few important core ideas that are worth touching upon.</p>

<p>With GraphQL, fields and relationships must be requested
<strong>explicitly</strong>. Here we ask for a user object including the
<code>currency</code>, <code>email</code>, and <code>subscriptions</code> fields:</p>

<pre><code class="language-js">getUser(id: &quot;user_123&quot;) {
  currency,
  email,
  subscriptions
}
</code></pre>

<p>There&rsquo;s no wildcard operator like a <code>SELECT *</code> from SQL.
Compared to REST, this has an advantage of reducing payload
size (especially helpful for mobile), but more importantly,
it establishes an explicit contract between the client and
server which allow APIs to be evolved more gracefully.
We&rsquo;ll talk about this more below.</p>

<p>GraphQL is automatically <strong>introspectable</strong> online. By
using the special <code>__type</code> operator, any client can get a
detailed understanding of a type and all its fields and
documentation:</p>

<pre><code class="language-js">{
  __type(name: &quot;User&quot;) {
    name
    fields {
      name
      type {
        name
      }
    }
  }
}
</code></pre>

<p>Every common implementation supports introspection (it&rsquo;s
required in <a href="https://graphql.github.io/graphql-spec/">the GraphQL spec</a>) and tooling can be
built to rely on it being available. Unlike REST, there&rsquo;s
no need to retrofit an unstandardized description language
like OpenAPI (or its myriad of competitors). Even today,
these are usually not available, and often not completely
accurate because the description isn&rsquo;t tied directly to the
implementation.</p>

<p>Finally, GraphQL is <strong>typed</strong>. Types often come in the form
of complex objects (e.g., <code>User</code>) or JSON scalars (e.g.,
int, string), but the type system also supports more
sophisticated features like enumerations, interfaces, and
union types. Nullability is baked in, which happens to work
out incredibly well when building APIs in languages that
don&rsquo;t allow null (like Rust) because every field comes out
as non-nullable by default. This additional constraint
makes handling API responses more deterministic and less
prone to error.</p>

<figure>
    <img alt="The relationships between people in a town are a graph. This is a stretch (but I like this photo)." class="overflowing" loading="lazy" src="/assets/images/graphql/village.jpg" srcset="/assets/images/graphql/village@2x.jpg 2x, /assets/images/graphql/village.jpg 1x">
    <figcaption>The relationships between people in a town are a graph. This is a stretch (but I like this photo).</figcaption>
</figure>

<h2 id="graph" class="link"><a href="#graph">The graph</a></h2>

<p>As its name would suggest, GraphQL models objects as a
graph. Technically, the graph starts with a root node that
branches into query and mutation nodes, which then descend
into API-specific resources.</p>

<p>GraphQL takes existing API paradigms to a logical
conclusion. Almost every REST API that exists today is
already a graph, but one that&rsquo;s more difficult to traverse.
Resources reference other resources by IDs (or links in
APIs which most strongly adhere to the principles of REST),
and relations are fetched with new HTTP requests. Making
relationships explicit is conceptually sound, and lets
consumers get work done with fewer API calls.</p>

<p>Stripe&rsquo;s API has a concept called <a href="https://stripe.com/docs/api/expanding_objects">object
expansion</a> that lets a user tell the server that it
would like an ID (e.g., <code>cus_123</code>) expanded into its full
object representation by passing an <code>expand[]=...</code>
parameter in with the request. Expansions are chainable, so
I can ask for <code>charge.customer</code> on a dispute to reveal the
dispute&rsquo;s associated charge, and that charge&rsquo;s customer.
The feature&rsquo;s most common effect is saving API calls &ndash;
instead of having to request two objects separately, just
one request can be made for the first object with the
second embedded. Users make extensive use of this feature
&ndash; we constrain expansions to three levels deep, but get
regular requests to allow up to four levels.</p>

<h2 id="discovery" class="link"><a href="#discovery">Discovery and exploration</a></h2>

<p>A core challenge of every API is making it approachable to
new users, and providing interactive way to explore them
and make ad-hoc requests is a great way to address that.
GraphQL provides an answer to this in the form of
<a href="https://github.com/graphql/graphiql">GraphiQL</a>, an in-browser tool that lets users
read documentation and build queries.</p>

<p>I&rsquo;d highly recommend taking a look at Shopify&rsquo;s <a href="https://help.shopify.com/en/api/custom-storefronts/storefront-api/graphql-explorer">public
installation</a> and trying some for
yourself. Remember to use the &ldquo;Docs&rdquo; link in the upper
right to pop open and explore the documentation. You should
find yourself being able to build a query that delves 4+
relations deep without much trouble.</p>

<figure>
    <img alt="Using GraphiQL to explore an API and graph." class="overflowing" loading="lazy" src="/assets/images/graphql/graphiql.png" srcset="/assets/images/graphql/graphiql@2x.png 2x, /assets/images/graphql/graphiql.png 1x">
    <figcaption>Using GraphiQL to explore an API and graph.</figcaption>
</figure>

<p>A vanilla installation of GraphiQL is a more powerful
integration tool for users than what 99% of REST providers
have, and it&rsquo;s available automatically (modulo a little
configuration for authentication, CORS, etc.), and for
free.</p>

<p>It&rsquo;s also worth remembering that GraphiQL&rsquo;s features are
built right onto the standard GraphQL introspection
primitives &ndash; it&rsquo;s just an HTML and JavaScript file that
can be hosted statically. For a big provider, building a
custom version of it that&rsquo;s tailored to the features and
layout of a specific API is well within reason.</p>

<h2 id="batch" class="link"><a href="#batch">Batch operations</a></h2>

<p>Every sufficiently long-lived web API that responds to user
feedback will eventually evolve a batch API.</p>

<p>In REST APIs, that involves building a custom batch
specification because there&rsquo;s nothing even close to wide
standardization for such a thing. Users adapt to each
exotic implementation by reading a lot of documentation. In
GraphQL, batch queries are built right in. Here&rsquo;s a
document containing multiple operations on the same query
and which uses aliases (<code>userA</code>, <code>userB</code>) so that the
results are disambiguated in the response:</p>

<pre><code class="language-js">userA: getUser(id: &quot;user_123&quot;) {
  email
}

userB: getUser(id: &quot;user_456&quot;) {
  email
}
</code></pre>

<p>Batch mutations are also allowed.</p>

<p>The availability of this feature doesn&rsquo;t necessarily give
users free reign the ability to run costly batch requests.
Remember that as an API provider, you can still put
restrictions on this within reason. For example, by
allowing only five operations per request (if that&rsquo;s the
right fit for you), or even just one.</p>

<h2 id="explicitness" class="link"><a href="#explicitness">Explicitness and graceful enhancement</a></h2>

<p>I mentioned above how fields in GraphQL must be requested
explicitly and that there&rsquo;s no SQL-like glob operator
(<code>SELECT *</code>) to get everything. This might be GraphQL&rsquo;s
most interesting feature because it lends itself so well to
API versioning and enhancement.</p>

<p>In a REST API, an API provider must assume that for any
given API resource, <em>every</em> field is in use by every user
because they have no insight at all into which ones they&rsquo;re
actually using. Removing any field must be considered a
breaking change and <a href="https://stripe.com/blog/api-versioning">an appropriate versioning
system</a> will need to be installed to manage
those changes.</p>

<p>In GraphQL, every contract is explicit and observable.
Providers can use something like a <a href="/canonical-log-lines">canonical log
line</a> to get perfect insight into the
fields that are in use for every request, and use that
information to make decisions around product development,
API changes, and retirement. For example, when introducing
a new field, we can explicitly measure its use over time to
see how successful it is. Alternatively, if we notice that
a field is only in use by a tiny fraction of users and it
fits poorly into the API&rsquo;s design or is expensive to
maintain, it&rsquo;s a good candidate for deprecation and
eventual removal.</p>

<h3 id="living-apis" class="link"><a href="#living-apis">Living APIs</a></h3>

<p>The REST model of little insight tends to produce APIs with
a strong tendency to ossify, with broad and abrupt changes
made intermittently with new versions. GraphQL produces an
environment that evolves much more gradually.</p>

<p>Fields that need to be phased out can be initially hidden
from documentation by marking them with GraphQL&rsquo;s built-in
<code>deprecated</code> annotation. From there, providers may choose
to even further restrict their use by gating in users who
were already consuming them, and disallowing everyone else,
possibly with an automatic process to remove those gated
exceptions as users upgrade organically over time and move
away from those deprecated fields. After a long grace
period, their use can be analyzed, and product teams can
start an active outreach campaign for total retirement
before removing them entirely.</p>

<p>Similarly, new fields are introduced one at a time and
their adoption can be observed immediately. Like a living
thing, the API changes little by little. New features are
added and old mistakes are fixed. It trends towards
maturity incrementally in a distant perfect form.</p>

<figure>
    <img alt="In the ideal case, we produce APIs that grow and improve like living things. My hands were really cold when I shot this." class="overflowing" loading="lazy" src="/assets/images/graphql/living.jpg" srcset="/assets/images/graphql/living@2x.jpg 2x, /assets/images/graphql/living.jpg 1x">
    <figcaption>In the ideal case, we produce APIs that grow and improve like living things. My hands were really cold when I shot this.</figcaption>
</figure>

<h2 id="convention" class="link"><a href="#convention">Shared convention and leverage</a></h2>

<p>GraphQL introduces many powerful ideas, and because it was
written in response to extensive real-world experience, it
addresses API scaling problems that most would-be API
designers wouldn&rsquo;t think about until it was too late.</p>

<p>It comes with a <a href="https://graphql.github.io/graphql-spec/">comprehensive spec</a> to help avoid
ambiguities. The result is that most GraphQL APIs look very
similar and features are widespread throughout all common
implementations. I&rsquo;d personally like to see its designers
take an even more opinionated stance on conventions like
naming, mutation granularity, and pagination, but even
without, it&rsquo;s still a far more sophisticated set of
constraints than what we have with REST. This forced
consistency leads to leverage in the form of tools like
GraphiQL (and many more to come) that can be shared amongst
any of its implementations.</p>

<p>REST&rsquo;s momentum may appear unstoppable, but underdesign and
loose conventions leave a lot to be desired. We&rsquo;d be doing
ourselves a favor by keeping our gaze on the horizon.</p>


]]></content>
    <published>2018-06-08T19:26:48Z</published>
    <updated>2018-06-08T19:26:48Z</updated>
    <link href="https://brandur.org/graphql"></link>
    <id>tag:brandur.org,2018-06-08:graphql</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Scaling a High-traffic Rate Limiting Stack with Redis Cluster</title>
    <summary>Flattening a single very hot vertical Redis node into a horizontal Redis Cluster at Stripe.</summary>
    <content type="html"><![CDATA[<p>Redis is the often unspoken workhorse of production. It&rsquo;s
not often used as a primary data store, but it has a sweet
spot in storing and accessing ephemeral data whose loss can
be tolerated &ndash; metrics, session state, caching &ndash; and it
does so <em>fast</em>, providing not only optimal performance, but
efficient algorithms on a useful set of built-in data
structures. It&rsquo;s one of the most common staples in the
modern technology stack.</p>

<p>Stripe&rsquo;s rate limiters are built on top of Redis, and until
recently, they ran on a single <em>very hot</em> instance of
Redis. The server had followers in place for failover, but
at any given time, one node was handling every operation.</p>

<p>You have to admire a system where this is even possible.
Various sources claim that Redis can handle a million
operations a second on one node &ndash; we weren&rsquo;t doing that
many, but we were doing a lot. Every rate limiting check
requires multiple Redis commands to run, and every API
request passes through many rate limiters. One node was
handling on the scale of tens to hundreds of thousands of
operations per second <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>.</p>

<p>The node&rsquo;s total saturation was leading to us seeing an
ambient level of failures happening around the clock. Our
services were written to tolerate Redis unavailability so
most of the time this was okay, but the severity of the
problem would escalate under certain conditions. We
eventually solved it by migrating to a 10-node <a href="https://redis.io/topics/cluster-tutorial">Redis
Cluster</a>. Impact on performance was
negligible, and we now have an easy knob to turn for
horizontal scalability.</p>

<p>The before and after error cliff <sup id="footnote-2-source"><a href="#footnote-2">2</a></sup>:</p>

<figure>
    <img alt="Errors subsiding after a transition to Redis Cluster." class="overflowing" loading="lazy" src="/assets/images/redis-cluster/errors.png" srcset="/assets/images/redis-cluster/errors@2x.png 2x, /assets/images/redis-cluster/errors.png 1x">
    <figcaption>Errors subsiding after a transition to Redis Cluster.</figcaption>
</figure>

<h2 id="limits" class="link"><a href="#limits">The limits of operation</a></h2>

<p>Before replacing a system, it&rsquo;s worth understanding the
cause and effect that led the original to fail.</p>

<p>A property of Redis that&rsquo;s worth understanding is that it&rsquo;s
a single-threaded program. This isn&rsquo;t <em>strictly</em> true
anymore because background threads handle some operations
like object deletion, but it&rsquo;s practically true in that all
executing operations block on access to a single flow
control point. It&rsquo;s relatively easy to understand how this
came about &ndash; Redis&rsquo; guarantee around the atomicity of any
given operation (be it a single command, <code>MULTI</code>, or
<code>EXEC</code>) stems from the fact that it&rsquo;s only executing one of
the them at a time. Even so, there are some obvious
parallelism opportunities, and <a href="https://redis.io/topics/faq#redis-is-single-threaded-how-can-i-exploit-multiple-cpu--cores">notes in the FAQ</a>
suggest that the intention is to start investigating a more
threaded design beyond 5.0.</p>

<p>That single-threaded model was indeed our bottleneck. You
could log onto the original node and see a single core
pegged at 100% usage.</p>

<h3 id="intersecting-failures" class="link"><a href="#intersecting-failures">Intersecting failures</a></h3>

<p>Even operating right at maximum capacity, we found Redis to
degrade quite gracefully. The main manifestation was an
increased rate of baseline connectivity errors as observed
from the nodes talking to Redis &ndash; in order to be tolerant
of a malfunctioning Redis they were constrained with
aggressive connect and read timeouts (~0.1 seconds), and
couldn&rsquo;t establish a connection of execute an operation
within that time when dealing with an overstrained host.</p>

<p>Although not optimal, the situation was okay most of the
time. It only became a real problem came in when we were
targeted with huge surges of illegitimate traffic (i.e.,
orders of magnitude over allowed limits) from <em>legitimate</em>
users who could authenticate successfully and run expensive
operations on the underlying database. That&rsquo;s expensive in
the relative sense &ndash; even returning a set of objects from
a list endpoint is far more expensive than denying the
request with a <code>401</code> because its authentication wasn&rsquo;t
valid, or with a <code>429</code> because it&rsquo;s over limit. These
surges are often the result of users building and running
high-concurrency scraping programs.</p>

<p>These traffics spikes would lead to a proportional increase
in error rate, and much of that traffic would be allowed
through as rate limiters defaulted to allowing requests
under error conditions. That would put increased pressure
on the backend database, which doesn&rsquo;t fail as gracefully
as Redis when overloaded. It&rsquo;s prone to seeing partitions
becoming almost wholly inoperable and timing out a sizable
number of the requests made to them.</p>

<h2 id="sharding" class="link"><a href="#sharding">Redis Cluster's sharding model</a></h2>

<p>A core design value of Redis is speed, and Redis Cluster is
structured so as not to compromise that. Unlike many other
distributed models, operations in Redis Cluster aren&rsquo;t
confirming on multiple nodes before reporting a success,
and instead look a lot more like a set of independent
Redis&rsquo; sharing a workload by divvying up the total space of
possible work. This sacrifices high availability in favor
of keeping operations fast &ndash; the additional overhead of
running an operation against a Redis Cluster is negligible
compared to a standard Redis standalone instance.</p>

<p>The total set of possible keys are divided into 16,384
<strong><em>slots</em></strong>. A key&rsquo;s slot is calculated with a stable
hashing function that all clients know how to do:</p>

<pre><code>HASH_SLOT = CRC16(key) mod 16384
</code></pre>

<p>For example, if we wanted to execute <code>GET foo</code>, we&rsquo;d get
the slot number for <code>foo</code>:</p>

<pre><code>HASH_SLOT = CRC16(&quot;foo&quot;) mod 16384 = 12182
</code></pre>

<p>Each node in a cluster will handle some fraction of those
total 16,384 slots, with the exact number depending on the
number of nodes. Nodes communicate with each other to
coordinate slot distribution, availability, and
rebalancing.</p>

<figure>
    <img alt="The set of hash slots spread across nodes in a cluster." class="overflowing" loading="lazy" src="/assets/images/redis-cluster/hash-slots.svg">
    <figcaption>The set of hash slots spread across nodes in a cluster.</figcaption>
</figure>

<p>Clients use the <code>CLUSTER</code> family of commands to query a
cluster&rsquo;s state. A common operation is <code>CLUSTER NODES</code> to
get a mapping of slots to nodes, the result of which is
generally cached locally as long as it stays fresh.</p>

<pre><code>127.0.0.1:30002 master - 0 1426238316232 2 connected 5461-10922
127.0.0.1:30003 master - 0 1426238318243 3 connected 10923-16383
127.0.0.1:30001 myself,master - 0 0 1 connected 0-5460
</code></pre>

<p>I&rsquo;ve simplified the output above, but the important parts
are the host addresses in the first column and the numbers
in the last. <code>5461-10922</code> means that this node handles the
range of slots starting at <code>5461</code> and ending at <code>10922</code>.</p>

<h3 id="moved" class="link"><a href="#moved">`MOVED` redirection</a></h3>

<p>If a node in a Redis Cluster receives a command for a key
in a slot that it doesn&rsquo;t handle, it makes no attempt to
forward that command elsewhere. Instead, the client is told
to try again somewhere else. This comes in the form of a
<code>MOVED</code> response with the address of the new target:</p>

<pre><code>GET foo
-MOVED 3999 127.0.0.1:6381
</code></pre>

<p>During a cluster rebalancing, slots migrate from one
node to another, and <code>MOVED</code> is an important signal that
servers use to tell a client its local mappings of slots to
nodes are stale.</p>

<figure>
    <img alt="A slot migrating from one node to another." class="overflowing" loading="lazy" src="/assets/images/redis-cluster/moved-redirection.svg">
    <figcaption>A slot migrating from one node to another.</figcaption>
</figure>

<p>Every node knows the current slot mapping, and in theory a
node that receives an operation that it can&rsquo;t handle could
ask the right node for the result and forward it back to
the client, but sending <code>MOVED</code> instead is a deliberate
design choice. It trades of some additional client
implementation complexity for fast and deterministic
performance. As long as a client&rsquo;s mappings are fresh,
operations are always executed in just one hop. Because
rebalancing is relatively rare, the coordination overhead
amortized over the cluster&rsquo;s lifetime is negligible.</p>

<p>Besides <code>MOVED</code>, there are a few other cluster-specific
mechanics at work, but I&rsquo;m going to skip them for brevity.
The <a href="https://redis.io/topics/cluster-spec">full specification</a> is a great resource for a
more thorough look at how Redis Cluster works.</p>

<h3 id="client" class="link"><a href="#client">How clients execute requests</a></h3>

<p>Redis clients need a few extra features to support Redis
Cluster, with the most important ones being support for the
key hashing algorithm and a scheme to maintain slot to
node mappings so that they know where to dispatch commands.</p>

<p>Generally, a client will operate like this:</p>

<ol>
<li>On startup, connect to a node and get a mapping table
with <code>CLUSTER NODES</code>.</li>
<li>Execute commands normally, targeting servers according
to key slot and slot mapping.</li>
<li>If <code>MOVED</code> is received, return to 1.</li>
</ol>

<p>A multi-threaded client can be optimized by having it
merely mark the mappings table dirty when receiving
<code>MOVED</code>, and have threads executing commands follow <code>MOVED</code>
responses with new targets while a background thread
refreshes the mappings asynchronously. In practice, even
while rebalancing the likelihood is that most slots won&rsquo;t
be moving, so this model allows <em>most</em> commands to continue
executing with no overhead.</p>

<h3 id="hash-tags" class="link"><a href="#hash-tags">Localizing multi-key operations with hash tags</a></h3>

<p>It&rsquo;s common in Redis to run operations that operate on
multiple keys through the use of the <code>EVAL</code> command with a
custom Lua script. This is an especially important feature
for implementing rate limiting, because all the work
dispatched via a single <code>EVAL</code> is guaranteed to be atomic.
This allows us to correctly calculate remaining quotas even
when there are concurrent operations that might conflict.</p>

<p>A distributed model would make this type of multi-key
operation difficult. Because the slot of each key is
calculated via hash, there&rsquo;d be no guarantee that related
keys would map to the same slot. My keys
<code>user123.first_name</code> and <code>user123.last_name</code>, obviously
meant to belong together, could end up on two completely
different nodes in the cluster. An <code>EVAL</code> that read from
both of them wouldn&rsquo;t be able to run on a single node
without an expensive remote fetch from another.</p>

<p>Say for example we have an <code>EVAL</code> operation that
concatenates a first and last name to produce a person&rsquo;s
full name:</p>

<pre><code># Gets the full name of a user
EVAL &quot;return redis.call('GET', KEYS[1]) .. ' ' .. redis.call('GET', KEYS[2])&quot;
    2 &quot;user123.first_name&quot; &quot;user123.last_name&quot;
</code></pre>

<p>A sample invocation:</p>

<pre><code>&gt; SET &quot;user123.first_name&quot; William
&gt; SET &quot;user123.last_name&quot; Adama

&gt; EVAL &quot;...&quot; 2 &quot;user123.first_name&quot; &quot;user123.last_name&quot;
&quot;William Adama&quot;
</code></pre>

<p>This script wouldn&rsquo;t run correctly if Redis Cluster didn&rsquo;t
provide a way for it to do so. Luckily it does through the
use of <strong><em>hash tags</em></strong>.</p>

<p>The Redis Cluster answer to <code>EVAL</code>s that would require
cross-node operations is to disallow them (a choice that
once again optimizes for speed). Instead, it&rsquo;s the user&rsquo;s
jobs to ensure that the keys that are part of any
particular <code>EVAL</code> map to the same slot by hinting how a
key&rsquo;s hash should be calculated with a hash tag. Hash tags
look like curly braces in a key&rsquo;s name, and they dictate
that only the surrounded part of the key is used for
hashing.</p>

<p>We&rsquo;d fix our script above by redefining our keys to only
hash their shared <code>user123</code>:</p>

<pre><code>&gt; EVAL &quot;...&quot; 2 &quot;{user123}.first_name&quot; &quot;{user123}.last_name&quot;
</code></pre>

<p>And when calculating a slot for one of them:</p>

<pre><code>HASH_SLOT = CRC16(&quot;{user123}.first_name&quot;) mod 16384
          = CRC16(&quot;user123&quot;) mod 16384
          = 13438
</code></pre>

<p><code>{user123}.first_name</code> and <code>{user123}.last_name</code> are now
guaranteed to map to the same slot, and <code>EVAL</code> operations
that contain both of them will be trouble-free. This is
only a basic example, but the same concept maps all the way
up to a complex rate limiter implementation.</p>

<h2 id="simple" class="link"><a href="#simple">Simple and reliable</a></h2>

<p>Transitioning over to Redis Cluster went remarkably
smoothly, with the most difficult part being shoring up one
of the Redis Cluster clients for production use. Even to
this day, good client support is somewhat spotty, which may
be an indication that Redis is fast enough that most people
using it can get away with a simple standalone instance.
Our error rates took a nosedive, and we&rsquo;re confident that
we have ample runway for continued growth.</p>

<p>Philosophically, there&rsquo;s a lot to like about Redis
Cluster&rsquo; design &ndash; simple, yet powerful. Especially when it
comes to distributed systems, many implementations are
exceedingly complicated, and that level of complexity can
be catastrophic when encountering a tricky edge in
production. Redis Cluster is scalable, and yet with few
enough moving parts that even a layperson like myself can
wrap their head around what it&rsquo;s doing. Its <a href="https://redis.io/topics/cluster-spec">design
doc</a> is comprehensive, but also approachable.</p>

<p>In the months since setting it up, I haven&rsquo;t touched it
again even once, despite the considerable load it&rsquo;s under
every second of the day. This is a rare quality in
production systems, and not found even amongst some of my
other favorites like Postgres. We need more building blocks
like Redis that do what they&rsquo;re supposed to, then get out
of the way.</p>

<figure>
    <img alt="Your daily dose of tangentially related photography: Stone at the top of Massive Mountain in Alberta sharding into thin flakes." class="overflowing" loading="lazy" src="/assets/images/redis-cluster/sharding.jpg" srcset="/assets/images/redis-cluster/sharding@2x.jpg 2x, /assets/images/redis-cluster/sharding.jpg 1x">
    <figcaption>Your daily dose of tangentially related photography: Stone at the top of Massive Mountain in Alberta sharding into thin flakes.</figcaption>
</figure>


]]></content>
    <published>2018-04-26T17:29:17Z</published>
    <updated>2018-04-26T17:29:17Z</updated>
    <link href="https://brandur.org/redis-cluster"></link>
    <id>tag:brandur.org,2018-04-26:redis-cluster</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Touring a Fast, Safe, and Complete(ish) Web Service in Rust</title>
    <summary>A detailed look at the frameworks, concurrency model, error handling, middleware constructs, and testing strategies of a web service written in Rust.</summary>
    <content type="html"><![CDATA[<p>For years now, I&rsquo;ve been having a crisis of faith in
interpreted languages. They&rsquo;re fast and fun to work in at
small scale, but when you have a project that gets big,
their attractive veneer quickly washes away. A big Ruby
or JavaScript (just to name a few) program in production is
a never ending game of whack-a-mock &ndash; you fix one problem
only to find a new one somewhere else. No matter how many
tests you write, or how well-disciplined your team, any new
development is sure to introduce a stream of bugs that will
need to be shored up over the course of months or years.</p>

<p>Central to the problem are the edges. People will reliably
do a good job of building and testing the happy paths, but
as humans we&rsquo;re <em>terrible</em> at considering the edge
conditions, and it&rsquo;s those edges and corners that cause
trouble over the years that a program is in service.</p>

<p>Constraints like a compiler and a discerning type system
are tools that help us to find and think about those edges.
There&rsquo;s a spectrum of permissiveness across the world of
programming languages, and my thesis right now is that more
time spent in development satisfying a language&rsquo;s rules
will lead to less time spent fixing problems online.</p>

<h2 id="rust" class="link"><a href="#rust">Rust</a></h2>

<p>If it&rsquo;s possible to build more reliable systems with
programming languages with stricter constraints, what about
languages with the <em>strongest</em> constraints? I&rsquo;ve skewed all
the way to the far end of the spectrum and have been
building a web service in Rust, a language infamous for its
uncompromising compiler.</p>

<p>The language is still new and somewhat impractical. It&rsquo;s
been a slog learning its rules around types, ownership, and
lifetimes. Despite the difficulty, it&rsquo;s been an interesting
learning experience throughout, and it&rsquo;s working. I run
into fewer forgotten edge conditions and runtime errors are
way down. Broad refactoring is no longer terror-inducing.</p>

<p>Here we&rsquo;ll run through some of the more novel ideas and
features of Rust, its core libraries, and various
frameworks that make this possible.</p>

<h2 id="foundation" class="link"><a href="#foundation">The foundation</a></h2>

<p>I built my service on <a href="https://github.com/actix/actix-web"><code>actix-web</code></a>, a web
framework layered on <a href="https://github.com/actix/actix"><code>actix</code></a>, an actor library for
Rust. <code>actix</code> is similar to what you might see in a
language like Erlang, except that it adds another degree of
robustness and speed by making heavy use of Rust&rsquo;s
sophisticated type and concurrency systems. For example,
it&rsquo;s not possible for an actor to receive a message that it
can&rsquo;t handle at runtime because it would have been
disallowed at compile-time.</p>

<p>There&rsquo;s a small chance that you&rsquo;ll recognize the name
because <code>actix-web</code> has made its way to the top of the
<a href="https://www.techempower.com/benchmarks/#section=data-r15&amp;hw=ph&amp;test=plaintext">TechEmpower benchmarks</a>. Programs built for
these sorts of benchmarks often turn out to be a little
contrived due to their optimizations, but its now contrived
Rust code that&rsquo;s sitting right up at the top of the list
with contrived C++ and Java code. But regardless of how you
feel about the validity of benchmark programs, the takeaway
is that <code>actix-web</code> is <em>fast</em>.</p>

<figure>
    <img alt="Rust is consistently ranking alongside C++ and Java on TechEmpower." class="overflowing" loading="lazy" src="/assets/images/rust-web/techempower.png" srcset="/assets/images/rust-web/techempower@2x.png 2x, /assets/images/rust-web/techempower.png 1x">
    <figcaption>Rust is consistently ranking alongside C++ and Java on TechEmpower.</figcaption>
</figure>

<p>The author of <code>actix-web</code> (and <code>actix</code>) commits a
prodigious amount of code &ndash; the project is only about six
months old, and not only is already more feature-complete
and with better APIs than web frameworks seen in other open
source languages, but more so than many of the frameworks
bankrolled by large organizations with huge development
teams. Niceties like HTTP/2, WebSockets, steaming
responses, graceful shutdown, HTTPS, cookie support, static
file serving, and good testing infrastructure are readily
available out of the box. The documentation is still a bit
rough, but I&rsquo;ve yet to run into a single bug.</p>

<h3 id="diesel" class="link"><a href="#diesel">Diesel and compile-time query checking</a></h3>

<p>I&rsquo;ve been using <a href="http://diesel.rs/"><code>diesel</code></a> as an ORM to talk to
Postgres. The most comforting thing about the project is
that it&rsquo;s an ORM written by someone with a lot of past
experience with building ORMs, having spent considerable
time in the trenches with Active Record. Many of the
pitfalls common to earlier generations of ORMs have been
avoided &ndash; for example, <code>diesel</code> doesn&rsquo;t try to pretend
that SQL dialects across every major database are the same,
it excludes a custom DSL for migrations (raw SQL is used
instead), and it doesn&rsquo;t do automagical connection
management at the global level. It <em>does</em> bake powerful
Postgres features like upsert and <code>jsonb</code> right into the
core library, and provides powerful safety mechanics
wherever possible.</p>

<p>Most of my database queries are written using <code>diesel</code>&rsquo;s
type-safe DSL. If I misreference a field, try to insert a
tuple into the wrong table, or even produce an impossible
join, the compiler tells me about it. Here&rsquo;s a typical
operation (in this case, a Postgres batch <code>INSERT INTO ...
ON CONFLICT ...</code>, or &ldquo;upsert&rdquo;):</p>

<pre><code class="language-rust">time_helpers::log_timed(&amp;log.new(o!(&quot;step&quot; =&gt; &quot;upsert_episodes&quot;)), |_log| {
    Ok(diesel::insert_into(schema::episode::table)
        .values(ins_episodes)
        .on_conflict((schema::episode::podcast_id, schema::episode::guid))
        .do_update()
        .set((
            schema::episode::description.eq(excluded(schema::episode::description)),
            schema::episode::explicit.eq(excluded(schema::episode::explicit)),
            schema::episode::link_url.eq(excluded(schema::episode::link_url)),
            schema::episode::media_type.eq(excluded(schema::episode::media_type)),
            schema::episode::media_url.eq(excluded(schema::episode::media_url)),
            schema::episode::podcast_id.eq(excluded(schema::episode::podcast_id)),
            schema::episode::published_at.eq(excluded(schema::episode::published_at)),
            schema::episode::title.eq(excluded(schema::episode::title)),
        ))
        .get_results(self.conn)
        .chain_err(|| &quot;Error upserting podcast episodes&quot;)?)
})
</code></pre>

<p>More complex SQL is difficult to represent using the DSL,
but luckily there&rsquo;s a great alternative in the form of
Rust&rsquo;s built-in <code>include_str!</code> macro. It ingests a file&rsquo;s
contents during compilation, and we can easily hand them
off them to <code>diesel</code> for parameter binding and execution:</p>

<pre><code class="language-rust">diesel::sql_query(include_str!(&quot;../sql/cleaner_directory_search.sql&quot;))
    .bind::&lt;Text, _&gt;(DIRECTORY_SEARCH_DELETE_HORIZON)
    .bind::&lt;BigInt, _&gt;(DELETE_LIMIT)
    .get_result::&lt;DeleteResults&gt;(conn)
    .chain_err(|| &quot;Error deleting directory search content batch&quot;)
</code></pre>

<p>The query lives in its own <code>.sql</code> file:</p>

<pre><code class="language-sql">WITH expired AS (
    SELECT id
    FROM directory_search
    WHERE retrieved_at &lt; NOW() - $1::interval
    LIMIT $2
),
deleted_batch AS (
    DELETE FROM directory_search
    WHERE id IN (
        SELECT id
        FROM expired
    )
    RETURNING id
)
SELECT COUNT(*)
FROM deleted_batch;
</code></pre>

<p>We lose compile-time SQL checking with this approach, but
we gain direct access to the raw power of SQL&rsquo;s semantics,
and great syntax highlighting in your favorite editor.</p>

<h2 id="concurreny-model" class="link"><a href="#concurreny-model">A fast (but not the fastest) concurrency model</a></h2>

<p><code>actix-web</code> is powered by <a href="https://github.com/tokio-rs/tokio"><code>tokio</code></a>, a fast event
loop library that&rsquo;s the cornerstone of Rust&rsquo;s concurrency
story <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>. When starting an HTTP server, <code>actix-web</code> spawns
a number of workers equal to the number of logical cores on
the server, each in its own thread, and each with its own
<code>tokio</code> reactor.</p>

<p>HTTP handlers can be written in a variety of ways. We might
write one that returns content synchronously:</p>

<pre><code class="language-rust">fn index(req: HttpRequest) -&gt; Bytes {
    ...
}
</code></pre>

<p>This will block the underlying <code>tokio</code> reactor until it&rsquo;s
finished, which is appropriate in situations where no other
blocking calls need to be made; for example, rendering a
static view from memory, or responding to a health check.</p>

<p>We can also write an HTTP handler that returns a boxed
future. This allows us to chain together a series of
asynchronous calls to ensure that the reactor&rsquo;s never
needlessly blocked.</p>

<pre><code class="language-rust">fn index(req: HttpRequest) -&gt; Box&lt;Future&lt;Item=HttpResponse, Error=Error&gt;&gt; {
    ...
}
</code></pre>

<p>Examples of this might be responding with a file that we&rsquo;re
reading from disk (blocking on I/O, albeit minimally), or
waiting on a response from our database. While waiting on a
future&rsquo;s result, the underlying <code>tokio</code> reactor will
happily fulfill other requests.</p>

<figure>
    <img alt="An example of a concurrency model with actix-web." class="overflowing" loading="lazy" src="/assets/images/rust-web/concurrency-model.svg">
    <figcaption>An example of a concurrency model with actix-web.</figcaption>
</figure>

<h3 id="sync-actors" class="link"><a href="#sync-actors">Synchronous actors</a></h3>

<p>Support for futures in Rust is widespread, but not
universal. Notably, <code>diesel</code> doesn&rsquo;t support asynchronous
operations, so all its operations will block. Using it from
directly within an <code>actix-web</code> HTTP handler would lock up
the thread&rsquo;s <code>tokio</code> reactor, and prevent that worker from
serving other requests until the operation finished.</p>

<p>Luckily, <code>actix</code> has a great solution for this problem in
the form of <em>synchronous actors</em>. These are actors that
expect to run their workloads synchronously, and so each is
assigned its own dedicated OS-level thread. The
<code>SyncArbiter</code> abstraction is provided to easily start a
number of copies of one type of actor, each sharing a
message queue so that it&rsquo;s easy to send work to the set
(referenced as <code>addr</code> below):</p>

<pre><code class="language-rust">// Start 3 `DbExecutor` actors, each with its own database
// connection, and each in its own thread
let addr = SyncArbiter::start(3, || {
    DbExecutor(SqliteConnection::establish(&quot;test.db&quot;).unwrap())
});
</code></pre>

<p>Although operations within a synchronous actor are
blocking, other actors in the system like HTTP workers
don&rsquo;t need to wait for any of it to finish &ndash; they get a
future back that represents the message result so that they
can do other work.</p>

<p>In my implementation, fast workloads like parsing
parameters and rendering views is performed inside
handlers, and synchronous actors are never invoked if they
don&rsquo;t need to be. When a response requires database
operations, a message is dispatched to a synchronous actor,
and the HTTP worker&rsquo;s underlying <code>tokio</code> reactor serves
other traffic while waiting for the future to resolve. When
it does, it renders an HTTP response with the result, and
sends it back to the waiting client.</p>

<h3 id="connection-management" class="link"><a href="#connection-management">Connection management</a></h3>

<p>At first glance, introducing synchronous actors into the
system might seem like purely a disadvantage because
they&rsquo;re an upper bound on parallelism. However, this limit
can also be an advantage. One of the first scaling problems
you&rsquo;re likely to run into with Postgres is its modest
limits around the maximum number of allowed simultaneous
connections. Even the biggest instances on Heroku or GCP
(Google Cloud Platform) max out at 500 connections, and the
smaller instances have limits that are <em>much</em> lower (my
small GCP database limits me to 25). Big applications with
coarse connection management schemes (e.g., Rails, but also
many others) tend to resort to solutions like
<a href="https://pgbouncer.github.io/">PgBouncer</a> to sidestep the problem.</p>

<p>Specifying the number of synchronous actors by extension
also implies the maximum number of connections that a
service will use, which leads to perfect control over its
connection usage.</p>

<figure>
    <img alt="Connections are held only when a synchronous actor needs one." class="overflowing" loading="lazy" src="/assets/images/rust-web/connection-management.svg">
    <figcaption>Connections are held only when a synchronous actor needs one.</figcaption>
</figure>

<p>I&rsquo;ve written my synchronous actors to check out individual
connections from a connection pool (<a href="https://github.com/sfackler/r2d2"><code>r2d2</code></a>) only
when starting work, and check them back in after they&rsquo;re
done. When the service is idle, starting up, or shutting
down, it uses zero connections. Contrast this to many web
frameworks where the convention is to open a database
connection as soon as a worker starts up, and to keep it
open as long as the worker is alive. That approach has a
~2x connection requirement for graceful restarts because
all workers being phased in immediately establish a
connection, even while all workers being phased out are
still holding onto one.</p>

<h3 id="ergonomics" class="link"><a href="#ergonomics">The ergonomic advantage of synchronous code</a></h3>

<p>Synchronous operations aren&rsquo;t as fast as a purely
asynchronous approach, but they have the benefit of ease of
use. It&rsquo;s nice that futures are fast, but getting them
properly composed is time consuming, and the compiler
errors they generate if you make a mistake are truly the
stuff of nightmares, which leads to a lot of time spent
debugging.</p>

<p>Writing synchronous code is faster and easier, and I&rsquo;m
personally fine with slightly suboptimal runtime speed if
it means I can implement more core domain logic, more
quickly.</p>

<h3 id="speed" class="link"><a href="#speed">Slow, but only relative to "very, VERY fast"</a></h3>

<p>That might sound disparaging of this model&rsquo;s performance
characteristics, but keep in mind that it&rsquo;s only slow
compared to a purely-asynchronous stack (i.e., futures
everywhere). It&rsquo;s still a conceptually sound concurrent
model with real parallelism, and compared with almost any
other framework and programming language, it&rsquo;s still
really, <em>really</em> fast. I write Ruby in my day job, and
compared to our thread-less model (normal for Ruby because
the GIL constrains thread performance) using forking
processes on a VM <a href="/ruby-memory">without a compacting GC</a>,
we&rsquo;re talking orders of magnitude better speed and memory
efficiency, easily.</p>

<p>At the end of the day, your database is going to be a
bottleneck for parallelism, and the synchronous actor model
supports about as much parallelism as we can expect to get
from it, while also supporting maximum throughput for any
actions that don&rsquo;t need database access.</p>

<h2 id="error-handling" class="link"><a href="#error-handling">Error handling</a></h2>

<p>Like any good Rust program, APIs almost everywhere
throughout return the <code>Result</code> type. Futures plumb through
their own version of <code>Result</code> containing either a
successful result or an error.</p>

<p>I&rsquo;m using <a href="https://github.com/rust-lang-nursery/error-chain">error-chain</a> to define my errors.
Most are internal, but I&rsquo;ve defined a certain group with
the explicit purpose of being user facing:</p>

<pre><code class="language-rust">error_chain!{
    errors {
        //
        // User errors
        //

        BadRequest(message: String) {
            description(&quot;Bad request&quot;),
            display(&quot;Bad request: {}&quot;, message),
        }
    }
}
</code></pre>

<p>When a failure should be surfaced to a user, I make sure to
map it to one of my user error types:</p>

<pre><code class="language-rust">Params::build(log, &amp;request).map_err(|e|
    ErrorKind::BadRequest(e.to_string()).into()
)
</code></pre>

<p>After waiting on a synchronous actor and after attempting
to construct a successful HTTP response, I potentially
handle a user error and render it. The implementation turns
out to be quite elegant (note that in future composition,
<code>then</code> differs from <code>and_then</code> in that it handles a success
<em>or</em> a failure by receiving a <code>Result</code>, as opposed to
<code>and_then</code> which only chains onto a success):</p>

<pre><code class="language-rust">let message = server::Message::new(&amp;log, params);

// Send message to synchronous actor
sync_addr
    .send(message)
    .and_then(move |actor_response| {
        // Transform actor response to HTTP response
    }
    .then(|res: Result&lt;HttpResponse&gt;|
        server::transform_user_error(res, render_user_error)
    )
    .responder()
</code></pre>

<p>Errors not intended to be seen by the user get logged and
<code>actix-web</code> surfaces them as a <code>500 Internal server error</code>
(although I&rsquo;ll likely add a custom renderer for those too
at some point).</p>

<p>Here&rsquo;s <code>transform_user_error</code>. A <code>render</code> function is
abstracted so that we can reuse this generically between an
API that renders JSON responses, and a web server that
renders HTML.</p>

<pre><code class="language-rust">pub fn transform_user_error&lt;F&gt;(res: Result&lt;HttpResponse&gt;, render: F) -&gt; Result&lt;HttpResponse&gt;
where
    F: FnOnce(StatusCode, String) -&gt; Result&lt;HttpResponse&gt;,
{
    match res {
        Err(e @ Error(ErrorKind::BadRequest(_), _)) =&gt; {
            // `format!` activates the `Display` traits and shows our error's `display`
            // definition
            render(StatusCode::BAD_REQUEST, format!(&quot;{}&quot;, e))
        }
        r =&gt; r,
    }
}
</code></pre>

<h2 id="middleware" class="link"><a href="#middleware">Middleware</a></h2>

<p>Like web frameworks across many languages, <code>actix-web</code>
supports middleware. Here&rsquo;s a simple one that initializes a
per-request logger and installs it into the request&rsquo;s
<code>extensions</code> (a collection of request state that will live
for as long as the request does):</p>

<pre><code class="language-rust">pub mod log_initializer {
    pub struct Middleware;

    pub struct Extension(pub Logger);

    impl&lt;S: server::State&gt; actix_web::middleware::Middleware&lt;S&gt; for Middleware {
        fn start(&amp;self, req: &amp;mut HttpRequest&lt;S&gt;) -&gt; actix_web::Result&lt;Started&gt; {
            let log = req.state().log().clone();
            req.extensions().insert(Extension(log));
            Ok(Started::Done)
        }

        fn response(
            &amp;self,
            _req: &amp;mut HttpRequest&lt;S&gt;,
            resp: HttpResponse,
        ) -&gt; actix_web::Result&lt;Response&gt; {
            Ok(Response::Done(resp))
        }
    }

    /// Shorthand for getting a usable `Logger` out of a request.
    pub fn log&lt;S: server::State&gt;(req: &amp;mut HttpRequest&lt;S&gt;) -&gt; Logger {
        req.extensions().get::&lt;Extension&gt;().unwrap().0.clone()
    }
}
</code></pre>

<p>A nice feature is that middleware state is keyed to a
<em>type</em> instead of a string (like you might find with Rack
in Ruby for example). This not only has the benefit of type
checking at compile-time so you can&rsquo;t mistype a key, but
also gives middlewares the power to control their
modularity. If we wanted to strongly encapsulate the
middleware above we could remove the <code>pub</code> from <code>Extension</code>
so that it becomes private. Any other modules that tried to
access its logger would be prevented from doing so by
visibility checks in the compiler.</p>

<h3 id="asynchrony" class="link"><a href="#asynchrony">Asynchrony all the way down</a></h3>

<p>Like handlers, <code>actix-web</code> middleware can be asynchronous
by returning a future instead of a <code>Result</code>. This would,
for example, let us to implement a rate limiting middleware
that made a call out to Redis in a way that doesn&rsquo;t block
the HTTP worker. Did I mention that <code>actix-web</code> is pretty
fast?</p>

<h2 id="testing" class="link"><a href="#testing">HTTP testing</a></h2>

<p><code>actix-web</code> documents a few recommendations for <a href="https://actix.github.io/actix-web/guide/qs_8.html">HTTP
testing methodologies</a>. I settled on a series
of unit tests that use <code>TestServerBuilder</code> to compose a
minimal app containing a single target handler, and then
execute a request against it. This is a nice compromise
because despite tests being minimal, they nonetheless
exercise an end-to-end slice of the HTTP stack, which makes
them fast <em>and</em> complete:</p>

<pre><code class="language-rust">#[test]
fn test_handler_graphql_get() {
    let bootstrap = TestBootstrap::new();
    let mut server = bootstrap.server_builder.start(|app| {
        app.middleware(middleware::log_initializer::Middleware)
            .handler(handler_graphql_get)
    });

    let req = server
        .client(
            Method::GET,
            format!(&quot;/?query={}&quot;, test_helpers::url_encode(b&quot;{podcast{id}}&quot;)).as_str(),
        )
        .finish()
        .unwrap();

    let resp = server.execute(req.send()).unwrap();

    assert_eq!(StatusCode::OK, resp.status());
    let value = test_helpers::read_body_json(resp);

    // The `json!` macro is really cool:
    assert_eq!(json!({&quot;data&quot;: {&quot;podcast&quot;: []}}), value);
}
</code></pre>

<p>I make heavy use of <code>serde_json</code>&rsquo;s (the standard Rust JSON
encoding and decoding library) <code>json!</code> macro, used on the
last line in the code above. If you look closely, you&rsquo;ll
notice that the in-line JSON is not a string &ndash; <code>json!</code>
lets me write actual JSON notation right into my code that
gets checked and converted to a valid Rust structure by the
compiler. This is <em>by far</em> the most elegant approach to
testing HTTP JSON responses that I&rsquo;ve seen across any
programming language, ever.</p>

<h2 id="summary" class="link"><a href="#summary">Summary: is Rust the future for resiliency?</a></h2>

<p>It&rsquo;d be fair to say that I could&rsquo;ve written an equivalent
service in Ruby in a tenth of the time it took me to write
this one in Rust. Some of that is Rust&rsquo;s learning curve,
but a lot of it isn&rsquo;t &ndash; the language is succinct to write,
but appeasing the compiler is often a long and frustrating
process.</p>

<p>That said, over and over I&rsquo;ve experienced passing that
final hurdle, running my program, and experiencing a
Haskell-esque euphoria in seeing it work <em>exactly</em> as I&rsquo;d
intended it to. Contrast that to an interpreted language
where you get it running on your 15th try, and even then,
the edge conditions are almost certainly still wrong. Rust
also makes big changes possible &ndash; it&rsquo;s not unusual for me
to refactor a thousand lines at a time, and once again,
have the program run perfectly afterwards. Anyone who&rsquo;s
seen a large program in an interpreted language at
production-scale knows that you never deploy a sizable
refactor to an important service except in miniscule chunks
&ndash; anything else is too risky.</p>

<p>Should you write your next web service in Rust? I don&rsquo;t
know yet, but we&rsquo;re getting to the point now where you
should at least consider it.</p>

<figure>
    <img alt="Your daily dose of tangentially related photography: Rust on a beam near Pier 28 in San Francisco." class="overflowing" loading="lazy" src="/assets/images/rust-web/rust.jpg" srcset="/assets/images/rust-web/rust@2x.jpg 2x, /assets/images/rust-web/rust.jpg 1x">
    <figcaption>Your daily dose of tangentially related photography: Rust on a beam near Pier 28 in San Francisco.</figcaption>
</figure>


]]></content>
    <published>2018-03-27T14:45:57Z</published>
    <updated>2018-03-27T14:45:57Z</updated>
    <link href="https://brandur.org/rust-web"></link>
    <id>tag:brandur.org,2018-03-27:rust-web</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Speed and Stability: Why Go is a Great Fit for Lambda</title>
    <summary>Why Go&amp;rsquo;s stability and simple deployments is a good fit for a serverless environment.</summary>
    <content type="html"><![CDATA[<p>A few days ago in a move foreshadowed by a hint at Amazons&rsquo;
re:Invent conference late last year, <a href="https://aws.amazon.com/blogs/compute/announcing-go-support-for-aws-lambda/">AWS released support
for Go on its Lambda platform</a>.</p>

<p>Go users can now build programs with typed structs
representing Lambda event sources and common responses in
the <a href="https://github.com/aws/aws-lambda-go"><code>aws-lambda-go</code> SDK</a>. These can then be compiled,
bundled up into a <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-go-how-to-create-deployment-package.html">&ldquo;Lambda deployment package&rdquo;</a>
(as simple as a ZIP file with a binary in it), and added to
a new Lambda function by selecting &ldquo;Go 1.x&rdquo; as a runtime.</p>

<figure>
    <img alt="Prompt for creating a new function on Lambda." class="overflowing" loading="lazy" src="/assets/images/go-lambda/create-function.png" srcset="/assets/images/go-lambda/create-function@2x.png 2x, /assets/images/go-lambda/create-function.png 1x">
    <figcaption>Prompt for creating a new function on Lambda.</figcaption>
</figure>

<p>Go fans around the world are undoubtedly celebrating the
addition, but Gopher or not, this is a big step forward for
everyone. Go may have its share of problems, but it has a
few properties that make it an absolutely ideal fit for a
serverless environment like Lambda.</p>

<h2 id="runtimes" class="link"><a href="#runtimes">Lambda runtimes are special snowflakes</a></h2>

<p>Lambda&rsquo;s exact implementation details have always been a
little mysterious, but we know a few things about them.
User processes are started in sandboxed containers, and
containers that have finished their execution may be kept
around to service a future invocation of the same Lambda
function (but might not be). Between function invocations
containers are frozen, and no user code is allowed to
execute.</p>

<p>Containers also flavored with one of the <a href="https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html">preconfigured
runtimes</a> allowed by Amazon (this list is current
as of January 16th):</p>

<ul>
<li><strong>Node.js</strong> â€“ v4.3.2 and 6.10.3</li>
<li><strong>Java</strong> â€“ Java 8</li>
<li><strong>Python</strong> â€“ Python 3.6 and 2.7</li>
<li><strong>.NET Core</strong> â€“ .NET Core 1.0.1 and .NET Core 2.0</li>
<li><strong>Go</strong> â€“ Go 1.x</li>
</ul>

<p>That&rsquo;s a pretty good variety of languages, but more
interesting is what&rsquo;s <em>missing</em> from the list. While .NET
Core and Python are relatively up-to-date, Java 9 is
absent, along with any recent major version of Node (7.x,
8.x, or 9.x). Notably, major features like <code>async/await</code>
(which landed in Node ~7.6) are still not available on
the Lambda platform even a year after release.</p>

<p>These holes tell us something else about Lambda: new
runtimes are non-trivial to create, run, and/or maintain,
so updated versions often lag far behind their public
availability. Given that Lambda <a href="https://docs.aws.amazon.com/lambda/latest/dg/history.html">will be four years old
this year</a>, it doesn&rsquo;t seem likely that Amazon
will be able to to address this deficiency anytime soon.</p>

<figure>
    <img alt="Go 1.x's longevity is so impressive that it feels like a part of the landscape." class="overflowing" loading="lazy" src="/assets/images/go-lambda/mountain.jpg" srcset="/assets/images/go-lambda/mountain@2x.jpg 2x, /assets/images/go-lambda/mountain.jpg 1x">
    <figcaption>Go 1.x's longevity is so impressive that it feels like a part of the landscape.</figcaption>
</figure>

<h2 id="tenacity" class="link"><a href="#tenacity">The remarkable tenacity of 1.x</a></h2>

<p>That brings us back to Go. Lambda&rsquo;s Go runtime specifies
version &ldquo;1.x&rdquo;. At first glance that might not look all that
different from other languages on the list, but there&rsquo;s a
considerable difference: Go 1 was first released almost
<em>six years ago</em> in <a href="https://golang.org/doc/devel/release.html#go1">March 2012</a>!</p>

<p>Since then, Go has followed up with nine more releases on
the 1.x line (and with a tenth expected soon), each of
which carried significant improvements and features. And
while it&rsquo;s rare to ever have a <em>perfect</em> release that
doesn&rsquo;t break anything, Go&rsquo;s done as good of a job as is
practically possible. Generally new releases are as pain
and worry-free as changing one number in a <code>.travis.yml</code>.</p>

<p>This level and length of API stability for a programming
language is all but unheard of, and it&rsquo;s made even <em>more</em>
impressive given that Go is one of the most actively
developed projects in the world &ndash; a far shot from being
stable only because it&rsquo;s stagnant. The only way this
remarkable feat has been made possible is that (presumably)
having experienced the pain involved in the API changes
that come along with most language upgrades, Go&rsquo;s team has
made stability a core philosophical value.</p>

<p>There&rsquo;s <a href="https://golang.org/doc/go1compat">an entire article</a> dedicated to the policies
around stability for the 1.x line. Here&rsquo;s an excerpt where
they explicitly call out that programs written for 1.x
should stay working for all future versions of 1.x:</p>

<blockquote>
<p>It is intended that programs written to the Go 1
specification will continue to compile and run correctly,
unchanged, over the lifetime of that specification. At
some indefinite point, a Go 2 specification may arise,
but until that time, Go programs that work today should
continue to work even as future &ldquo;point&rdquo; releases of Go 1
arise (Go 1.1, Go 1.2, etc.).</p>

<p>The APIs may grow, acquiring new packages and features,
but not in a way that breaks existing Go 1 code.</p>
</blockquote>

<p>This might sound like normal <a href="https://semver.org/">semantic versioning</a>
(semver), but semver only dictates what to do in the event
of a breaking change. It doesn&rsquo;t say anything about
frequency of change, or committing to not making breaking
changes. Go&rsquo;s proven track record in this area puts it well
ahead of just about any other project.</p>

<p>That brings us back to Lambda. If we look back at our list
of runtimes, the supported versions across languages might
not look all that different, but it&rsquo;s a reasonably safe bet
that the &ldquo;Go 1.x&rdquo; in that list is going to outlive every
other option, probably by a wide margin.</p>

<h2 id="static" class="link"><a href="#static">Static binaries, dependencies, and forward compatibility</a></h2>

<p><a href="https://aws.amazon.com/blogs/compute/announcing-go-support-for-aws-lambda/">The Lambda guide for Go</a> suggests creating a
function by building a statically-linked binary (the
standard for Go), zipping it up, and uploading the whole
package to AWS:</p>

<pre><code class="language-sh">$ GOOS=linux go build -o main
$ zip deployment.zip main
$ aws lambda create-function ...
</code></pre>

<p>This is in sharp contrast to other support languages where
you send either source-level code (Node, Python), or
compiled bytecode (.NET Core, Java). Static binaries have
some major advantages over both of these approaches.</p>

<p>Static linking removes the need for a dependency deployment
system, which is often a heavy part of other language
stacks. Anything that&rsquo;s needed by a final program is linked
in at compile time, and once a program needs to execute, it
doesn&rsquo;t need to think about project layout, include paths,
or requirements files. Source-level dependency management
has been a long criticized blindspot of Go, but with the
addition of the <code>vendor/</code> directory in Go 1.6 and rapid
uptake on the new <a href="https://github.com/golang/dep"><code>dep</code></a> dependency management tool,
the future is looking brighter than ever.</p>

<p>Static binaries also carry the promise of forward
compatibility. Unlike even a bytecode interpreter, when a
new version of Go is released, the Lambda runtime may not
necessarily need an update given that existing containers
will be able to run the new binary. Time will tell for
sure, but unlike Node users who are still transpiling to
get <code>async/await</code> on Lambda, Go users should be able to
push updated programs on the release day of a new version
of Go <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>.</p>

<h2 id="stability" class="link"><a href="#stability">Stability as a feature</a></h2>

<p>It&rsquo;s rare to write software and not have it come back to
haunt you in a few year&rsquo;s time as it needs to be fixed and
upgraded. In a craft generally akin to the shifting sands
in a whirling windstorm, Go is a rare oasis of stability.
More recently there has been some speculation as to <a href="https://blog.golang.org/toward-go2">what
Go 2.0 might look like</a>, there are still no concrete
plans for any major breaking changes, and that&rsquo;s a feature.</p>

<p>Along with the languages normal strengths &ndash; incredible
runtime speed, an amazing concurrency story, a great
batteries-included standard library, and the fastest
edit-compile-debug loop in the business &ndash; Go&rsquo;s stability
and ease of deployment is going to make it a tremendous
addition to the Lambda platform. I&rsquo;d even go so far as to
say that you might want to consider not writing another
serverless function in anything else.</p>


]]></content>
    <published>2018-01-17T16:31:34Z</published>
    <updated>2018-01-17T16:31:34Z</updated>
    <link href="https://brandur.org/go-lambda"></link>
    <id>tag:brandur.org,2018-01-17:go-lambda</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Scaling Postgres with Read Replicas &amp; Using WAL to Counter Stale Reads</title>
    <summary>Scaling out operation with read replicas and avoiding the downside of stale reads by observing replication progress.</summary>
    <content type="html"><![CDATA[<p>A common technique when running applications powered by
relational databases like Postgres, MySQL, and SQL Server
is offloading read operations to readonly replicas <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>,
helping to distribute load between more nodes in the
system by re-routing queries that don&rsquo;t need to run on the
primary. These databases are traditionally single master,
so writes have to go to the primary that&rsquo;s leading the
cluster, but reads can go to any replica as long as it&rsquo;s
reasonably current.</p>

<p>Spreading load across more servers is good, and the pattern
shows itself to be even more useful when considering that
although write operations might be numerous, most of them
have predictable performance &ndash; they&rsquo;re often inserting,
updating, or deleting just a single record. Reads on the
other hand are often more elaborate, and by extension, more
expensive.</p>

<figure>
    <img alt="Writes on the primary and reads on its replicas." class="overflowing" loading="lazy" src="/assets/images/postgres-reads/replica-reads.svg">
    <figcaption>Writes on the primary and reads on its replicas.</figcaption>
</figure>

<p>Even as part of a normal application&rsquo;s workload (barring
analytical queries that can be even more complex), we might
join on two or three different tables in order to perform
an eager load, or even just have to read out a few dozen
rows to accurately render a response. A mature application
might execute hundreds of queries to fulfill even a single
request, and farming these out to replicas would yield huge
benefits in reducing pressure on the primary.</p>

<h2 id="stale-reads" class="link"><a href="#stale-reads">A complication: stale reads</a></h2>

<p>Running reads on replicas is a pretty good high-impact and
low-effort win for scalability, but it&rsquo;s not without its
challenges. The technique introduces the possibility of
<strong><em>stale reads</em></strong> that occur when an application reads from
replica before that replica has received relevant updates
that have been committed to the primary. A user might
update some key details, and then go to view their changes
and see stale data representing the pre-update state.</p>

<figure>
    <img alt="A stale read that went to a replica that hadn't yet applied changes from the primary." class="overflowing" loading="lazy" src="/assets/images/postgres-reads/stale-read.svg">
    <figcaption>A stale read that went to a replica that hadn't yet applied changes from the primary.</figcaption>
</figure>

<p>Stale reads are a race condition. Modern databases
operating over low latency connections can keep replicas
trailing their primary <em>very</em> closely, and probably spend
most of their time less than a second out of date. Even
systems using read replicas without any techniques for
mitigating stale reads will produce correct results most of
the time.</p>

<p>But as software engineers interested in building
bulletproof systems, &ldquo;most of the time&rdquo; isn&rsquo;t good enough,
and we can do better. Let&rsquo;s take a look at a technique to
make sure that stale reads <em>never</em> occur. We&rsquo;ll use
Postgres&rsquo;s own understanding of its replication state and
some in-application intelligence around connection
management to accomplish it.</p>

<h2 id="postgres-wal" class="link"><a href="#postgres-wal">The Postgres WAL</a></h2>

<p>First, we&rsquo;re going to have to understand a little bit about
how replication works in Postgres.</p>

<p>Postgres commits all changes to a <strong><em>WAL</em></strong> (write-ahead
log) for durability reasons. Every change is written out as
a new entry in the WAL and it acts the canonical reference
as to whether any change in the system occurred &ndash;
committed information is written to a data directory like
you might expect, but is only considered visible to new
transactions if the WAL confirms that it&rsquo;s committed (see
<a href="/postgres-atomicity">How Postgres makes transactions
atomic</a> for more on this subject).</p>

<p>Changes are written to the WAL one entry at a time and each
one is assigned a <strong><em>LSN</em></strong> (log sequence number). Changes
are batched in 16 MB <strong><em>WAL segments</em></strong>.</p>

<h3 id="wal-replication" class="link"><a href="#wal-replication">The WAL's role in replication</a></h3>

<p>A Postgres database can dump a representation of its
current state to a <em>base backup</em> which can be used to
initialize replica. From there, the replica stays in
lockstep with its primary by consuming changes in its
emitted WAL. A base backup comes with a pointer to the
current LSN so that when a replica starts to consume the
WAL, it knows where to start.</p>

<figure>
    <img alt="A replica being initialized from base backup and consuming its primary's WAL." class="overflowing" loading="lazy" src="/assets/images/postgres-reads/replicas-and-wal.svg">
    <figcaption>A replica being initialized from base backup and consuming its primary's WAL.</figcaption>
</figure>

<p>There are a few ways for a replica to consume WAL. The
first is &ldquo;log shipping&rdquo;: completed WAL segments (16 MB
chunks of the WAL) are copied from primary to replicas and
consumed as a single batch. This has the major advantage of
efficiency (it&rsquo;s fast to copy files around, and has
negligible cost to the primary), but with a tradeoff of how
closely any secondary can be following its primary &ndash;
secondaries will be at least as behind as the current
segment that&rsquo;s still being written.</p>

<p>Another common configuration for consuming WAL is
&ldquo;streaming&rdquo;, where WAL is emitted by the primary to
replicas over an open connection. This has the advantage of
secondaries being very current at the cost of some extra
resource consumption.</p>

<p>Based on their respective aptitude&rsquo;s for becoming primary
at a moment&rsquo;s notice, replicas consuming WAL with log
shipping are also known as &ldquo;warm standbys&rdquo; while those
using streaming are called &ldquo;hot standbys&rdquo;. Hot standbys are
often seen in production setups because maintain state that
closely matches their primary and make great targets to
fail over to at a moment&rsquo;s notice. The technique we&rsquo;re
going to discuss works better with streaming, but should
yield at benefits with either method.</p>

<h2 id="routing-reads" class="link"><a href="#routing-reads">Routing reads based on replica WAL position</a></h2>

<p>By routing read operations only to replicas that are caught
up enough to run them accurately, we can eliminate stale
reads. This necessitates an easy way of measuring how far
behind a replica is, and the WAL&rsquo;s LSN is perfect for this
use.</p>

<p>When mutating a resource in the system we&rsquo;ll store the
last committed LSN for the entity making the request. Then,
when we subsequently want to fulfill a read operation for
that same entity, we&rsquo;ll check which replicas have consumed
to that point or beyond it, and randomly select one from
the pool. If no replicas are sufficiently advanced (i.e.
say a read operation is being run very closely after the
initial write), we&rsquo;ll fall back to the master. Stale reads
become impossible regardless of the state of any given
replica.</p>

<figure>
    <img alt="Routing read operations based on replica progress in the WAL." class="overflowing" loading="lazy" src="/assets/images/postgres-reads/routing.svg">
    <figcaption>Routing read operations based on replica progress in the WAL.</figcaption>
</figure>

<p>The technique is inspired by <a href="https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/#sticky-connections">GitLab&rsquo;s article on scaling
their database</a>, where they refer to it as &ldquo;sticky
connections&rdquo;. Their large Postgres installation is still
unpartitioned, and using replicas for extra read capacity
is key in managing its considerable load.</p>

<h3 id="rocket-rides" class="link"><a href="#rocket-rides">Scalable Rocket Rides</a></h3>

<p>To build a working demo we&rsquo;ll be returning to the same toy
application that we used to show off an implementation for
<a href="/idempotency-keys">idempotency keys</a> and <a href="/redis-streams">the unified
log</a> &ndash; <em>Rocket Rides</em>. As a quick
reminder, <em>Rocket Rides</em> is a Lyft-like app that lets its
users get rides with pilots wearing jetpacks; a vast
improvement over the everyday banality of a car.</p>

<p>Our new <em>Scalable Rocket Rides</em> demo has an <code>api</code> process
that writes to a Postgres database. It&rsquo;s configured with a
number of read replicas that are configured with Postgres
replication to receive changes from the primary. When
performing a read, the <code>api</code> tries to route it to one of a
random replica that&rsquo;s sufficiently caught up to fulfill the
operation for a particular user.</p>

<p>We&rsquo;ll be using the Sequel gem, which can be configured with
a primary and any number of read replicas. Replicas are
assigned names like <code>replica0</code>, and operations are sent to
them with the <code>server(...)</code> helper:</p>

<pre><code class="language-ruby">DB = Sequel.connect(&quot;postgres://localhost:5433/rocket-rides-scalale&quot;,
  servers: {
    replica0: { port: 5434 },
    replica1: { port: 5435 },
    replica2: { port: 5436 },
    ...
  }

# routes to primary
DB[:users].update(...)

# routes to replica0
DB[:users].server(:replica0).select(...)
</code></pre>

<p>A working version of all this code is available in the
<a href="https://github.com/brandur/rocket-rides-scalable"><em>Scalable Rocket Rides</em></a> repository. We&rsquo;ll
walk through the project with a number of extracted
snippets, but if you prefer, you can download the code and
follow along:</p>

<pre><code class="language-sh">git clone https://github.com/brandur/rocket-rides-scalable.git
</code></pre>

<h3 id="cluster" class="link"><a href="#cluster">Bootstrapping a cluster</a></h3>

<p>For demo purposes it&rsquo;s useful to create a small
locally-running cluster with a primary and some replicas.
The project <a href="https://github.com/brandur/rocket-rides-scalable/tree/master/scripts/create_cluster">includes a small script to help with
that</a>. It initializes and starts a primary,
and for a number of times equal to the <code>NUM_REPLICAS</code>
environment variable performs a base backup and boots a
replica with it</p>

<p>Postgres daemons are started as children of the script with
Ruby&rsquo;s <code>Process.spawn</code> and will all die when it&rsquo;s stopped.
The setup&rsquo;s designed to be ephemeral and any data added to
the primary is removed when the cluster bootstraps itself
again on the script&rsquo;s next run.</p>

<h3 id="observer" class="link"><a href="#observer">The Observer: tracking replication status</a></h3>

<p>To save every <code>api</code> process from having to reach out and
check on the replication status of every replica for
itself, we&rsquo;ll have a process called an <code>observer</code> that
periodically refreshes the state of every replica and
stores it to a Postgres table.</p>

<p>The table contains a common <code>name</code> for each replica (e.g.
<code>replica0</code>) and a <code>last_lsn</code> field that stores a sequence
number as Postgres&rsquo;s native <code>pg_lsn</code> data type:</p>

<pre><code class="language-sql">CREATE TABLE replica_statuses (
    id       BIGSERIAL    PRIMARY KEY,
    last_lsn PG_LSN       NOT NULL,
    name     VARCHAR(100) NOT NULL UNIQUE
);
</code></pre>

<p>Keep in mind that this status information could really go
anywhere. If we have Redis available, we could put it in
there for fast access, or have every <code>api</code> worker cache it
in-process periodically for even faster access. Postgres is
convenient, and as we&rsquo;ll see momentarily, makes lookups
quite elegant, but it&rsquo;s not necessary.</p>

<p>The <code>observer</code> runs in a loop, and executes something like
this on every iteration:</p>

<pre><code class="language-ruby"># exclude :default at the zero index
replica_names = DB.servers[1..-1]

last_lsns = replica_names.map do |name|
  DB.with_server(name) do
    DB[Sequel.lit(&lt;&lt;~eos)].first[:lsn]
      SELECT pg_last_wal_replay_lsn() AS lsn;
    eos
  end
end

insert_tuples = []
replica_names.each_with_index do |name, i|
  insert_tuples &lt;&lt; { name: name.to_s, last_lsn: last_lsns[i] }
end

# update all replica statuses at once with upsert
DB[:replica_statuses].
  insert_conflict(target: :name,
    update: { last_lsn: Sequel[:excluded][:last_lsn] }).
  multi_insert(insert_tuples)

$stdout.puts &quot;Updated replica LSNs: results=#{insert_tuples}&quot;
</code></pre>

<p>A connection is made to every replica and
<code>pg_last_wal_replay_lsn()</code> is used to see its current
location in the WAL. When all statuses have been collected,
Postgres upsert (<code>INSERT INTO ... ON CONFLICT ...</code>) is used
to store the entire set to <code>replica_statuses</code>.</p>

<h3 id="min-lsn" class="link"><a href="#min-lsn">Saving minimum LSN</a></h3>

<p>Knowing the status of our replicas is half of the
implementation. The other half is knowing the minimum
replication progress for every user that will give us the
horizon beyond which stale reads are impossible. This is
determined by saving the primary&rsquo;s current LSN whenever the
user makes a change in the system.</p>

<p>We&rsquo;ll model this as a <code>min_lsn</code> field on our <code>users</code>
relation (and again use the built-in <code>pg_lsn</code> data type):</p>

<pre><code class="language-sql">CREATE TABLE users (
    id      BIGSERIAL    PRIMARY KEY,
    email   VARCHAR(255) NOT NULL UNIQUE,
    min_lsn PG_LSN
);
</code></pre>

<p>For any action that will later affect reads, we touch the
user&rsquo;s <code>min_lsn</code> by setting it to the value of the
primary&rsquo;s <code>pg_current_wal_lsn()</code>. This is performed in
<code>update_user_min_lsn</code> in this simple implementation:</p>

<pre><code class="language-ruby">post &quot;/rides&quot; do
  user = authenticate_user(request)
  params = validate_params(request)

  DB.transaction(isolation: :serializable) do
    ride = Ride.create(
      distance: params[&quot;distance&quot;],
      user_id: user.id,
    )
    update_user_min_lsn(user)

    [201, JSON.generate(serialize_ride(ride))]
  end
end

def update_user_min_lsn(user)
  User.
    where(id: user.id).
    update(Sequel.lit(&quot;min_lsn = pg_current_wal_lsn()&quot;))
end
</code></pre>

<h3 id="select-replica" class="link"><a href="#select-replica">Selecting an eligible replica</a></h3>

<p>Now that replication status and minimum WAL progress for
every user is being tracked, <code>api</code> processes need a way to
select an eligible replica candidate for read operations.
Here&rsquo;s an implementation that does just that:</p>

<pre><code class="language-ruby">def select_replica(user)
  # If the user's `min_lsn` is `NULL` then they haven't performed an operation
  # yet, and we don't yet know if we can use a replica yet. Default to the
  # primary.
  return :default if user.min_lsn.nil?

  # exclude :default at the zero index
  replica_names = DB.servers[1..-1].map { |name| name.to_s }

  res = DB[Sequel.lit(&lt;&lt;~eos), replica_names, user.min_lsn]
    SELECT name
    FROM replica_statuses
    WHERE name IN ?
      AND pg_wal_lsn_diff(last_lsn, ?) &gt;= 0;
  eos

  # If no candidates are caught up enough, then go to the primary.
  return :default if res.nil? || res.empty?

  # Return a random replica name from amongst the candidates.
  candidate_names = res.map { |res| res[:name].to_sym }
  candidate_names.sample
end
</code></pre>

<p><code>pg_wal_lsn_diff()</code> returns the difference between two
<code>pg_lsn</code> values, and we use it to compare the stored status
of each replica in <code>replica_statuses</code> to the <code>min_lsn</code>
value of the current user (<code>&gt;= 0</code> means that the replica is
ahead of the user&rsquo;s minimum). We take the name of a random
replica from the returned set. If the set was empty, then
no replica is advanced enough for our purposes, so we fall
back to the primary.</p>

<p>Here&rsquo;s <code>select_replica</code> in action on an API endpoint:</p>

<pre><code class="language-ruby">get &quot;/rides/:id&quot; do |id|
  user = authenticate_user(request)

  name = select_replica(user)
  $stdout.puts &quot;Reading ride #{id} from server '#{name}'&quot;

  ride = Ride.server(name).first(id: id)
  if ride.nil?
    halt 404, JSON.generate(wrap_error(
      Messages.error_not_found(object: &quot;ride&quot;, id: id)
    ))
  end

  [200, JSON.generate(serialize_ride(ride))]
end
</code></pre>

<p>And that&rsquo;s it! The repository also comes with a simulator
that creates a new ride and then immediately tries to read
it. Running the constellation of programs will show that
most of the time these reads will be served from a replica,
but occasionally from the primary (<code>default</code> in Sequel) as
replication falls behind or the <code>observer</code> hasn&rsquo;t performed
its work loop in a while:</p>

<pre><code>$ forego start | grep 'Reading ride'
api.1       | Reading ride 96 from server 'replica0'
api.1       | Reading ride 97 from server 'replica0'
api.1       | Reading ride 98 from server 'replica0'
api.1       | Reading ride 99 from server 'replica1'
api.1       | Reading ride 100 from server 'replica4'
api.1       | Reading ride 101 from server 'replica2'
api.1       | Reading ride 102 from server 'replica0'
api.1       | Reading ride 103 from server 'default'
api.1       | Reading ride 104 from server 'default'
api.1       | Reading ride 105 from server 'replica2'
</code></pre>

<h2 id="should-i" class="link"><a href="#should-i">Should I do this?</a></h2>

<p>Maybe. The implementation&rsquo;s major downside is that each
user&rsquo;s <code>min_lsn</code> needs to be updated every time an action
that affects read results is performed. If you squint just
a little bit, you&rsquo;ll notice that this looks a lot like
cache invalidation &ndash; a technique infamous for working well
until it doesn&rsquo;t. In a more complex codebase save hooks and
update triggers can be useful in helping to ensure
correctness, but given enough lines of code and enough
people working on it, <em>perfect</em> correctness can be
frustratingly elusive.</p>

<p>Projects that produce only moderate database load (the
majority of all projects) shouldn&rsquo;t bother, and keep their
implementations simple by running everything against the
primary. Projects that need infinitely scalable storage
(i.e. disk usage is expected to grow well beyond what a
single node can handle) should probably look into a more
elaborate partitioning scheme (<a href="https://www.citusdata.com/">like Citus</a>).</p>

<p>There is a sweet spot of projects that can keep their
storage within a single node, but still want to scale out
on computation. For this sort of use moving reads to
replicas can be quite beneficial because it greatly expands
the runway for scalability while also avoiding the
considerable overhead and operational complexity of
partitioning.</p>


]]></content>
    <published>2017-11-17T22:02:56Z</published>
    <updated>2017-11-17T22:02:56Z</updated>
    <link href="https://brandur.org/postgres-reads"></link>
    <id>tag:brandur.org,2017-11-17:postgres-reads</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Redis Streams and the Unified Log</title>
    <summary>Building a log-based architecture that&amp;rsquo;s fast, efficient, and resilient on the new stream data structure in Redis.</summary>
    <content type="html"><![CDATA[<p>Years ago an article came out of LinkedIn <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">about the
unified log</a>, a useful architectural pattern for
services in a distributed system share state with one
another. In the log&rsquo;s design, services emit state changes
into an ordered data structure in which each new record
gets a unique ID. Unlike a queue, a log is durable across
any number of reads until it&rsquo;s explicitly truncated.</p>

<p>Consumers track changes in the wider system by reading the
log. Each one maintains the ID of the last record it
successfully consumed and aims to read every record at
least once &ndash; nothing should ever be missed. When a
consumer is knocked offline, it restarts and looks up the
last ID that it saw, and continues reading the log from
there.</p>

<p>The log is <strong><em>unified</em></strong> because it acts as a master ledger
of state changes in a wider system. All components that are
making critical changes write to it, and all components
that need to track distributed state are subscribed.</p>

<p>LinkedIn&rsquo;s article is sober enough to point out that this
design is nothing new: we&rsquo;ve been using logs in various
forms in computer science for decades. <a href="https://en.wikipedia.org/wiki/Journaling_file_system">Journaling file
systems</a> use them to protect data against
corruption. Databases use them in places like the
<a href="https://www.postgresql.org/docs/current/static/wal.html">write-ahead log (WAL)</a> in Postgres as they stream
changes to their read replicas.</p>

<figure>
    <img alt="The unified log: a producer emits to the stream and consumers read from it." class="overflowing" loading="lazy" src="/assets/images/redis-streams/unified-log.svg">
    <figcaption>The unified log: a producer emits to the stream and consumers read from it.</figcaption>
</figure>

<p>Even so, the unified log was a refreshingly novel idea when
the article was written, and still is. File systems and
databases use the structure because it&rsquo;s effective, and it
lends itself just as well to distributed architectures.
Kafka is more prevalent in 2017, but most of us are still
gluing components together with patches and duct tape.</p>

<p>Chatty services exchange high-frequency messages back and
forth in a way that&rsquo;s slow (they rely on synchrony),
inefficient (single messages are passed around), and
fragile (every individual message introduces some
possibility of failure). In contrast, the log is
asynchronous, its records are produced and consumed in
batches, and its design builds in resilience at every turn.</p>

<h2 id="redis-streams" class="link"><a href="#redis-streams">Redis streams</a></h2>

<p>This brings us to Redis. I was happy to hear recently that
the project will soon <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup> be shipping with a new data
structure that&rsquo;s a perfect foundation for a unified log:
<a href="http://antirez.com/news/114">streams</a>. Unlike a Redis list, records in a
stream are assigned with addressable IDs and are indexed or
sliced with those IDs instead than a relative offset (i.e.
like <code>0</code> or <code>len() - 1</code>).This lends itself well to having
multiple consumers reading out of a single stream and
tracking their position within it by persisting the ID of
the last record they read.</p>

<p>The new <code>XADD</code> command appends to one:</p>

<pre><code>&gt; XADD rocket-rides-log * id 123 distance 456.7
1506871964177.0
</code></pre>

<p>A record with <code>id = 123</code> and <code>distance = 456.7</code> is appended
to the stream <code>rocket-rides-log</code>. Redis responds with a
unique ID for the record that&rsquo;s made up of a timestamp and
a sequence number (<code>.0</code>) to disambiguate entries created
within the same millisecond.</p>

<p><code>XRANGE</code> is <code>XADD</code>&rsquo;s counterpart. It reads a set of records
from a stream:</p>

<pre><code>&gt; XRANGE rocket-rides-log - + COUNT 2
1) 1) 1506871964177.0
   2) 1) &quot;id&quot;
      2) &quot;123&quot;
      3) &quot;distance&quot;
      4) &quot;456.7&quot;
2) 1) 1506872463535.0
   2) 1) &quot;id&quot;
      2) &quot;124&quot;
      3) &quot;distance&quot;
      4) &quot;89.0&quot;
</code></pre>

<p>The tokens <code>-</code> and <code>+</code> are special in that they tell Redis
to read from the first available record in the stream and
up to the last available record in the stream respectively.
Either one can be replaced with an ID like
<code>1506871964177.0</code> to read from or up to a specific record.
Using this capability allows us to slice out just records
that we haven&rsquo;t consumed yet. Specifying <code>COUNT 2</code> lets us
bound the number of records read so that we can process the
stream in efficient batches.</p>

<h3 id="kakfa" class="link"><a href="#kakfa">Versus Kafka</a></h3>

<p>Kafka is a popular system component that also makes a nice
alternative for a unified log implementation; and once
everything is in place, probably a better one compared to
Redis thanks to its sophisticated design around high
availability and other advanced features.</p>

<p>Redis streams aren&rsquo;t exciting for their innovativeness, but
rather than they bring building a unified log architecture
within reach of a small and/or inexpensive app. Kafka is
infamously difficult to configure and get running, and is
expensive to operate once you do. Pricing for a small Kafka
cluster on Heroku costs $100 a month and climbs steeply
from there. It&rsquo;s tempting to think you can do it more
cheaply yourself, but after factoring in server and
personnel costs along with the time it takes to build
working expertise in the system, it&rsquo;ll cost more.</p>

<p>Redis on the other hand is probably already in your stack.
Being the Swiss army knife of cloud persistence, it&rsquo;s
useful for a multitude of things including caching, rate
limiting, storing user sessions, etc. Even if you don&rsquo;t
already have it, you can compile it from source and get it
configured and running in all of about thirty seconds.
Dozens of cloud providers (including big ones like AWS)
offer a hosted version.</p>

<p>Once you&rsquo;re operating at serious scale, Kafka might be the
right fit. In the meantime, Redis streams make a great (and
economic) alternative.</p>

<h3 id="redis-durability" class="link"><a href="#redis-durability">Configuring Redis for durability</a></h3>

<p>One highly desirable property of a unified log is that it&rsquo;s
<strong><em>durable</em></strong>, meaning that even if its host crashes or
something terrible happens, it doesn&rsquo;t lose information
that producers think they had persisted.</p>

<p>By default Redis is not durable; a sane configuration
choice when it&rsquo;s been used for caching or rate limiting,
but not when it&rsquo;s being used for a log. To make Redis fully
durable, tell it to keep an append-only file (AOF) with
<code>appendonly</code> and instruct it to perform fsync on every
command written to the AOF with <code>appendfsync always</code> (more
details <a href="https://redis.io/topics/persistence">in the Redis documentation on
persistence</a>):</p>

<pre><code>appendonly yes
appendfsync always
</code></pre>

<p>There&rsquo;s an inherent tradeoff between durability and
performance (ever wonder <a href="/fragments/mongo-durability">how MongoDB performed so well on
its early benchmarks?</a>). Redis doing the
extra work to keep an AOF and performing more fsyncs will
make commands slower (although still very fast). If you&rsquo;re
using it for multiple things, it might be useful to make a
distinction between places where ephemerality is okay and
where it isn&rsquo;t, and run two separate Redises with different
configuration.</p>

<h2 id="rocket-rides-unified" class="link"><a href="#rocket-rides-unified">Unified Rocket Rides</a></h2>

<p>We&rsquo;re going to be returning to the Rocket Rides example
that we talked about while implementing <a href="/idempotency-keys">idempotency
keys</a>. As a quick reminder, Rocket Rides
is a Lyft-like app that lets its users get rides with
pilots wearing jetpacks; a vast improvement in speed and
adrenaline flow over the every day banality of a car.</p>

<p>As new rides come in, the <em>Unified Rocket Rides</em> API will
emit a new record to the stream that contains the ID of the
ride and the distance traveled. From there, a couple
different consumers will read the stream and keep a running
tally of the total distance traveled for every ride in the
system that&rsquo;s been taken.</p>

<figure>
    <img alt="Clients sending data to the API which passes it onto the stream and is ingested by stream consumers." class="overflowing" loading="lazy" src="/assets/images/redis-streams/streaming-model.svg">
    <figcaption>Clients sending data to the API which passes it onto the stream and is ingested by stream consumers.</figcaption>
</figure>

<p>Both producer and consumers will be using database
transactions to guarantee that all information is correct.
No matter what kind of failures occur in clients, API,
consumers, or elsewhere in the system, the totals being
tracked by consumers should always agree with each other
for any given Redis or ride ID.</p>

<p>A working version of all this code is available in the
<a href="https://github.com/brandur/rocket-rides-unified"><em>Unified Rocket Rides</em></a> repository. It might
be easier to download that code and follow along that way:</p>

<pre><code class="language-sh">git clone https://github.com/brandur/rocket-rides-unified.git
</code></pre>

<h3 id="at-least-once" class="link"><a href="#at-least-once">At-least once design</a></h3>

<p>For systems powered by a unified log, resilience and
correctness are the name of the game. Consumers shouldn&rsquo;t
just get most messages that a producer sends, they should
get <em>every</em> message. To that end programs are built to
guarantee <strong><em>at-least once</em></strong> delivery semantics. Messages
are usually sent once, but in cases where there&rsquo;s
uncertainty around whether the transmission occurred, a
message will be sent as many times as necessary to be sure.</p>

<p>At-least once delivery is opposed to <strong><em>best-effort
delivery</em></strong> where messages will be received once under
normal conditions, but may be dropped in degraded cases.
It&rsquo;s also opposed by <strong><em>exactly-once delivery</em></strong>; a panacea
of distributed systems. Exactly-once delivery is a
difficult guarantee to make, and even if possible, would
add costly coordination overhead to transmission. In
practice, at-least once semantics are robust and easy to
work with as long as systems are built to consider them
from the beginning.</p>

<h3 id="api" class="link"><a href="#api">The API</a></h3>

<p>The <em>Unified Rocket Rides</em> API receives requests over HTTP
for new rides from clients. When it does it (1) creates a
ride entry in the local database, and (2) emits a record
into the unified log to show that it did.</p>

<pre><code class="language-ruby">post &quot;/rides&quot; do
  params = validate_params(request)

  DB.transaction(isolation: :serializable) do
    ride = Ride.create(
      distance: params[&quot;distance&quot;]
    )

    StagedLogRecord.insert(
      action: ACTION_CREATE,
      object: OBJECT_RIDE,
      data: Sequel.pg_jsonb({
        id:       ride.id,
        distance: ride.distance,
      })
    )

    [201, JSON.generate(wrap_ok(
      Messages.ok(distance: params[&quot;distance&quot;].round(1))
    ))]
  end
end
</code></pre>

<p>Rather than emit directly to Redis, a &ldquo;staged&rdquo; record is
created in Postgres. This indirection is useful so that in
case the request&rsquo;s transaction rolls back due to a
serialization error or other problem, no invalid data (i.e.
data that was only relevant in a now-aborted transaction)
is left in the log. This principle is identical to that of
<a href="/job-drain">transactionally-staged job drains</a>, which do
the same thing for background work.</p>

<p>The staged records relation in Postgres look like:</p>

<pre><code class="language-sql">CREATE TABLE staged_log_records (
    id     BIGSERIAL PRIMARY KEY,
    action TEXT      NOT NULL,
    data   JSONB     NOT NULL,
    object TEXT      NOT NULL
);
</code></pre>

<h3 id="streamer" class="link"><a href="#streamer">The streamer</a></h3>

<p>The streamer moves staged records into Redis once they
become visible outside of the transaction that created
them. It runs as a separate process, and sends records in
batches for improved efficiency.</p>

<pre><code class="language-ruby">def run_once
  num_streamed = 0

  # Need at least repeatable read isolation level so that our DELETE after
  # enqueueing will see the same records as the original SELECT.
  DB.transaction(isolation_level: :repeatable_read) do
    records = StagedLogRecord.order(:id).limit(BATCH_SIZE)

    unless records.empty?
      RDB.multi do
        records.each do |record|
          stream(record.data)
          num_streamed += 1

          $stdout.puts &quot;Enqueued record: #{record.action} #{record.object}&quot;
        end
      end

      StagedLogRecord.where(Sequel.lit(&quot;id &lt;= ?&quot;, records.last.id)).delete
    end
  end

  num_streamed
end

#
# private
#

# Number of records to try to stream on each batch.
BATCH_SIZE = 1000
private_constant :BATCH_SIZE

private def stream(data)
  # XADD mystream MAXLEN ~ 10000  * data &lt;JSON-encoded blob&gt;
  #
  # MAXLEN ~ 10000 caps the stream at roughly that number (the &quot;~&quot; trades
  # precision for speed) so that it doesn't grow in a purely unbounded way.
  RDB.xadd(STREAM_NAME,
    &quot;MAXLEN&quot;, &quot;~&quot;, STREAM_MAXLEN,
    &quot;*&quot;, &quot;data&quot;, JSON.generate(data))
end
</code></pre>

<p>In accordance with at-least once design, the streamer only
removes staged records once their receipt has been
confirmed by Redis. If part of the workflow fails then the
process will run again and select the same batch of records
from <code>staged_log_records</code> a second time. They&rsquo;ll be
re-emitted into the stream even if it means that some
consumers will see them twice.</p>

<p>Records are sent to the stream with ascending ride <code>id</code>s.
It&rsquo;s possible for a record with a smaller <code>id</code> to be
present after one with a higher <code>id</code>, but <em>only</em> in the
case of a double-send. With the exception of that one
caveat, consumers can always assume that they&rsquo;re receiving
<code>id</code>s in order.</p>

<h4 id="truncation" class="link"><a href="#truncation">Log truncation</a></h4>

<p>Unlike a queue, consumers don&rsquo;t remove records from a log,
and without management it would be in danger of growing in
an unbounded way. In the example above, the streamer uses
the <code>MAXLEN</code> argument to <code>XADD</code> to tell Redis that the
stream should have a maximum length. The tilde (<code>~</code>)
operator is an optimization that indicates to Redis that
the stream should be truncated to <em>approximately</em> the
specified length when it&rsquo;s possible to remove an entire
node. This is significantly faster than trying to prune it
to an exact number.</p>

<p>This rough truncation will work well in most of the time,
but lack of safety measures means that it&rsquo;s <em>possible</em> that
records might be removed which have not yet been read by a
consumer that&rsquo;s fallen way behind. A more resilient system
should track the progress of each consumer and only
truncate records that are no longer needed by any of them.</p>

<h3 id="consumers" class="link"><a href="#consumers">Consumers & checkpointing</a></h3>

<p>Consumers read records out of the log in batches and
consume them one-by-one. When a batch has been successfully
processed, they set a <strong><em>checkpoint</em></strong> containing the ID of
the last record consumed. The next time a consumer restarts
(due to a crash or otherwise), it reads its last checkpoint
and starts reading the log from the ID that it contained.</p>

<p>Checkpoints are stored as a relation in Postgres:</p>

<pre><code class="language-sql">CREATE TABLE checkpoints (
    id            BIGSERIAL PRIMARY KEY,
    name          TEXT      NOT NULL UNIQUE,
    last_redis_id TEXT      NOT NULL,
    last_ride_id  BIGINT    NOT NULL
);
</code></pre>

<p>Recall that in our simple example, consumers add up the
<code>distance</code> of every ride created on the platform. We&rsquo;ll
keep this running tally in a <code>consumer_states</code> table which
has an entry for each consumer:</p>

<pre><code class="language-sql">CREATE TABLE consumer_states (
    id             BIGSERIAL        PRIMARY KEY,
    name           TEXT             NOT NULL UNIQUE,
    total_distance DOUBLE PRECISION NOT NULL
);
</code></pre>

<p>The code for a consumer to iterate the stream and update
its checkpoint and state will look a little like this:</p>

<pre><code class="language-ruby">def run_once
  num_consumed = 0

  DB.transaction do
    checkpoint = Checkpoint.first(name: name)

    # &quot;-&quot; is a special symbol in Redis streams that dictates that we should
    # start from the earliest record in the stream. If we don't already have
    # a checkpoint, we start with that.
    start_id = &quot;-&quot;
    start_id = self.class.increment(checkpoint.last_redis_id) unless checkpoint.nil?

    checkpoint = Checkpoint.new(name: name, last_ride_id: 0) if checkpoint.nil?

    records = RDB.xrange(STREAM_NAME, start_id, &quot;+&quot;, &quot;COUNT&quot;, BATCH_SIZE)
    unless records.empty?
      # get or create a new state for this consumer
      state = ConsumerState.first(name: name)
      state = ConsumerState.new(name: name, total_distance: 0.0) if state.nil?

      records.each do |record|
        redis_id, fields = record

        # [&quot;data&quot;, &quot;{\&quot;id\&quot;:123}&quot;] -&gt; {&quot;data&quot;=&gt;&quot;{\&quot;id\&quot;:123}&quot;}
        fields = Hash[*fields]

        data = JSON.parse(fields[&quot;data&quot;])

        # if the ride's ID is lower or equal to one that we know we consumed,
        # skip it; this is a double send
        if data[&quot;id&quot;] &lt;= checkpoint.last_ride_id
          $stdout.puts &quot;Skipped record: #{fields[&quot;data&quot;]} &quot; \
            &quot;(already consumed this ride ID)&quot;
          next
        end

        state.total_distance += data[&quot;distance&quot;]

        $stdout.puts &quot;Consumed record: #{fields[&quot;data&quot;]} &quot; \
          &quot;total_distance=#{state.total_distance.round(1)}m&quot;
        num_consumed += 1

        checkpoint.last_redis_id = redis_id
        checkpoint.last_ride_id = data[&quot;id&quot;]
      end

      # now that all records for this round are consumed, persist state
      state.save

      # and persist the changes to the checkpoint
      checkpoint.save
    end
  end

  num_consumed
end
</code></pre>

<p>Like the streamer, the consumer is designed with at-least
once semantics in mind. In the event of a crash, neither
the <code>total_distance</code> or the checkpoint is updated because a
raised exception aborts the transaction that wraps the
entire set of operations. When the consumer restarts, it
happily consumes the last batch again with no ill effects.</p>

<p>Along with a Redis stream ID, a checkpoint also tracks the
last consumed ride ID. This is so consumers can handle
records that were written to the stream more than once. IDs
can always be assumed to be ordered and if a consumer sees
an ID smaller or equal to one that it knows that it
consumes, it safely skips to the next record.</p>

<h3 id="simulating-failure" class="link"><a href="#simulating-failure">Simulating failure</a></h3>

<p>I&rsquo;ve claimed this system is fault-tolerant, but it&rsquo;s more
believable if I can demonstrate it. Operating at our small
scale we&rsquo;re unlikely to see many problems, so processes are
written to simulate some. 10% of the time, the streamer
will double-send every event in a batch. This models it
failing midway through sending a batch and having to retry
the entire operation.</p>

<p>Likewise, each consumer will crash 10% of the time after
handling a batch but before committing the transaction that
would set its state and checkpoint.</p>

<p>The system&rsquo;s been designed to handle these edge cases and
despite the artificial problems, it will manage itself
gracefully. Run <code>forego start</code> (after following the
appropriate setup in <code>README.md</code>) and leave the fleet of
processes running. Despite the double sends and each
consumer failing randomly and independently, no matter how
long you wait, the consumers should always stay roughly
caught up to each other and show the same <code>total_distance</code>
reading for any given ID.</p>

<p>Here&rsquo;s <code>consumer0</code> and <code>consumer1</code> showing an identical
total for ride <code>521</code>:</p>

<pre><code>consumer0.1 | Consumed record: {&quot;id&quot;:521,&quot;distance&quot;:539.836923415231}
              total_distance=257721.7m
consumer1.1 | Consumed record: {&quot;id&quot;:521,&quot;distance&quot;:539.836923415231}
              total_distance=257721.7m
</code></pre>

<h2 id="considerations" class="link"><a href="#considerations">Other considerations</a></h2>

<h3 id="non-transaction" class="link"><a href="#non-transaction">Non-transactional consumers & idempotency</a></h3>

<p>Consumers don&rsquo;t necessarily have to be transactional as
long as the work they do while consuming records can be
re-applied cleanly given at-least once semantics. Another
way of saying this is that a consumer doesn&rsquo;t need a
transaction as long as every operation it applies is
<strong><em>idempotent</em></strong>.</p>

<p>Notably our example here wouldn&rsquo;t yield correct results
without being nested in a transaction: if it successfully
updated <code>total_count</code> but failed to set the checkpoint,
then it would double-count the distance of those records
the next time it tried to consume them.</p>

<p>But if all operations are idempotent, we could remove the
transaction. An example of this is a consumer that&rsquo;s
reading a stream to add, update, or remove information in a
data warehouse. As long as creation records are treated as
something like an upsert instead of <code>INSERT</code> and a deletion
is tolerant if the target doesn&rsquo;t exist, then all
operations can safely be considered to be idempotent.</p>

<h3 id="logical-replication" class="link"><a href="#logical-replication">Versus Postgres logical replication</a></h3>

<p>Postgres aficionados might notice that what we&rsquo;ve built
looks pretty similar to <a href="https://www.postgresql.org/docs/10/static/logical-replication.html">logical replication</a>
in Postgres 10, which can similarly guarantee that all
emitted data makes it from producer to consumer.</p>

<p>There are a few advantages to using a stream over logical
replication:</p>

<ul>
<li>It&rsquo;s possible to have multiple producers move information
to a single stream without sharing a database.</li>
<li>Producers can stream a public representation of data
instead of one tied to their internal schema. This allows
producers to change their internal schema without
breaking consumers.</li>
<li>You&rsquo;re less likely to leave yourself tied into fairly
esoteric internal features of Postgres. Understanding how
to best configure and operate subscriptions and
replication slots won&rsquo;t be trivial.</li>
</ul>

<h3 id="absolute" class="link"><a href="#absolute">Are delivery guarantees absolute?</a></h3>

<p>Nothing in software is absolute. We&rsquo;ve built architecture
based on powerful primitives in system design like ACID
transactions and at-least once delivery semantics, and in
practice, it&rsquo;s likely to be quite robust. But not even a
transaction can protect us from every bug, and eventually
something&rsquo;s going to go wrong enough that these safety
features won&rsquo;t be enough &ndash; for example, the code to stage
stream records in the API might be accidentally removed
through human error. Even if it&rsquo;s noticed and fixed
quickly, some inconsistency will have been introduced into
the system.</p>

<p>Consumers that require absolute precision will need a
secondary mechanism that they can run occasionally to
reconcile their state against canonical sources. In
<em>Unified Rocket Rides</em>, we might run a nightly job that
reduces distances across every known ride and emits a tuple
of <code>(total_distance, last_ride_id)</code> that consumers can use
to reset their state before continuing to consume the
stream.</p>

<h2 id="log-architecture" class="link"><a href="#log-architecture">Log-based architecture</a></h2>

<p>Log-based architecture provides an effective backbone for
distributed systems by being fast, efficient, and
resilient. Redis streams will provide (when available at
roughly the end of the year) a user-friendly and ubiquitous
log implementation with which to it. Even while Kafka will
continue to be beneficial to the largest of web platforms,
a stack built on Redis and Postgres will serve quite well
right up until that point.</p>


]]></content>
    <published>2017-11-08T15:15:31Z</published>
    <updated>2017-11-08T15:15:31Z</updated>
    <link href="https://brandur.org/redis-streams"></link>
    <id>tag:brandur.org,2017-11-08:redis-streams</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Implementing Stripe-like Idempotency Keys in Postgres</title>
    <summary>Building resilient services by identifying foreign state mutations and grouping local changes into restartable atomic phases so that every request can be driven to completion.</summary>
    <content type="html"><![CDATA[<p>In APIs <strong><em>idempotency</em></strong> is a powerful concept. An
idempotent endpoint is one that can be called any number of
times while guaranteeing that the side effects will occur
only once. In a messy world where clients and servers that
may occasionally crash or have their connections drop
partway through a request, it&rsquo;s a huge help in making
systems more robust to failure. Clients that are uncertain
whether a request succeeded or failed can simply keep
retrying it until they get a definitive response.</p>

<p>As we&rsquo;re about to see in this article, implementing a
server so that all requests to it are perfectly idempotent
isn&rsquo;t always easy. For endpoints that get away with only
mutating local state in an ACID database, it&rsquo;s possible to
get a robust and simple idempotency implementation by
mapping requests to transactions which I wrote about <a href="/http-transactions">in
more detail a few weeks ago</a>. This
approach is far easier and less complicated than what&rsquo;s
described here, and I&rsquo;d suggest that anyone who can get
away with it take that path.</p>

<h2 id="keys" class="link"><a href="#keys">Idempotency with keys</a></h2>

<p>Implementations that need to make synchronous changes in
foreign state (i.e. outside of a local ACID store) are
somewhat more difficult to design. A basic example of this
is if an app needs to make a Stripe request to create a charge and
needs to know in-band whether it went through so that it
can decide whether to proffer some good or service. To
guarantee idempotency on this type of endpoint we&rsquo;ll need
to introduce <strong><em>idempotency keys</em></strong>.</p>

<p>An idempotency key is a unique value that&rsquo;s generated by a
client and sent to an API along with a request. The server
stores the key to use for bookkeeping the status of that
request on its end. If a request should fail partway
through, the client retries with <em>the same</em> idempotency key
value, and the server uses it to look up the request&rsquo;s
state and continue from where it left off. The name
&ldquo;idempotency key&rdquo; <a href="https://stripe.com/blog/idempotency">comes from Stripe&rsquo;s
API</a>.</p>

<p>A common way to transmit an idempotency key is through an
HTTP header:</p>

<pre><code class="language-sh">POST /v1/charges

...
Idempotency-Key: 0ccb7813-e63d-4377-93c5-476cb93038f3
...

amount=1000&amp;currency=usd
</code></pre>

<p>Once the server knows that a request has definitively
finished by either succeeding or failing in a way that&rsquo;s
not recoverable, it stores the request&rsquo;s results and
associates them with the idempotency key. If a client makes
another request with the same key, the server simply short
circuits and returns the stored results.</p>

<p>Keys are not meant to be used as a permanent request
archive but rather as a mechanism for ensuring near-term
correctness. Servers should recycle them out of the system
beyond a horizon where they won&rsquo;t be of much use &ndash; say 24
hours or so.</p>

<h2 id="rocket-rides" class="link"><a href="#rocket-rides">Rocket Rides</a></h2>

<p>Let&rsquo;s look at how to design idempotency keys for an API by
building a reference implementation.</p>

<p>Our great dev relations team at Stripe has built an app
called <a href="https://github.com/stripe/stripe-connect-rocketrides">Rocket Rides</a> to demonstrate the use
of the Connect platform and other interesting parts of the
API. In Rocket Rides, users who are in a hurry share a ride
with a jetpack-certified pilot to get where they&rsquo;re going
<em>fast</em>. SOMA&rsquo;s gridlock traffic disappears into the
distance as they soar free through virgin skies. Travel can
be a little more risky than Lyft, so make sure to pack an
extra parachute.</p>

<figure>
    <img alt="Rocket Rides the app." class="overflowing" loading="lazy" src="/assets/images/idempotency-keys/rocketrides-ios-ride.png" srcset="/assets/images/idempotency-keys/rocketrides-ios-ride@2x.png 2x, /assets/images/idempotency-keys/rocketrides-ios-ride.png 1x">
    <figcaption>Rocket Rides the app.</figcaption>
</figure>

<p>The <a href="https://github.com/stripe/stripe-connect-rocketrides">Rocket Rides repository</a> comes with a
simple server implementation, but software tends to grow
with time, so to be more representative of what a real
service with 15 engineers and half a dozen product owners
would look like, we&rsquo;re going to complicate things with a
few embellishments.</p>

<h3 id="lifecycle" class="link"><a href="#lifecycle">The request lifecycle</a></h3>

<p>When a new rides comes in we&rsquo;ll perform this set of
operations:</p>

<ol>
<li>Insert an idempotency key record.</li>
<li>Create a ride record to track the ride that&rsquo;s about to
happen.</li>
<li>Create an audit record referencing the ride.</li>
<li><strong>Make an API call to Stripe to charge the user for the
ride</strong> (here we&rsquo;re leaving our own stack, and this
presents some risk).</li>
<li>Update the ride record with the created charge ID.</li>
<li>Send the user a receipt via email.</li>
<li>Update idempotency key with results.</li>
</ol>

<figure>
    <img alt="A typical API request to our embellished Rocket Rides backend." class="overflowing" loading="lazy" src="/assets/images/idempotency-keys/api-request.svg">
    <figcaption>A typical API request to our embellished Rocket Rides backend.</figcaption>
</figure>

<p>Our backend implementation will be called from the Rocket
Rides mobile app with an idempotency key. If a request
fails, the app will continue retrying the operation with
the same key, and our job as backend implementers is to
make sure that&rsquo;s safe. We&rsquo;ll be charging users&rsquo; credit
cards as part of the request, and we absolutely can&rsquo;t take
the risk of charging them twice.</p>

<h3 id="failure" class="link"><a href="#failure">The entropy of production</a></h3>

<p>Most of the time we can expect every one of our Rocket Rides
API calls to go swimmingly, and every operation will succeed
without a problem. However, when we reach the scale of
thousands of API calls a day, we&rsquo;ll start to notice a few
problems appearing here and there; requests failing due to
poor cellular connectivity, API calls to Stripe failing
occasionally, or bad turbulence caused by moving at
supersonic speeds periodically knocking users offline.
After we reach the scale of millions of API calls a day,
basic probability will dictate that we&rsquo;ll be seeing these
sorts of things happening all the time.</p>

<p>Let&rsquo;s look at a few examples of things that can go wrong:</p>

<ul>
<li>Inserting the idempotency key or ride record could fail
due to a constraint violation or a database connectivity
problem.</li>
<li>Our call to Stripe could time out, leaving it unclear
whether our charge went through or not.</li>
<li>Contacting Mailgun to send the receipt could fail,
leaving the user with a credit card charge but no formal
notification of the transaction.</li>
<li>The client could disconnect as they&rsquo;re transmitting a
request to the server, cancelling the operation midway
through.</li>
</ul>

<p>Now that we have a premise in place, let&rsquo;s introduce some
ideas that will let us elegantly solve this problem.</p>

<h2 id="foreign-state" class="link"><a href="#foreign-state">Foreign state mutations</a></h2>

<p>To shore up our backend, it&rsquo;s key to identify where we&rsquo;re
making <strong><em>foreign state mutations</em></strong>; that is, calling out
and manipulating data on another system. This might be
creating a charge on Stripe, adding a DNS record, or
sending an email.</p>

<p>Some foreign state mutations are idempotent by nature (e.g.
adding a DNS record), some are not idempotent but can be
made idempotent with the help of an idempotency key (e.g.
charge on Stripe, sending an email), and some operations
are not idempotent, most often because a foreign service
hasn&rsquo;t designed them that way and doesn&rsquo;t provide a
mechanism like an idempotency key.</p>

<p>The reason that the local vs. foreign distinction matters
is that unlike a local set of operations where we can
leverage an ACID store to roll back a result that we didn&rsquo;t
like, once we make our first foreign state mutation, we&rsquo;re
committed one way or another <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>. <strong>We&rsquo;ve pushed data into a
system beyond our own boundaries and we shouldn&rsquo;t lose
track of it.</strong></p>

<h3 id="two-systems" class="link"><a href="#two-systems">Between any two systems</a></h3>

<p>We&rsquo;re using an API call to Stripe as a common example, but
remember that even foreign calls within your own
infrastructure count! It&rsquo;s tempting to treat emitting
records to Kafka as part of atomic operations because they
have such a high success rate that they feel like they are.
They&rsquo;re not, and should be treated like any other fallible
foreign state mutation.</p>

<h2 id="atomic-phases" class="link"><a href="#atomic-phases">Atomic phases</a></h2>

<p>An <strong><em>atomic phase</em></strong> is a set of local state mutations
that occur in transactions <em>between</em> foreign state
mutations. We say that they&rsquo;re atomic because we can use an
ACID-compliant database like Postgres to guarantee that
either all of them will occur, or none will.</p>

<p>Atomic phases should be safely committed <em>before</em>
initiating any foreign state mutation. If the call fails,
our local state will still have a record of it happening
that we can use to retry the operation.</p>

<h3 id="recovery-points" class="link"><a href="#recovery-points">Recovery points</a></h3>

<p>A <strong><em>recovery point</em></strong> is a name of a check point that we
get to after having successfully executed any atomic phase
<em>or</em> foreign state mutation. Its purpose is to allow a
request that&rsquo;s being retried to jump back to the point in
the lifecycle just before the last attempt failed.</p>

<p>For convenience, we&rsquo;re going to store the name of the
recovery point reached right onto the idempotency key
relation that we&rsquo;ll build. All requests will initially get
a recovery point of <code>started</code>, and after any request is
complete (again, through either a success or definitive
error) it&rsquo;ll be assigned a recovery point of <code>finished</code>.
When in an atomic phase, the transition to a new recovery
point should be committed as part of that phase&rsquo;s
transaction.</p>

<h3 id="background-jobs" class="link"><a href="#background-jobs">Background jobs and staging</a></h3>

<p>In-band foreign state mutations make a request slower and
more difficult to reason about, so they should be avoided
when possible. In many cases it&rsquo;s possible to defer this
type of work to after the request is complete by sending it
to a background job queue.</p>

<p>In our Rocket Rides example the charge to Stripe probably
<em>can&rsquo;t</em> be deferred &ndash; we want to know whether it succeeded
right away so that we can deny the request if it didn&rsquo;t.
Sending an email <em>can</em> and should be sent to the
background.</p>

<p>By using a <a href="/job-drain"><em>transactionally-staged job
drain</em></a>, we can hide jobs from workers until
we&rsquo;ve confirmed that they&rsquo;re ready to be worked by
isolating them in a transaction. This also means that the
background work becomes part of an atomic phase and greatly
simplifies its operational properties. Work should always
be offloaded to background queues wherever possible.</p>

<h2 id="interstellar" class="link"><a href="#interstellar">Hardening Rocket Rides for interstellar travel</a></h2>

<p>Now that we&rsquo;ve covered a few key concepts, we&rsquo;re ready to
shore up Rocket Rides so that it&rsquo;s resilient against any
kind of failure imaginable. Let&rsquo;s put together the basic
schema, break the lifecycle up into atomic phases, and
assemble a simple implementation that will recover from
failures.</p>

<p>A working version (with testing) of all of this is
available in the <a href="https://github.com/brandur/rocket-rides-atomic"><em>Atomic Rocket Rides</em></a>
repository. It might be easier to download that code and
follow along.</p>

<pre><code class="language-sh">git clone https://github.com/brandur/rocket-rides-atomic.git
</code></pre>

<h3 id="idempotency-key" class="link"><a href="#idempotency-key">The idempotency key relation</a></h3>

<p>Let&rsquo;s design a Postgres schema for idempotency keys in our
app:</p>

<pre><code class="language-sql">CREATE TABLE idempotency_keys (
    id              BIGSERIAL   PRIMARY KEY,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT now(),
    idempotency_key TEXT        NOT NULL
        CHECK (char_length(idempotency_key) &lt;= 100),
    last_run_at     TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    locked_at       TIMESTAMPTZ DEFAULT now(),

    -- parameters of the incoming request
    request_method  TEXT        NOT NULL
        CHECK (char_length(request_method) &lt;= 10),
    request_params  JSONB       NOT NULL,
    request_path    TEXT        NOT NULL
        CHECK (char_length(request_path) &lt;= 100),

    -- for finished requests, stored status code and body
    response_code   INT         NULL,
    response_body   JSONB       NULL,

    recovery_point  TEXT        NOT NULL
        CHECK (char_length(recovery_point) &lt;= 50),
    user_id         BIGINT      NOT NULL
);

CREATE UNIQUE INDEX idempotency_keys_user_id_idempotency_key
    ON idempotency_keys (user_id, idempotency_key);
</code></pre>

<p>There are a few notable fields here:</p>

<ul>
<li><p><code>idempotency_key</code>: This is the user-specified idempotency
key. It&rsquo;s good practice to send something with good
randomness like a UUID, but not necessarily required. We
constrain the field&rsquo;s length so that nobody sends us
anything too exotic.</p>

<p>We&rsquo;ve made <code>idempotency_key</code> unique, but across
<code>(user_id, idempotency_key)</code> so that it&rsquo;s possible to
have the same idempotency key for different requests as
long as it&rsquo;s across different user accounts.</p></li>

<li><p><code>locked_at</code>: A field that indicates whether this
idempotency key is actively being worked. The first API
request that creates the key will lock it automatically,
but subsequent retries will also set it to make sure that
they&rsquo;re the only request doing the work.</p></li>

<li><p><code>params</code>: The input parameters of the request. This is
stored mostly so that we can error if the user sends two
requests with the same idempotency key but with different
parameters, but can also be used for our own backend to
push unfinished requests to completion (see <a href="#completionist">the
completionist</a> below).</p></li>

<li><p><code>recovery_point</code>: A text label for the last phase
completed for the idempotent request (see <a href="#recovery-points">recovery
points</a> above). Gets an initial value
of <code>started</code> and is set to <code>finished</code> when the request is
considered to be complete.</p></li>
</ul>

<h3 id="other-schema" class="link"><a href="#other-schema">Other schema</a></h3>

<p>Recall our target API lifecycle for Rocket Rides from
above.</p>

<figure>
    <img alt="A typical API request to our embellished Rocket Rides backend." class="overflowing" loading="lazy" src="/assets/images/idempotency-keys/api-request.svg">
    <figcaption>A typical API request to our embellished Rocket Rides backend.</figcaption>
</figure>

<p>Let&rsquo;s bring up Postgres relations for everything else we&rsquo;ll
need to build this app including audit records, rides, and
users. Given that we aim to maximize reliability, we&rsquo;ll try
to follow database best practices and use <code>NOT NULL</code>,
unique, and foreign key constraints wherever we can.</p>

<pre><code class="language-sql">--
-- A relation to hold records for every user of our app.
--
CREATE TABLE users (
    id                 BIGSERIAL       PRIMARY KEY,
    email              TEXT            NOT NULL UNIQUE
        CHECK (char_length(email) &lt;= 255),

    -- Stripe customer record with an active credit card
    stripe_customer_id TEXT            NOT NULL UNIQUE
        CHECK (char_length(stripe_customer_id) &lt;= 50)
);

--
-- Now that we have a users table, add a foreign key
-- constraint to idempotency_keys which we created above.
--
ALTER TABLE idempotency_keys
    ADD CONSTRAINT idempotency_keys_user_id_fkey
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE RESTRICT;

--
-- A relation that hold audit records that can help us piece
-- together exactly what happened in a request if necessary
-- after the fact. It can also, for example, be used to
-- drive internal security programs tasked with looking for
-- suspicious activity.
--
CREATE TABLE audit_records (
    id                 BIGSERIAL       PRIMARY KEY,

    -- action taken, for example &quot;created&quot;
    action             TEXT            NOT NULL
        CHECK (char_length(action) &lt;= 50),

    created_at         TIMESTAMPTZ     NOT NULL DEFAULT now(),
    data               JSONB           NOT NULL,
    origin_ip          CIDR            NOT NULL,

    -- resource ID and type, for example &quot;ride&quot; ID 123
    resource_id        BIGINT          NOT NULL,
    resource_type      TEXT            NOT NULL
        CHECK (char_length(resource_type) &lt;= 50),

    user_id            BIGINT          NOT NULL
        REFERENCES users ON DELETE RESTRICT
);

--
-- A relation representing a single ride by a user.
-- Notably, it holds the ID of a successful charge to
-- Stripe after we have one.
--
CREATE TABLE rides (
    id                 BIGSERIAL       PRIMARY KEY,
    created_at         TIMESTAMPTZ     NOT NULL DEFAULT now(),

    -- Store a reference to the idempotency key so that we can recover an
    -- already-created ride. Note that idempotency keys are not stored
    -- permanently, so make sure to SET NULL when a referenced key is being
    -- reaped.
    idempotency_key_id BIGINT
        REFERENCES idempotency_keys ON DELETE SET NULL,

    -- origin and destination latitudes and longitudes
    origin_lat         NUMERIC(13, 10) NOT NULL,
    origin_lon         NUMERIC(13, 10) NOT NULL,
    target_lat         NUMERIC(13, 10) NOT NULL,
    target_lon         NUMERIC(13, 10) NOT NULL,

    -- ID of Stripe charge like ch_123; NULL until we have one
    stripe_charge_id   TEXT            UNIQUE
        CHECK (char_length(stripe_charge_id) &lt;= 50),

    user_id            BIGINT          NOT NULL
        REFERENCES users ON DELETE RESTRICT,
    CONSTRAINT rides_user_id_idempotency_key_unique UNIQUE (user_id, idempotency_key_id)
);

CREATE INDEX rides_idempotency_key_id
    ON rides (idempotency_key_id)
    WHERE idempotency_key_id IS NOT NULL;

--
-- A relation that holds our transactionally-staged jobs
-- (see &quot;Background jobs and job staging&quot; above).
--
CREATE TABLE staged_jobs (
    id                 BIGSERIAL       PRIMARY KEY,
    job_name           TEXT            NOT NULL,
    job_args           JSONB           NOT NULL
);
</code></pre>

<h3 id="rocket-rides-phases" class="link"><a href="#rocket-rides-phases">Designing atomic phases</a></h3>

<p>Now that we&rsquo;ve got a feel for what our data should look
like, let&rsquo;s break the API request into distinct atomic
phases. These are the basic rules for identifying them:</p>

<ol>
<li>Upserting the idempotency key gets its own atomic phase.</li>
<li>Every foreign state mutation gets its own atomic phase.</li>
<li>After those phases have been identified, <strong><em>all other
operations between</em></strong> them are grouped into atomic
phases. Even if there are 100 operations against an ACID
database between two foreign state mutations, they can
all safely belong to the same phase.</li>
</ol>

<p>So in our example, we have an atomic phase for inserting
the idempotency key (<code>tx1</code>) and another for making our charge
call to Stripe (<code>tx3</code>) and storing the result. Every other
operation around <code>tx1</code> and <code>tx3</code> gets grouped together and
becomes part of two more phases, <code>tx2</code> and <code>tx4</code>. <code>tx2</code>
through <code>tx4</code> can each be reached by a recovery point
that&rsquo;s set by the transaction that committed before it
(<code>started</code>, <code>ride_created</code>, and <code>charge_created</code>).</p>

<figure>
    <img alt="API request to Rocket Rides broken into foreign state mutations and atomic phases." class="overflowing" loading="lazy" src="/assets/images/idempotency-keys/atomic-phases.svg">
    <figcaption>API request to Rocket Rides broken into foreign state mutations and atomic phases.</figcaption>
</figure>

<h3 id="atomic-phase-implementation" class="link"><a href="#atomic-phase-implementation">Atomic phase implementation</a></h3>

<p>Our implementation for an atomic phase will wrap everything
in a transaction block (note we&rsquo;re using Ruby, but this
same concept is possible in any language) and give each
phase three options for what it can return:</p>

<ol>
<li>A <code>RecoveryPoint</code> which sets a new recovery point. This
happens within the same transaction as the rest of the
phase so it&rsquo;s all guaranteed to be atomic. Execution
continues normally into the next phase.</li>
<li>A <code>Response</code> which sets the idempotent request&rsquo;s
recovery point to <code>finished</code> and returns a response to
the user. This should be used as part of the normal
success condition, but can also be used to return early
with a non-recoverable error. Say for example that a
user&rsquo;s credit card is not valid &ndash; no matter how many
times the request is retried, it will never go through.</li>
<li>A <code>NoOp</code> which indicates that program flow should
continue, but that neither a recovery point nor response
should be set.</li>
</ol>

<p>Don&rsquo;t worry about parsing the specific code too much, but
here&rsquo;s what it might look like:</p>

<pre><code class="language-ruby">def atomic_phase(key, &amp;block)
  error = false
  begin
    DB.transaction(isolation: :serializable) do
      ret = block.call

      if ret.is_a?(NoOp) || ret.is_a?(RecoveryPoint) || ret.is_a?(Response)
        ret.call(key)
      else
        raise &quot;Blocks to #atomic_phase should return one of &quot; \
          &quot;NoOp, RecoveryPoint, or Response&quot;
      end
    end
  rescue Sequel::SerializationFailure
    # you could possibly retry this error instead
    error = true
    halt 409, JSON.generate(wrap_error(Messages.error_retry))
  rescue
    error = true
    halt 500, JSON.generate(wrap_error(Messages.error_internal))
  ensure
    # If we're leaving under an error condition, try to unlock the idempotency
    # key right away so that another request can try again.
    if error &amp;&amp; !key.nil?
      begin
        key.update(locked_at: nil)
      rescue StandardError
        # We're already inside an error condition, so swallow any additional
        # errors from here and just send them to logs.
        puts &quot;Failed to unlock key #{key.id}.&quot;
      end
    end
  end
end

# Represents an action to perform a no-op. One possible option for a return
# from an #atomic_phase block.
class NoOp
  def call(_key)
    # no-op
  end
end

# Represents an action to set a new recovery point. One possible option for a
# return from an #atomic_phase block.
class RecoveryPoint
  attr_accessor :name

  def initialize(name)
    self.name = name
  end

  def call(key)
    raise ArgumentError, &quot;key must be provided&quot; if key.nil?
    key.update(recovery_point: name)
  end
end

# Represents an action to set a new API response (which will be stored onto an
# idempotency key). One  possible option for a return from an #atomic_phase
# block.
class Response
  attr_accessor :data
  attr_accessor :status

  def initialize(status, data)
    self.status = status
    self.data = data
  end

  def call(key)
    raise ArgumentError, &quot;key must be provided&quot; if key.nil?
    key.update(
      locked_at: nil,
      recovery_point: RECOVERY_POINT_FINISHED,
      response_code: status,
      response_body: data
    )
  end
end

</code></pre>

<p>In the case of a serialization error, we return a <code>409
Conflict</code> because that almost certainly means that a
concurrent request conflicted with what we were trying to
do. In a real app, you probably want to just retry the
operation right away because there&rsquo;s a good chance it will
succeed this time.</p>

<p>For other errors we return a <code>500 Internal Server Error</code>.
For either type of error, we try to unlock the idempotency
key before finishing so that another request has a chance
to retry with it.</p>

<h3 id="upserting-key-upsert" class="link"><a href="#upserting-key-upsert">Idempotency key upsert</a></h3>

<p>When a new idempotency key value comes into the API, we&rsquo;re
going to create or update a corresponding row that we&rsquo;ll
use to track its progress.</p>

<p>The easiest case is if we&rsquo;ve never seen the key before. If
so, just insert a new row with appropriate values.</p>

<p>If we have seen the key, lock it so that no other
requests that might be operating concurrently also try the
operation. If the key was already locked, return a <code>409
Conflict</code> to indicate that to the user.</p>

<p>A key that&rsquo;s already set to <code>finished</code> is simply allowed to
fall through and have its response return on the standard
success path. We&rsquo;ll see that in just a moment.</p>

<pre><code class="language-ruby">key = nil

atomic_phase(key) do
  key = IdempotencyKey.first(user_id: user.id, idempotency_key: key_val)

  if key
    # Programs sending multiple requests with different parameters but the
    # same idempotency key is a bug.
    if key.request_params != params
      halt 409, JSON.generate(wrap_error(Messages.error_params_mismatch))
    end

    # Only acquire a lock if the key is unlocked or its lock has expired
    # because the original request was long enough ago.
    if key.locked_at &amp;&amp; key.locked_at &gt; Time.now - IDEMPOTENCY_KEY_LOCK_TIMEOUT
      halt 409, JSON.generate(wrap_error(Messages.error_request_in_progress))
    end

    # Lock the key and update latest run unless the request is already
    # finished.
    if key.recovery_point != RECOVERY_POINT_FINISHED
      key.update(last_run_at: Time.now, locked_at: Time.now)
    end
  else
    key = IdempotencyKey.create(
      idempotency_key: key_val,
      locked_at:       Time.now,
      recovery_point:  RECOVERY_POINT_STARTED,
      request_method:  request.request_method,
      request_params:  Sequel.pg_jsonb(params),
      request_path:    request.path_info,
      user_id:         user.id,
    )
  end

  # no response and no need to set a recovery point
  NoOp.new
end
</code></pre>

<p>At first glance this code might not look like it&rsquo;s safe
from having two concurrent requests come in in close
succession and try to the lock the same key, but it is
because the atomic phase is wrapped in a <code>SERIALIZABLE</code>
transaction. If two different transactions both try to lock
any one key, one of them will be aborted by Postgres.</p>

<h3 id="acyclic-state-machine" class="link"><a href="#acyclic-state-machine">A directed and acyclic state machine</a></h3>

<p>We&rsquo;re going to implement the rest of the API request as a
simple state machines whose states are a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic
graph (DAG)</a>. Unlike a normal graph, a DAG moves only
in one direction and never cycles back on itself.</p>

<p>Each atomic phase will be activated from a recovery point,
which was either read from a recovered idempotency key, or
set by the previous atomic phase. We continue to move
through phases until reaching a <code>finished</code> state, upon
which the loop is broken and a response is sent back to the
user.</p>

<p>An idempotency key that was already finished will enter the
loop, break immediately, and send back whatever response
was stored onto it.</p>

<pre><code class="language-ruby">loop do
  case key.recovery_point
  when RECOVERY_POINT_STARTED
    atomic_phase(key) do
      ...
    end

  when RECOVERY_POINT_RIDE_CREATED
    atomic_phase(key) do
      ...
    end

  when RECOVERY_POINT_CHARGE_CREATED
    atomic_phase(key) do
      ....
    end

  when RECOVERY_POINT_FINISHED
    break

  else
    raise &quot;Bug! Unhandled recovery point '#{key.recovery_point}'.&quot;
  end

  # If we got here, allow the loop to move us onto the next phase of the
  # request. Finished requests will break the loop.
end

[key.response_code, JSON.generate(key.response_body)]
</code></pre>

<h3 id="initial-bookkeeping" class="link"><a href="#initial-bookkeeping">Initial bookkeeping</a></h3>

<p>The second phase (<code>tx2</code> in the diagram above) is simple:
create a record for the ride in our local database, insert
an audit record, and set a new recovery point to
<code>ride_created</code>.</p>

<pre><code class="language-ruby">atomic_phase(key) do
  ride = Ride.create(
    idempotency_key_id: key.id,
    origin_lat:         params[&quot;origin_lat&quot;],
    origin_lon:         params[&quot;origin_lon&quot;],
    target_lat:         params[&quot;target_lat&quot;],
    target_lon:         params[&quot;target_lon&quot;],
    stripe_charge_id:   nil, # no charge created yet
    user_id:            user.id,
  )

  # in the same transaction insert an audit record for what happened
  AuditRecord.insert(
    action:        AUDIT_RIDE_CREATED,
    data:          Sequel.pg_jsonb(params),
    origin_ip:     request.ip,
    resource_id:   ride.id,
    resource_type: &quot;ride&quot;,
    user_id:       user.id,
  )

  RecoveryPoint.new(RECOVERY_POINT_RIDE_CREATED)
end
</code></pre>

<h3 id="calling-stripe" class="link"><a href="#calling-stripe">Calling Stripe</a></h3>

<p>With basic records in place, it&rsquo;s time to try our foreign
state mutation by trying to charge the customer via Stripe.
Here we initiate a charge for $20 using a Stripe customer
ID that was already stored on their user record. On
success, update the ride created in the last step with the
new Stripe charge ID and set recovery point
<code>charge_created</code>.</p>

<pre><code class="language-ruby">atomic_phase(key) do
  # retrieve a ride record if necessary (i.e. we're recovering)
  ride = Ride.first(idempotency_key_id: key.id) if ride.nil?

  # if ride is still nil by this point, we have a bug
  raise &quot;Bug! Should have ride for key at #{RECOVERY_POINT_RIDE_CREATED}.&quot; \
    if ride.nil?

  raise &quot;Simulated fail with `raise_error` param.&quot; if raise_error

  # Rocket Rides is still a new service, so during our prototype phase
  # we're going to give $20 fixed-cost rides to everyone, regardless of
  # distance. We'll implement a better algorithm later to better
  # represent the cost in time and jetfuel on the part of our pilots.
  begin
    charge = Stripe::Charge.create({
      amount:      20_00,
      currency:    &quot;usd&quot;,
      customer:    user.stripe_customer_id,
      description: &quot;Charge for ride #{ride.id}&quot;,
    }, {
      # Pass through our own unique ID rather than the value
      # transmitted to us so that we can guarantee uniqueness to Stripe
      # across all Rocket Rides accounts.
      idempotency_key: &quot;rocket-rides-atomic-#{key.id}&quot;
    })
  rescue Stripe::CardError
    # Sets the response on the key and short circuits execution by
    # sending execution right to 'finished'.
    Response.new(402, wrap_error(Messages.error_payment(error: $!.message)))
  rescue Stripe::StripeError
    Response.new(503, wrap_error(Messages.error_payment_generic))
  else
    ride.update(stripe_charge_id: charge.id)
    RecoveryPoint.new(RECOVERY_POINT_CHARGE_CREATED)
  end
end
</code></pre>

<p>The call to Stripe produces a few possibilities for
unrecoverable errors (i.e. an error that no matter how many
times is retried will never see the call succeed). If we
run into one, set the request to <code>finished</code> and return an
appropriate response. This might occur if the credit card
was invalid or the transaction was otherwise declined by
the payment gateway.</p>

<h3 id="send-receipt-finish" class="link"><a href="#send-receipt-finish">Send receipt and finish</a></h3>

<p>Now that our charge has been persisted, the next step is to
send a receipt to the user. Making an external mail call
would normally require its own foreign state mutation, but
because we&rsquo;re using a transactionally-staged job drain, we
get a guarantee that the operation commits along with the
rest of the transaction.</p>

<pre><code class="language-ruby">atomic_phase(key) do
  StagedJob.insert(
    job_name: &quot;send_ride_receipt&quot;,
    job_args: Sequel.pg_jsonb({
      amount:   20_00,
      currency: &quot;usd&quot;,
      user_id:  user.id
    })
  )
  Response.new(201, wrap_ok(Messages.ok))
end
</code></pre>

<p>The final step is to set a response telling the user that
everything worked as expected. We&rsquo;re done!</p>

<h2 id="other-processes" class="link"><a href="#other-processes">Other processes</a></h2>

<p>Besides the web process running the API, a few others are
needed to make everything work (see <a href="https://github.com/brandur/rocket-rides-atomic/blob/master/Procfile"><em>Atomic Rocket Ride</em>&rsquo;s
<code>Procfile</code></a> for the full list and the
corresponding implementations in the same repository).</p>

<h3 id="enqueuer" class="link"><a href="#enqueuer">The enqueuer</a></h3>

<p>There should be an <strong><em>enqueuer</em></strong> that moves jobs from
<code>staged_jobs</code> to the job queue after their inserting
transaction has committed. See <a href="/job-drain">this article</a> for
details on how to build one, or <a href="https://github.com/brandur/rocket-rides-atomic/blob/master/enqueuer.rb">the
implementation</a> from <em>Atomic Rocket Rides</em>.</p>

<h3 id="completer" class="link"><a href="#completer">The completer</a></h3>

<p>One problem with this implementation is we&rsquo;re reliant on
clients to push indeterminate requests (for example, one
that might have appeared to be a timeout) to completion.
Usually clients are willing to do this because they want to
see their requests go through, but there can be cases where
a client starts working, never quite finishes, and drops
forever.</p>

<p>A stretch goal is to implement a <strong><em>completer</em></strong>. Its only
job is to find requests that look like they never finished
to satisfaction and which it looks like clients have
dropped, and push through to completion.</p>

<p>It doesn&rsquo;t even have to have special knowledge about how
the stack is implemented. It just needs to know how to read
idempotency keys and have a specialized internal
authentication path that allows it to retry anyone&rsquo;s
request.</p>

<p>See the <em>Atomic Rocket Rides</em> repository for <a href="https://github.com/brandur/rocket-rides-atomic/blob/master/completer.rb">a completer
implementation</a>.</p>

<h3 id="reaper" class="link"><a href="#reaper">The reaper</a></h3>

<p>Idempotency keys are meant to act as a mechanism for
guaranteeing idempotence, and not as a permanent archive of
historical requests. After some amount of time a
<strong><em>reaper</em></strong> process should go through keys and delete
them.</p>

<p>I&rsquo;d suggest a threshold of about 72 hours so that even if a
bug is deployed on Friday that errors a large number of
valid requests, an app could still keep a record of them
throughout the weekend and onto Monday where a developer
would have a chance to commit a fix and have the completer
push them through to success.</p>

<p>An ideal reaper might even notice requests that could not
be finished successfully and try to do some cleanup on
them. If cleanup is difficult or impossible, it should put
them in a list somewhere so that a human can find out what
failed.</p>

<p>See the <em>Atomic Rocket Rides</em> repository for <a href="https://github.com/brandur/rocket-rides-atomic/blob/master/reaper.rb">a reaper
implementation</a>.</p>

<h2 id="murphys-law" class="link"><a href="#murphys-law">Murphy in action</a></h2>

<p>Now that we have all the pieces in place, let&rsquo;s assume the
truth of <a href="https://en.wikipedia.org/wiki/Murphy%27s_law">Murphy&rsquo;s Law</a> and imagine some
scenarios that could go wrong while a client app is talking
to the new <em>Atomic Rocket Rides</em> backend:</p>

<ul>
<li><p><em>The client makes a request, but the connection breaks
before it reaches the backend:</em> The client, having used
an idempotency key, knows that retries are safe and so
retries. The next attempt succeeds.</p></li>

<li><p><em>Two requests try to create an idempotency key at the
same time:</em> A <code>UNIQUE</code> constraint in the database
guarantees that only one request can succeed. One goes
through, and the other gets a <code>409 Conflict</code>.</p></li>

<li><p><em>An idempotency key is created, but the database goes
down and it fails soon after:</em> The client continues to
retry against the API until it comes back online. Once it
does, the created key is recovered and the request is
continued.</p></li>

<li><p><em>Stripe is down:</em> The atomic phase containing the Stripe
request fails, and the API responds with an error that
tells the client to retry. They continue to do so until
Stripe comes back online and the charge succeeds.</p></li>

<li><p><em>A server process dies while waiting for a response from
Stripe:</em> Luckily, the call to Stripe was also made with
its own idempotency key. The client retries and a new
call to Stripe is invoked with the same key. Stripe&rsquo;s own
idempotency guarantees ensure that we haven&rsquo;t
double-charged our user.</p></li>

<li><p><em>A bad deploy 500s all requests midway through:</em>
Developers scramble and deploy a fix for the bug. After
it&rsquo;s out, clients retry and the original requests succeed
along the newly bug-free path. If the fix took so long to
get out that clients have long since gone away, then the
completer process pushes them through.</p></li>
</ul>

<p>Our care around implementing a failsafe design has paid off
&ndash; the system is safe despite a wide variety of possible
failures.</p>

<h2 id="complications" class="link"><a href="#complications">Complications</a></h2>

<h3 id="non-idempotent" class="link"><a href="#non-idempotent">Non-idempotent foreign state mutations</a></h3>

<p>If we know that a foreign state mutation is an idempotent
operation or it supports an idempotency key (like Stripe
does), we know that it&rsquo;s safe to retry any failures that we
see.</p>

<p>Unfortunately, not every service will make this guarantee.
If we try to make a non-idempotent foreign state mutation
and we see a failure, we may have to persist this operation
as permanently errored. In many cases we won&rsquo;t know whether
it&rsquo;s safe to retry or not, and we&rsquo;ll have to take the
conservative route and fail the operation.</p>

<p>The exception is if we got an error back from the
non-idempotent API, but one that tell us explicitly that
it&rsquo;s okay to retry. Indeterminate errors like a connection
reset or timeout will have to be marked as failed.</p>

<p>This is why you should implement idempotency and/or
idempotency keys on all your services!</p>

<h3 id="non-acid" class="link"><a href="#non-acid">Non-ACID data stores</a></h3>

<p>It&rsquo;s worth mentioning that none of this is possible on a
non-ACID store like MongoDB. Without transactional
semantics a database can&rsquo;t ever guarantee that any two
operations commit atomically &ndash; <em>every</em> operation against
your database becomes equivalent to a foreign state
mutation because the notion of an atomic phase is
impossible.</p>

<h2 id="beyond-apis" class="link"><a href="#beyond-apis">Beyond APIs</a></h2>

<p>This article focuses heavily on APIs, but note that this
same technique is reusable for other software as well. A
common problem in web apps is double form submission. A
user clicking the &ldquo;Submit&rdquo; button twice in quick succession
may initiate two separate HTTP calls, and in cases where
submissions have non-idempotent side effects (e.g. charging
the user) this is a problem.</p>

<p>When rendering the form initially, we can add a <code>&lt;input
type=&quot;hidden&quot;&gt;</code> to it that contains an idempotency key.
This value will stay the same across multiple submissions,
and the server can use it to dedup the request.</p>

<h2 id="passive-safety" class="link"><a href="#passive-safety">Cultivating passive safety</a></h2>

<p>API backends should aim to be <em>passively safe</em> &ndash; no matter
what kind of failures are thrown at them they&rsquo;ll end up in
a stable state, and users are never left broken even in the
most extreme cases. From there, active mechanisms can drive
the system towards perfect cohesion. Ideally, human
operators never have to intervene to fix things (or at
least as infrequently as possible).</p>

<p><a href="/http-transactions">Purely idempotent transactions</a> and
the idempotency keys with atomic phases described here are
two ways to move in that direction. Failures are not only
understood to be possible, but are expected, and enough
thought has been applied to the system&rsquo;s design that we
know it&rsquo;ll tolerate failure cleanly no matter what happens.</p>


]]></content>
    <published>2017-10-27T13:52:12Z</published>
    <updated>2017-10-27T13:52:12Z</updated>
    <link href="https://brandur.org/idempotency-keys"></link>
    <id>tag:brandur.org,2017-10-27:idempotency-keys</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
  <entry>
    <title>Should You Build a Webhooks API?</title>
    <summary>When it comes to streaming APIs, there&amp;rsquo;s now a lot of great options like SSE, GraphQL subscriptions, and GRPC streams. Let&amp;rsquo;s examine whether webhooks are still a good choice in 2017.</summary>
    <content type="html"><![CDATA[<p>The term &ldquo;webhook&rdquo; was coined back in 2007 by Jeff Lindsay
as a &ldquo;hook&rdquo; (or callback) for the web; meant to be a
general purpose system to allow Internet systems to be
composed in the same spirit as the Unix pipe. By speaking
HTTP and being symmetrical to common HTTP APIs, they were
an elegant answer to a problem without many options at the
time &ndash; <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a> wouldn&rsquo;t be standardized
until 2011 and would only see practical use much later.
Most other contemporary streaming options were still only a
distant speck on the horizon.</p>

<p>For a few very common APIs like GitHub, Slack, or Stripe,
the push stream available over webhooks might be one of the
best-known features. They&rsquo;re reliable, can be used to
configure multiple receivers that receive customized sets
of events, and they even work for accounts connected via
OAuth, allowing platforms built on the APIs to tie into the
activity of their users. They&rsquo;re a great feature and
aren&rsquo;t going anywhere anytime soon, but they&rsquo;re also far
from perfect. In the spirit of avoiding <a href="/accidental-evangelist">accidental
evangelism</a>, here we&rsquo;ll talk about
whether they&rsquo;re a good pattern for new API providers to
emulate.</p>

<h2 id="case" class="link"><a href="#case">A basic case for webhooks</a></h2>

<p>First, let&rsquo;s take a look at why webhooks are useful. While
REST APIs commonly make up the backbone for accessing and
manipulating information in a web platform, webhooks are
often used as a second facet that augments it by streaming
real-time updates.</p>

<p>Say you&rsquo;re going to write a mini-CI service that will build
any branches that are opened via pull request on one of
your GitHub repositories. Like Travis, we want it to be
able to detect a pull request, and then assign it a status
check icon that will only be resolved when the build
completes.</p>

<figure>
    <img alt="Travis putting status checks on a pull request that are contingent on a successful build." class="overflowing" loading="lazy" src="/assets/images/webhooks/github-status-check.png" srcset="/assets/images/webhooks/github-status-check@2x.png 2x, /assets/images/webhooks/github-status-check.png 1x">
    <figcaption>Travis putting status checks on a pull request that are contingent on a successful build.</figcaption>
</figure>

<p>GitHub has a <a href="https://developer.github.com/v3/repos/statuses/">status API</a> that can assign or
update statuses associated with a given commit SHA. With
just a REST API, we&rsquo;d have to poll the list endpoint every
few seconds to know when new pull requests come in. Luckily
though, there&rsquo;s much better way: we can listen on for
GitHub&rsquo;s <code>pull_request</code> webhook, and it&rsquo;ll notify us when
anything changes.</p>

<p>Our CI service listens for <code>pull_request</code> webhooks, creates
a new status via the REST API when it sees one, and then
updates that status when its corresponding build succeeds
or fails. It&rsquo;s able to add status checks in a timely manner
(ideally users see a <code>pending</code> status the moment they
open a new pull), and with no inefficient polling involved.</p>

<figure>
    <img alt="A basic webhooks flow to build a simple CI system for GitHub." class="overflowing" loading="lazy" src="/assets/images/webhooks/ci.svg">
    <figcaption>A basic webhooks flow to build a simple CI system for GitHub.</figcaption>
</figure>

<h2 id="user-ergonomics" class="link"><a href="#user-ergonomics">The virtues of user ergonomics</a></h2>

<p>As we see above webhooks are convenient and work pretty
well, but that said, they&rsquo;re far from perfect in a number
of places. Let&rsquo;s look at a few ways that using them can be
a little painful.</p>

<h3 id="endpoints" class="link"><a href="#endpoints">Endpoint provisioning and management</a></h3>

<p>Getting an HTTP endpoint provisioned to receive a webhook
isn&rsquo;t technically difficult, but it can be bureaucratically
so.</p>

<p>The classic example is the large enterprise where getting a
new endpoint exposed to the outside world might be a
considerable project involving negotiations with
infrastructure and security teams, requisitioning new
hardware, and piles of paperwork. In the worst cases,
webhooks might be wholly incompatible with an
organization&rsquo;s security model where user data is
uncompromisingly kept within a secured perimeter at all
times.</p>

<figure>
    <img alt="Difficulty in provisioning an HTTP endpoint that can talk to the outside world." class="overflowing" loading="lazy" src="/assets/images/webhooks/provisioning-woes.svg">
    <figcaption>Difficulty in provisioning an HTTP endpoint that can talk to the outside world.</figcaption>
</figure>

<p>Development and testing are also difficult cases. There&rsquo;s
no perfectly fluid way of getting an endpoint from a
locally running environment exposed for a webhook provider
to access. Programs like <a href="https://ngrok.com/">Ngrok</a> are good options,
but still add a step and complication that wouldn&rsquo;t be
necessary with an alternate scheme.</p>

<h3 id="security" class="link"><a href="#security">Uncertain security</a></h3>

<p>Because webhook endpoints are publicly accessible HTTP
APIs, it&rsquo;s up to providers to build in a security scheme
to ensure that an attacker can&rsquo;t issue malicious requests
containing forged payloads. There&rsquo;s a variety of commonly
seen techniques:</p>

<ol>
<li><strong><em>Webhook signing:</em></strong> Sign webhook payloads and send the
signature via HTTP header so that users can verify it.</li>
<li><strong><em>HTTP authentication:</em></strong> Force users to provide HTTP
basic auth credentials when they configure endpoints to
receive webhooks.</li>
<li><strong><em>API retrieval:</em></strong> Provide only an event identifier in
webhook payload and force recipients to make a
synchronous API request to get the message&rsquo;s full
contents.</li>
</ol>

<figure>
    <img alt="Endpoint signing secrets in Stripe's dashboard." class="overflowing" loading="lazy" src="/assets/images/webhooks/signing-secrets.png" srcset="/assets/images/webhooks/signing-secrets@2x.png 2x, /assets/images/webhooks/signing-secrets.png 1x">
    <figcaption>Endpoint signing secrets in Stripe's dashboard.</figcaption>
</figure>

<p>Good security is possible, but a fundamental problem with
webhooks is that it&rsquo;s difficult as a provider to <em>ensure</em>
that your users are following best practices. Of the three
options above, only the third guarantees strong security;
even if you provide signatures you can&rsquo;t know for sure that
your users are verifying them, and if forced to provide
HTTP basic auth credentials, many users will opt for weak
ones, which combined with endpoints that are probably not
rate limited, leave them vulnerable to brute force attacks.</p>

<p>This is in sharp contrast to synchronous APIs where a
provider gets to choose exactly what API keys will look
like and dictate best practices around how they&rsquo;re issued
and how often they&rsquo;re rotated.</p>

<h3 id="development" class="link"><a href="#development">Development and testing</a></h3>

<p>It&rsquo;s relatively easy to provide a stub or live testmode for
a synchronous API, but a little more difficult for
webhooks because the user needs some mechanic to request
that a test webhook be sent.</p>

<p>At Stripe, we provide a &ldquo;Send test webhook&rdquo; function from
the dashboard. This provides a reasonable developer
experience in that at least testing an endpoint is
possible, but it&rsquo;s manual and not especially conducive to
being integrated into an automated test suite.</p>

<figure>
    <img alt="Sending a test webhook in Stripe's dashboard." class="overflowing" loading="lazy" src="/assets/images/webhooks/send-test-webhook.png" srcset="/assets/images/webhooks/send-test-webhook@2x.png 2x, /assets/images/webhooks/send-test-webhook.png 1x">
    <figcaption>Sending a test webhook in Stripe's dashboard.</figcaption>
</figure>

<p>Most developers will know that manual testing is never
enough. It&rsquo;ll get a program working today and that program
will probably stay working tomorrow, but without more
comprehensive CI something&rsquo;s likely to break given a long
enough timeline.</p>

<h3 id="order" class="link"><a href="#order">No ordering guarantees</a></h3>

<p>Transmission failures, variations in latency, and quirks in
the provider&rsquo;s implementation means that even though
webhooks are sent to an endpoint roughly ordered, there are
no guarantees that they&rsquo;ll be received that way.</p>

<p>For example, a provider might send a <code>created</code> event for
<code>resource123</code>, but a send failure causes it to be queued
for retransmission. In the meantime, <code>resource123</code> is
deleted and its <code>deleted</code> event sends correctly. Later, the
<code>created</code> event is also sent, but by then the consumer&rsquo;s
received it after its corresponding <code>deleted</code>. A lot of the
time this isn&rsquo;t a big problem, but consumers must be built
to be tolerant of these anomalies.</p>

<figure>
    <img alt="A consumer receiving events out of order due to a send failure." class="overflowing" loading="lazy" src="/assets/images/webhooks/out-of-order.svg">
    <figcaption>A consumer receiving events out of order due to a send failure.</figcaption>
</figure>

<p>In an ideal world, a real-time stream would be reliable
enough that a consumer could use it as an <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">ordered
append-only log</a> which could be used to manage state
in a database. Webhooks are not this system.</p>

<h3 id="versioning" class="link"><a href="#versioning">Version upgrades</a></h3>

<p>For providers that version their API like we do at Stripe,
version upgrades can be a problem. Normally we allow users
to explicitly request a new version with an API call so
that they can verify that their integration works before
upgrading their account, but with webhooks the provider has
to decide in advance what version to send. Often this leads
to users trying to write code that&rsquo;s compatible across
multiple versions, and then flipping the upgrade switch and
praying that it works (when it doesn&rsquo;t, the upgrade must be
rolled back).</p>

<p>Once again, this can be fixed with great tooling, but
that&rsquo;s more infrastructure that a provider needs to
implement for a good webhook experience. We recently added
a feature that lets users configured the API version that
gets sent to each of their webhook endpoints, but for a
long time upgrades were a scary business.</p>

<figure>
    <img alt="Upgrading the API version sent to a webhook endpoint in Stripe's dashboard." class="overflowing" loading="lazy" src="/assets/images/webhooks/upgrade-version.png" srcset="/assets/images/webhooks/upgrade-version@2x.png 2x, /assets/images/webhooks/upgrade-version.png 1x">
    <figcaption>Upgrading the API version sent to a webhook endpoint in Stripe's dashboard.</figcaption>
</figure>

<h2 id="kitchens" class="link"><a href="#kitchens">The toil in the kitchens</a></h2>

<p>Possibly a bigger problem than any of their user
shortcomings is that webhooks are painful to run. Let&rsquo;s
look at the specifics.</p>

<h3 id="misbehavior" class="link"><a href="#misbehavior">Misbehavior is onerous</a></h3>

<p>If a consumer endpoint is slow to respond or suddenly
starts denying requests, it puts pressure on the provider&rsquo;s
infrastructure. A big user might have millions of outgoing
webhooks and just them going down might be enough to start
backing up global queues, leading to a degraded system for
everyone.</p>

<p>Worse yet, there&rsquo;s no real incentive for recipients to fix
the problem because the entirety of the burden lands on the
webhook provider. We&rsquo;ve been stuck in positions where we
have to email huge users as millions of failed webhooks
pile up in the backlog with something like, &ldquo;we don&rsquo;t want
to disable you, but please fix your systems or we&rsquo;re going
to have to&rdquo; and hoping that they get back to us before
things are really on fire.</p>

<p>You can put in a system where recipients have to meet
certain uptime and latency SLAs or have their webhooks
disabled, but once again, that needs additional tooling and
documentation, and the additional restrictions won&rsquo;t make
your users particularly happy.</p>

<h3 id="retries" class="link"><a href="#retries">Retries</a></h3>

<p>To ensure receipt, webhook system needs to be built with
retry policies. A recipient could shed a single request due
to an intermittent network problem, so you retry a few
moments later to ensure that all messages make it through.</p>

<p>This is a nice feature, but is expensive and wasteful at
the edges. Say for example that a user takes down one of
their servers without deleting a corresponding endpoint. At
Stripe, we&rsquo;ll try to redeliver every generated event 72
times (once an hour for three days) before finally giving
up, which could mean tens of thousands wasted connections.</p>

<p>You can mitigate this by disabling endpoints that look like
they&rsquo;re dead and sending an email to notify their owner,
but again this needs to be tooled and documented. It&rsquo;s a
bit of a compromise because you have less tech savvy users
who legitimately have a server go down for a day or two,
and may later be surprised that their webhooks are no
longer being delivered. You can also have endpoints that
are &ldquo;the living dead&rdquo;: they time out most requests after
tying up your clients for 30 seconds or so, but
successfully respond often enough that they&rsquo;re never fully
disabled. These are costly to support.</p>

<h3 id="chattiness" class="link"><a href="#chattiness">Chattiness and communication (in)efficiency</a></h3>

<p>Webhooks are one HTTP request for one event. You can apply
a few tricks like keeping connections open to servers that
you deliver to frequently to save a few round trips on
transport construction (for setting up a connection and
negotiating TLS), but they&rsquo;re a very chatty protocol at
heart.</p>

<p>We&rsquo;ve got enough modern languages and frameworks that
providers can build massively concurrent implementations
with relative ease, but compared to something like
streaming a few thousand events over a big connected
firehose, webhooks are very inefficient.</p>

<h3 id="internal-security" class="link"><a href="#internal-security">Internal security</a></h3>

<p>The servers sending webhooks are within a provider&rsquo;s
internal infrastructure, and depending on architecture, may
be able to access other services. A common &ldquo;first timer&rdquo;
webhooks provider mistake is to not insulate the senders
from other infrastructure; allowing an attacker to probe it
by configuring webhook endpoints with internal URLs.</p>

<figure>
    <img alt="An attacker crafting a malicious webhook to target an internal service." class="overflowing" loading="lazy" src="/assets/images/webhooks/attack.svg">
    <figcaption>An attacker crafting a malicious webhook to target an internal service.</figcaption>
</figure>

<p>This is mitigable (and every big provider has measures in
place to do so), but webhook infrastructure will be
dangerous by default.</p>

<h2 id="features" class="link"><a href="#features">What makes webhooks great</a></h2>

<p>We&rsquo;ve talked mostly about the shortfalls of webhooks, but
they&rsquo;ve got some nice properties too. Here are a few of the
best.</p>

<h3 id="balancing" class="link"><a href="#balancing">Automatic load balancing</a></h3>

<p>A commonly overlooked but <em>amazing</em> feature of webhooks is
that they provide automatic load balancing and allow a
consumer&rsquo;s traffic to ramp up gracefully.</p>

<p>The alternative to webhooks is some kind of &ldquo;pull&rdquo; API
where a provider streams events through a connection. This
is mostly fine, but given enough volume, eventually some
kind of partitioning scheme is going to be needed as the
stream grows past the capacity of any single connection
(think like you&rsquo;d see in <a href="https://kafka.apache.org/">Kafka</a> or
<a href="http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html">Kinesis</a>). Partitioning works fine, but is
invariably more complicated and makes integrating more
difficult. Getting consumers to upgrade from one to two
partitions when the limits of a single partition are
reached is <em>really</em> difficult.</p>

<p>With webhooks, scaling is almost entirely seamless for
recipients. They need to make sure that their endpoints are
scaled out to handle the extra load, but this is a well
understood problem. Horizontal scaling combined with an
off-the-shelf load balancer (DNS, HAProxy, ELBs, &hellip;) will
make this relatively painless.</p>

<h3 id="http" class="link"><a href="#http">Lingua franca</a></h3>

<p>Web servers are absolutely ubiquitous across every
conceivable programming language and framework which means
that everyone can receive a webhook, and without pulling
down any unusual dependencies.</p>

<p>Webhooks are <em>accessible</em> in a way that more exotic
technologies may never be, and that by itself is good
reason to use them. Accessible technologies have a greater
pool of potential developers, and that&rsquo;s going to lead to
more integrations. An easy API in the form of webhooks has
undoubtedly helped companies like GitHub and Slack grow
their platforms.</p>

<h2 id="road-ahead" class="link"><a href="#road-ahead">The road ahead</a></h2>

<p>Lately I&rsquo;ve been talking about what <a href="/api-paradigms">API paradigms might
look like beyond our current world of
REST</a>, so it seems like a good time to look
at some modern alternatives to webhooks.</p>

<h3 id="http-log" class="link"><a href="#http-log">The HTTP log</a></h3>

<p>Since the inception of webhooks there&rsquo;s been a few
technologies that have been standardized that are
well-suited for streaming changes. <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a>
and <a href="https://en.wikipedia.org/wiki/Server-sent_events">server-sent events</a> (SSE) are two great examples.</p>

<p>Consumers would negotiate a stream over HTTP with the
normal RESTish API, and hold onto it listening for new
events from the server as long as they can. Unlike
webhooks, events are easily accessible from any environment
(that allows outgoing connections), fully verified,
ordered, and even potentially versioned according to the
consumer&rsquo;s request.</p>

<p>A downside is that it&rsquo;s the consumer&rsquo;s responsibility to
make requests and track where they left off. This isn&rsquo;t
an overly difficult requirement, but it&rsquo;s likely to cause
problems for at least some users as they lose their place
in the stream, or don&rsquo;t fetch incoming events in time.
Providers would undoubtedly also have to put limits on how
far back in history users are allowed to request, and have
an implementation that makes sending lots of aging event
data efficient.</p>

<h3 id="graphql" class="link"><a href="#graphql">GraphQL subscriptions</a></h3>

<p>Along with queries and mutations, GraphQL supports a third
type of operation called a &ldquo;subscription&rdquo; (<a href="https://facebook.github.io/graphql/#sec-Subscription">see that in the
spec here</a>). A provider provides an available
subscription that describes the type of events that a
recipient will receive:</p>

<pre><code class="language-json">subscription StoryLikeSubscription($input: StoryLikeSubscribeInput) {
  storyLikeSubscribe(input: $input) {
    story {
      likers { count }
      likeSentence { text }
    }
  }
}
</code></pre>

<p>Along with an input type that recipients will use to
specify the parameters of the stream:</p>

<pre><code class="language-json">input StoryLikeSubscribeInput {
  storyId: string
  clientSubscriptionId: string
}
</code></pre>

<p>Like with a lot of GraphQL, the specifics around
implementation for subscriptions aren&rsquo;t strongly defined.
In <a href="http://graphql.org/blog/subscriptions-in-graphql-and-relay/">a blog post announcing the feature</a>, a
Facebook engineer mentions that they receive subscription
events over an <a href="http://mqtt.org/">MQTT</a> topic, but lots of options for
pub/sub technology are available.</p>

<h3 id="grpc" class="link"><a href="#grpc">GRPC streaming RPC</a></h3>

<p><a href="http://www.grpc.io/">GRPC</a> is a framework created by Google that enables
easy remote procedure calls (RPC) from client to server
across a wide variety of supported languages and platforms.
It builds on top of protocol buffers, a well-vetted
serialization technology that&rsquo;s been around for more than a
decade.</p>

<p>Although it&rsquo;s largely used for one-off request/responses,
it also supports <a href="http://www.grpc.io/docs/guides/concepts.html#server-streaming-rpc">streaming remote procedure
calls</a> where a provider can send back any
number of messages before the connection is finalized. This
simple Go example demonstrates roughly how it works (and
keep in mind that the feature is available in GRPC&rsquo;s
impressive set of supported languages):</p>

<pre><code class="language-go">stream, err := client.ListFeatures(...)
if err != nil {
    ...
}
for {
    feature, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        ...
    }
    log.Println(feature)
}
</code></pre>

<p>Bi-directional streams are also supported for back and
forth communication over a single re-used connection.</p>

<h2 id="today" class="link"><a href="#today">What to do today</a></h2>

<p>Webhooks are a fine system for real time streaming and
providers who already offer them and have their operational
dynamics figured out should probably stick with them. They
work well and are widely understood.</p>

<p>However, between somewhat less-than-optimal developer
experience and considerable operational concerns, providers
who are building new APIs today should probably be
considering every available option. Those who are already
building systems on non-REST paradigms like GraphQL or GRPC
have a pretty clear path forward, and for those who aren&rsquo;t,
modeling something like a log over HTTP/WebSockets/SSE
might be a good way to go.</p>
]]></content>
    <published>2017-09-28T15:28:56Z</published>
    <updated>2017-09-28T15:28:56Z</updated>
    <link href="https://brandur.org/webhooks"></link>
    <id>tag:brandur.org,2017-09-28:webhooks</id>
    <author>
      <name>Brandur Leach</name>
      <uri>https://brandur.org</uri>
    </author>
  </entry>
</feed>